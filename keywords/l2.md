# L2 (Least Squares) loss

Parents: [[loss]], [[02_Regression]]
Related: [[ridge_regression]], [[logreg]], [[normal]]

#stats #loss


Loss: J(θ) = ∑(h-yi)² across all data points yi. It is also called **Squared error**, or **squared distance**, or **Mean Squared Error** (MSE), or **Least Squares**. Because of its convex shape, it is sensitive to outliers, but permissive to small deviations around zero.

> Nice practical illustration from the [Google crash course](https://developers.google.com/machine-learning/crash-course/): 2 outliers of Δy = 2 are worse than 4 outliers of Δy = 1, as 8>4. 

Derivation: assume that y ~ h + ε, where ε is an error that is independent (and thus orthogonal) to everything you made h of, and is normally distributed. Both are logical assumptions for a continuous y, for practical reasons, and due to the central limit theorem. Then maximizing the likelihood of fitting the Gaussian cloud of ε with h means maximizing $L = \prod_i G(y_i | h(x_i))$ $\displaystyle = \prod_i \frac{1}{2πσ^2} \exp\left(\frac{(y_i-h_i)^2}{2σ^2}\right)$ $\displaystyle = (2πσ^2)^{n/2} \exp\left(\frac{\sum (y_i - h_i)^2}{σ^2}\right)$. Maximizing $\log L$ is the same as maximizing L, so we can go to $\log L$, which turns the const into a +const (that we ignore), and turns exp(Σ) into a simple Σ. Then we ignore σ², and instead of maximizing log L start to minimize -log L, which by now means minimizing the squared distances.

**Are there alternatives to L2?** Sure, **L1 norm** = abs(distance), which effectively pushes f(x) towards median(y) rather than the mean(y): sum of distances to 2 points is min when you're exactly between them. Hard to work with, as derivatives are discontinuous.

**Max Likelihood** (aka MLE): If basic assumptions behind the L2 calculation are violated, one can go back to the basics and re-consider what max-likelihood means in this particular case. For example, for nominal data y_i we get log-prob of observing it is equal to: L(θ) = ∑ log P(y_i | θ), summed by i (all points). We can try to find θ that maximizes ∏P, and thus ∑logP. But it is only different from L2 if errors are assumed to be non-normal; for normally distributed errors P(y|x,θ)=𝒩(h,σ²) where h = Xβ if you go through MLE calculations, you'll arrive at same equlations as for L2 loss.

> ESL has a formula 2.35 for it, but without derivation. I think they may have found a derivative by θ, set it to zero, then integrate the right side by x, but I'm not sure.

# Refs

https://www.expunctis.com/2019/01/27/Loss-functions.html