# L2 (Least Squares) loss

Parents: [[loss]], [[regression]]
Related: [[ridge_regression]], [[logreg]], [[normal]]

#stats #loss


Loss: J(Î¸) = âˆ‘(h-yi)Â² across all data points yi. It is also called **Squared error**, or **squared distance**, or **Mean Squared Error** (MSE), or **Least Squares**. Because of its convex shape, it is sensitive to outliers, but permissive to small deviations around zero.

> Nice practical illustration from the [Google crash course](https://developers.google.com/machine-learning/crash-course/): 2 outliers of Î”y = 2 are worse than 4 outliers of Î”y = 1, as 8>4. 

Derivation: assume that y ~ h + Îµ, where Îµ is an error that is independent (and thus orthogonal) to everything you made h of, and is normally distributed. Both are logical assumptions for a continuous y, for practical reasons, and due to the central limit theorem. Then maximizing the likelihood of fitting the Gaussian cloud of Îµ with h means maximizing $L = \prod_i G(y_i | h(x_i))$ $\displaystyle = \prod_i \frac{1}{2Ï€Ïƒ^2} \exp\left(\frac{(y_i-h_i)^2}{2Ïƒ^2}\right)$ $\displaystyle = (2Ï€Ïƒ^2)^{n/2} \exp\left(\frac{\sum (y_i - h_i)^2}{Ïƒ^2}\right)$. Maximizing $\log L$ is the same as maximizing L, so we can go to $\log L$, which turns the const into a +const (that we ignore), and turns exp(Î£) into a simple Î£. Then we ignore ÏƒÂ², and instead of maximizing log L start to minimize -log L, which by now means minimizing the squared distances.

**Are there alternatives to L2?** Sure, **L1 norm** = abs(distance), which effectively pushes f(x) towards median(y) rather than the mean(y): sum of distances to 2 points is min when you're exactly between them. Hard to work with, as derivatives are discontinuous.

**Max Likelihood** (aka MLE): If basic assumptions behind the L2 calculation are violated, one can go back to the basics and re-consider what max-likelihood means in this particular case. For example, for nominal data y_i we get log-prob of observing it is equal to: L(Î¸) = âˆ‘ log P(y_i | Î¸), summed by i (all points). We can try to find Î¸ that maximizes âˆP, and thus âˆ‘logP. But it is only different from L2 if errors are assumed to be non-normal; for normally distributed errors P(y|x,Î¸)=ð’©(h,ÏƒÂ²) where h = XÎ² if you go through MLE calculations, you'll arrive at same equlations as for L2 loss.

> ESL has a formula 2.35 for it, but without derivation. I think they may have found a derivative by Î¸, set it to zero, then integrate the right side by x, but I'm not sure.

# Refs

https://www.expunctis.com/2019/01/27/Loss-functions.html