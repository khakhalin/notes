# L2 loss

#stats #loss

Parents: [[02_Regression]]
Related: [[ridge_regression]]

Loss: J(Î¸) = âˆ‘(h-yi)Â² across all data points yi. It is also called **Squared error**, or **squared distance**, or **Mean Squared Error** (MSE), or **Least Squares**. Because of its convex shape, it is sensitive to outliers, but permissive to small deviations around zero.

> Nice practical illustration from the [Google crash course](https://developers.google.com/machine-learning/crash-course/): 2 outliers of Î”y = 2 are worse than 4 outliers of Î”y = 1, as 8>4. 

**Are there alternatives to L2?** Sure, **L1 norm** = abs(distance), which effectively pushes f(x) towards median(y) rather than the mean(y): sum of distances to 2 points is min when you're exactly between them. Hard to work with, as derivatives are discontinuous.

**Max Likelihood** (aka MLE): In some way, can be considered an alternative to L2 (MSE) loss. _Is it really true?? I am not sure it is, and I don't quite understand what ESL was saying here. Isn't L2 a direct consequence of Max Likelihood, assuming Gaussian distribution of errors?_ For a data sample y_i, the log-prob of observing it is equal to: L(Î¸) = âˆ‘ log P(y_i | Î¸), summed by i (all points). We can try to find Î¸ that maximizes âˆP, and thus âˆ‘logP. But it is only different from L2 if errors are assumed to be non-normal; for normally distributed errors P(y|x,Î¸)=ð’©(h,ÏƒÂ²) where h = XÎ² if you go through MLE calculations, you'll arrive at same equlations as for L2 loss.

> ESL has a formula 2.35 for it, but without derivation. I think they may have found a derivative by Î¸, set it to zero, then integrate the right side by x, but I'm not sure.

# Refs

https://www.expunctis.com/2019/01/27/Loss-functions.html