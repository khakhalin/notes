# Vision

#bib #neuro

Parents: [[12_Neuro]], [[bib_image]], [[video]], [[deepneuro]]
Related: [[superstimulus]], [[illusions]], [[ene_070_vision]]

Pure ML vision is covered in [[bib_image]] and [[video]], so here we have only neuro-inspired ML, and pure neuroscience.

# Neuro-inspired ML

* [[convnet]] - everything about convnets

# ML as a model for Neuro

Papers:
* [[Yamins2016viscortex]] - a review comparing primate vision to convnets
* [[Zhang2018perceptive_dl]] - convnet models are really good for identifying perceptually similar images
* [[Bashivan2019control]] - using DL to create stimuli for brains (see [[superstimulus]])
* [[Walker2018inception]] - also using DL to create superstimuli
* [[KhalighRazavi2014cortex]] - recordings from humans and monkeys match DL activation
* [[hmax]] - old (1999) manual model of primate vision
* [[Walker2018inception]] - reconstructing visual perception from visual recordings


Pospisil, D. A., Pasupathy, A., & Bair, W. (2018). 'Artiphysiology'reveals V4-like shape tuning in a deep network trained for image classification. Elife, 7, e38242. https://elifesciences.org/articles/38242 - They seem to claim that CNN deep layer neurons selectivity is actually similar to primate V4. Not too wildly cited tho.

Nayebi, A., Sagastuy-Brena, J., Bear, D. M., Kar, K., Kubilius, J., Ganguli, S., ... & Yamins, D. L. (2021). Goal-Driven Recurrent Neural Network Models of the Ventral Visual Stream. bioRxiv. https://www.biorxiv.org/content/biorxiv/early/2021/02/18/2021.02.17.431717.full.pdf

# Pure Neuro vision

Olshausen, B. A., & Field, D. J. (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381(6583), 607-609.
Some classic paper that (if I got the abstract right?) derives wavelet-like RFs from the idea of sparseness.

Olshausen, B. A., & Field, D. J. (2006). What is the other 85 percent of V1 doing. L. van Hemmen, & T. Sejnowski (Eds.), 23, 182-211. http://www.rctn.org/bruno/papers/V1-chapter.pdf
A famous paper on how we don't really understand V1 (relevant for [[convnet]]-discussions)