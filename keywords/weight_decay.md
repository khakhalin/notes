# Weight Decay

#dl

Related: [[dl]], [[adam]], [[regression]] (everything abour regularizations)

A type of **regularization**, in which weights a made to decay to 0 during training. For **Stochastic Gradient Descent** (SGD), it's exactly equivalent to L2 regularization, but for other descent methods (such as ADAM), it's not necessarily the same thing.

# Refs

Hanson, S. J., & Pratt, L. Y. (1989). Comparing biases for minimal network construction with back-propagation. In Advances in neural information processing systems (pp. 177-185).
https://papers.nips.cc/paper/156-comparing-biases-for-minimal-network-construction-with-back-propagation.pdf