# Recruiting, hiring, interviewing

#management #bib #interview

See also: 
* [[job_search]] - kind of the opposite perspective
* [[system_interview]] - how to prepare for systems interivews
* [[behav_interview]] - how to prepare for behav interviews

Related:
* [[id20201020160912]] - why CS recruiting market is broken, Aline Lerner 2020
* [[Saroufim2021screening]] - that objective tech interviews are impossible, Mark Saroufim 2021
* [[Bock2015workrules]] - a book about Google corporate culture

General ideas for good interviewing:
* Don't do unstructured interviews (they just reinforce your biases). Always come with a structure first, a list of questions to ask, and ask all candidates these questions
* Questions that don't help (don't correlate with performance): trivia, puzzles, invasive behavioral questions (like "If I ask your colleagues about your main weakness, what would they say?" - very bad, potentially discriminatory!)
* Questions that help: anything that resembles real work. So either questions about past work (asking them to explain something they did - tests both what they did, and how well they can explain it), and questions about potential problems they may face in their new job (like, how would you approach this problem, or that situation)
* Actual mini-projects (2-3 hours) in theory work even better, but it may be hard to come up with them.

# Good practices

Erik Bernhardsson. How to hire smarter than the market: a toy model. 2020.
https://erikbern.com/2020/01/13/how-to-hire-smarter-than-the-market-a-toy-model.html
1. In any competitive pool, experience and performance (or any 2 talents!) will anti-correlate. Because people develop either skill 1 or skill 2; with spent effort being const, it's a zero-sum game.
2. Which means that going for "most rounded candidates" we will actually end up with subpar performance. Maybe it's better to have "pokemon teams" where people with extreme strengths balance each other?
3. It's also good to go against the grain of the market, and to officially give up on some of the "virtues" in your candidates. It allows to get better candidates on all other dimensions.

# Bad practices

Zhang, D. C. (2021). Horse-sized ducks or duck-sized horses? Oddball Personality Questions are likable (but useless) for organizational recruitment. Journal of Business and Psychology, 1-19. https://psyarxiv.com/phnsr/

Kausel, E. E., Culbertson, S. S., & Madrid, H. P. (2016). Overconfidence in personnel selection: When and why unstructured interview information can hurt hiring decisions. Organizational Behavior and Human Decision Processes, 137, 27-44.
Unstructured interviews not just don't help, but actually HURT the quality of hiring.

Woolley, K., & Fishbach, A. (2018). Underestimating the importance of expressing intrinsic motivation in job interviews. Organizational Behavior and Human Decision Processes, 148, 1-11.
Recruiters actually care about intrinsic motivations of candidates (seeking a meaningful job, and having the description match what is available), which candidates don't quite realize. So from a practical pov, it means that it's good to express a genuine interest in the kind of work you're applying for.

Ammon Bartram.. Who Y Combinator companies want. 2015.
https://triplebyte.com/blog/who-y-combinator-companies-want
They asked ~30 big software companies to fill a table about what properties of job candidates they value, and which ones actually hurt an application. They found:
1. Almost no correlation between companies! It is completely unpredictable (and actually seems to most closely follow the profile of the founder of each company).
2. There are some universals though:
    * Everyone likes product-oriented people (product programmers), as opposed to technique-based programmers;
    * Everyone likes practical programmers, and hates academic programmers.
    * Experience matters (but that's obvious)
Tweet to quote: https://twitter.com/DynamicWebPaige/status/1377464761013653505

Meijer, R. R., Neumann, M., Hemker, B. T., & Niessen, A. S. M. (2020). A tutorial on mechanical decision-making for personnel and educational selection. Frontiers in psychology, 10, 3002.
https://www.frontiersin.org/articles/10.3389/fpsyg.2019.03002/full
In hiring, more information is not better. Even completely noisy measures will be given some weight by people (that's just how our brains work), making hiring outcomes objectively worse.
* Footnote: citation for that: Adams, G.S., Converse, B.A., Hales, A.H. et al. People systematically overlook subtractive changes. Nature 592, 258–261 (2021). https://doi.org/10.1038/s41586-021-03380-y

Dalal, D. K., Sassaman, L., & Zhu, X. S. (2020). The impact of nondiagnostic information on selection decision making: A cautionary note and mitigation strategies. Personnel Assessment and Decisions, 6(2), 7.
https://scholarworks.bgsu.edu/cgi/viewcontent.cgi?article=1117&context=pad
About biases in hiring (they call it "nondiagnostic information"). "Stubborn reliance on intuition" as the main enemy of good hiring (see next ref).

Highhouse, S. (2008). Stubborn reliance on intuition and subjectivity in employee selection. Industrial and Organizational Psychology, 1(3), 333-342
https://www.guruji24.com/acwi_guru/resources/blog_files/6Organizational_Psychology_-_Reliance.pdf

Anti-test sentiment and a desire for "holistic assessment" are ruining selection and hiring, as they introduce random noise in decision-making. And yet people keep to stubbornly believe in their ability to guess who the candidates "really are". Summarizes hard data that adding personal assessment to test scores reduces quality of prediction about the success of students, candidates. Moreover, intuition of a recruiter doesn't improve with experience (many refs here). People believe that they get better in finding good candidates, but they actually aren't getting better; they are just getting more confident.

The only counter-argument against fully mechanistic selection is the "broken-leg factor": human can incorporate "out-of-distribution" unexpected information in ways that rubrics and pre-trained models can't (the name comes from an example: a person with a freshly broken leg is not likely to go to a cinema, which has nothing to do with whether they like the movie, and that a model trained on personal tastes will be likely to miss).

# Cognitive tests

Kuncel, N. R., & Hezlett, S. A. (2010). Fact and fiction in cognitive ability testing for admissions and hiring decisions. _Current Directions in Psychological Science_, _19_(6), 339-345.
https://journals.sagepub.com/doi/pdf/10.1177/0963721410389459?casa_token=apoYkIPvH4IAAAAA:4U6Z4lcceg7zSlKESb0d2cAM5x9n9b8usV4d1yDXq1WsEehY4qwJElJTmNl7J-KAyLkUopnF8SG5
A review of literature, showing that job performance correlates with standardized tests of aptitude and personality traits. However for them r=0.6 (measrued cognitive ability vs high complexity job performance) seems high, and they are not ashamed to mention that high GPA is a "meaningful predictor" (with r=0.2). Very uncriticial of their own analysis. To exaggerate, the text is written as if they just randomly decided to understand "practically valid" as "statistically significant in high samples", and then proceeded to write a paper about that. Which makes their claims technically true, but at the same time useless in business practice. The analysis of biases is very shallow, and with some "orange flags" in terms of accents.

Kuncel, N. R., Ones, D. S., & Sackett, P. R. (2010). Individual differences as predictors of work, educational, and broad life outcomes. _Personality and individual differences_, _49_(4), 331-336.
https://www.sciencedirect.com/science/article/pii/S0191886910001765?casa_token=IMaLMvi5nMEAAAAA:pAZ1p4J1gBYfSQW4m2FgrAgQ6D0EWZ5BSMnONSwHNBBvkLOy6oXxgIih9WYVPXWvieh21HyYUg
Almost identical to the previous one; same claims, but different language. For example, they find a ~0.4 correlation between SAT and college GPA; admit that it is decreased to r = 0.15-0.2 for racially uniform samples, and still call it "valid and reliable". Hashtag shrug, hashtag ok boomer.

Wee, S., Newman, D. A., & Joseph, D. L. (2014). More than g: Selection quality and adverse impact implications of considering second-stratum cognitive abilities. _Journal of applied psychology_, _99_(4), 547.
https://www.researchgate.net/profile/Serena-Wee/publication/287207336_Wee2014b/links/5673918e08aedbbb3f9fac7b/Wee2014b.pdf
When hiring based on tests, if you use total scores, you get rigid non-diverse teams with members following a certain "rounded" profile. The only way to get a more interesting team is to pick people with complementing skills by selecting winners of individual sub-scores. Start with a quick summary that cognitive tests work better than interviews and biographies (2 refs), but worse than work samples and assessments of past performance (4 refs).

# Personality tests

(as an alternative to [[behav_interview]])

The Validity and Utility of Selection Methods in Personnel Psychology: Practical and Theoretical Implications of 100 Years
https://home.ubalt.edu/tmitch/645/session%204/Schmidt%20&%20Oh%20validity%20and%20util%20100%20yrs%20of%20research%20Wk%20PPR%202016.pdf

# Hiring in Data Science specifically

James Densmore, 2017. There are two types of data scientists — and two types of problems to solve
https://medium.com/@jamesdensmore/there-are-two-types-of-data-scientists-and-two-types-of-problems-to-solve-a149a0148e64
Type A (for "Analysis") like working with data, answering questions, applying methods, finding patterns, making sense of numbers, designing experiments, applying statistics. Type B (for "Builders") like developing new tools for production (more of a software engineer type), such as prediction systems, pipelines, etc. Two types of problems, two types of people, and they need to match, or everyone are unhappy.