# Interpretability

For now, a stub, to collect good info on interpretability.

#interpretability #dim #datavis #bib

See also: [[fairness]]

# Papers

* [[Hinton2006dim]] - used a PCA-like plot to understand representation in a deepest layer of the autoencoder, by training a separate 2-wide autoencoder
* [[Olah2017visualization]] - interpreting visual classifiers
* [[Saxe2020interpretable]] - a blog post about additive ANN models (GAM)

# Comparing networks

Maybe (probably) need to be moved to a separate section? But how to name it? Basically, it's about figuring out whether 2 networks are "equivalent" in some sort, or not.

* [[Yamins2016viscortex]] - comparing layers from their activations

Rolnick, D., & Kording, K. (2020, November). Reverse-engineering deep ReLU networks. In International Conference on Machine Learning (pp. 8178-8187). PMLR. https://arxiv.org/abs/1910.00744

Haber, A., & Schneidman, E. (2020). Learning the architectural features that predict functional similarity of neural networks. bioRxiv. https://www.biorxiv.org/content/10.1101/2020.04.27.057752v1

Maheswaranathan, N., Williams, A. H., Golub, M. D., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks. Advances in neural information processing systems, 2019, 15629. https://arxiv.org/abs/1907.08549

# To read

https://distill.pub/2018/building-blocks/
Olah2018interpretability

https://distill.pub/2019/computing-receptive-fields/

https://distill.pub/2020/circuits/curve-detectors/

https://distill.pub/2021/multimodal-neurons/
