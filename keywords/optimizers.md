# Optimizers

Parents: [[hyperparameters]]
See also: [[regularization]], [[archsearch]]

#optimizers


Pretty much just read this: [[Ruder2016descent]]

# Learning rate

**Goldilocks principle** - the best learning rate should "magically" put you in the minimum in a very few steps. Large learning rate leads to noisy oscillations after what looked like a convergence. It may even break everything after convergence (unstable).