# Influence Maximization

#networks

Path: [[09_Graphs]] / [[sir]] + [[graph_cascades]]
See also: [[algos_graph]]

The task from **marketing**: we tend to trust (or follow) our friends, so behaviors (brand preferences etc.) tend to propagate through a network. But some people (nodes) are more influential than others (Kate Middleton effect - some member of the British Royal family apparently affects the sales of like clothes and stuff), so it would be useful to be able to identify these influential people and use them as seeds.

One obvious property is that influential nodes should have a high degree, but if we want to find a **set of k seeds** of influential nodes (on a directed graph), we can no longer go by simple degree, as the calculation becomes non-trivial for two reasons: 
1. the overlap between zones of influence starts to matter. We don't want all nodes tap into the same pool, even if their degree is high;
2. we care about cascades (2nd, 3d order connectedness), not just about immediate neighbors.

Two classic models:

**Linear Threshold Model** - a decision-based model. Let each node have a threshold $θ_i$ (say, random and uniformly distributed on 0..1). Once the weighted sum of influences from activated neighbors $\sum_j b_j$ exceeds the threshold, the node also gets activated. (Here j only runs among nodes neighboring with i, and also activated; and $b_j$ are edge weights = strength of influence).

> But we won't be considering this model here, haha. Joke on you.

**Probabilistic contagion** - a pandemic-like model. Again, a weighted directed graph, but weights of edges $p_{ij}$ are now probabilities of contagion traveling along this edge. Each node has only one chance of getting activated by another node: **each edge fires only once**. If node i gets active, it has one chance to switch each of its neighbors j from an "undecided" state to either "active" state (with probability $p_{ij}$). If node j has several active neighbors (say, i and k), they each get a go, and the sequence obviously doesn't matter: the total probability of getting activated turns to be $1-(1-p_{ij})(1-p_{kj})$, whichever one tries first (it's like tossing a coin twice).

How to find a set S of k most influential nodes, to produce the **largest expected cascade size** f(S)? This maximization problem $\displaystyle \max_S f(S)$ is actually **NP-complete** (at least as hard as the **set cover problem**). 

> Reminder: **Set cover problem**: given a collection of sets {S}, find the smallest number of sets that together (in terms of union) cover the entire "universe" (full union over the collection).
https://en.wikipedia.org/wiki/Set_cover_problem

However there's a greedy approximate solution, and it is proven that is at worst only 0.63 smaller than the actual optimal solution. (Where this 0.63 actually comes from $1-1/e$ , as it turns out).

$\displaystyle f(S_{\text{greedy}}) ≥ (1-1/e)\cdot f(S_{\text{optimal}})$, and $(1-1/e) \approx 63\%$. 

# Greedy algorithm

**Greedy Hill-Climbing** (apparently often called just GREEDY in small-caps): at each step, activate node $u$ that provides the largest marginal gain: $\displaystyle \max_u f(S_{t-1} \cup {u})$. _We don't yet tell here how to do it; we're just setting the goal for what we are trying to do._ For the guarantee above (of being at most 0.63 worse than the optimal solution) to hold, f(S) should have to properties:
1. it needs to be monotone: activating more nodes never hurts, so S ⊆ T ⇒ f(S) ≤ f(T).
2. and submodular, aka has diminishing returns: adding an element later always helps less than adding it earlier. ∀S ⊆ T, adding new node u to S improves the value of f( ) more than adding u to T. That is: $S ⊆ T ⇒ f(S \cup u)-f(S) ≥ f(T \cup u)-f(T)$.

27:19 they go to a proof

**Draft of a proof:** (aka Nemhauser-Wolsey proof) How to show that the function f(S) is submodular? It's like with a set cover problem, except sets are generated by a random process. To deal with this randomness (to address it in the proof), let's use **deferred decision** principle: instead of flipping coins for each edge as we go, let's consider many different "universes" (for each run of the model), and in each universe we'll flip all coins in advance (a biased coin per age). Now we have a deterministic graph (we we remove all edges that didn't fire, aka for which the coin failed). Within each of these instances (worlds, experiments) the influence set X of a node v is a set of all nodes simply reachable from v. Now, within each of the worlds we have a simple statement on sets: if $T⊇S$ because  $T = S \cup Y$, then $Δ_T = (T \cup X - X) ⊆ Δ_S = (S \cup X - X)$; namely, $Δ_S - Δ_T = Y \cap X$. And because this is true for every run, and f(S) = expectation(S) is essentially a linear combination of all runs, it would be true for expectations as well. Similarly (and arguably easier) we can prove that f(S) is monotonously increasing.

> This 1-1/e comes from an actual proof for the worst cause, described in "proof" papers, referenced below. But roughly, it comes from a limit $\displaystyle \lim_{K→∞}\left(\frac{K-1}{K}\right)^K$, where K is the step size. So we consider the slowest possible growth of a greedy algorithm, and it give us some guarantees. The full proof with all examples is like 30 pages, so let's resort to faith here.

# Sketch-based algorithm

But so, **how to use all of this in practice?** One approach would be to actually run (simulate) the process, with honest coin-flipping (~10 000 times for every node), thus estimating influence sets $X_v$. That's what GREEDY does (Kempe 2003). But this is super-slow, with complexity of O(k ∙ N_nodes ∙ N_simulations ∙ N_edges):
1. we repeat everything k times (as we need to collect k nodes)
2. at every step we consider almost-N possible nodes
3. as we work with expectations f(S) = E(S), we need to run many simulations (coin-flipping), every time removing a different set of edges
4. and for every simulation, to calculate the influence set of a node, we'll need to go through edges
We need so speed it up!

**Sketches** (aka **SKIM**: SKetch-based Influence Maximization) allow to reduce the estimation of influence from O(N_edges) to O(1). How? Two basic ideas:
1. we can pre-compute a small (limited) structure for every node, then run influence maximization using these pre-computed estimates. It's a bit like memoization: we calculate them only once, and then reuse them, with minimal modifications where necessary.
2. we don't have to estimate influence areas with some ridiculous precision; all we need is to identify the best node. So we can stop incremental improvements once the leader is clear.

In practice, after coin-flipping (after randomly removing some edges), assign every node a random number (either uniformly distributed on 0..1, or `range(N_nodes)` randomly assigned). Then for each node, find the minimal node-value that can be reached from this node (using BFS on a reversed graph). Call it the rank of this node. The intuition is that the more influential the node, the more nodes you can reach from it, and so the lower it's rank value will be. But it's not a guarantee: it's still a random process, so we need to try it again and again  C times (essentially, new N_simulations). So instead of a single rank, we will work with a vector of ranks (length C), and every time calculate elementwise min of these rank-vectors. u⇝G

> In the lecture the lector said something to the effect of "Every time we need to find a new most influential node, we just pick a node with smallest rank-vector, lexicographically". But the original paper doesn't mention lexicographic order, so either they have it, but don't call it that way, or they don't have it. Moreover, the original paper uses ranks differently; they are not min-based, they are not in 0..1, and they are somehow incremented by 1. But I can't understand what is happening in the algorithm, as somehow I cannot understand the notation :\

> Overall, while the original paper has the algorithm all spelled out, it's not easy to understand. So I'll call it quits for now.

After the target node u is added to a growing set, we need to go and update "instantiated graphs" for all "coinflipped universes". We need to go through all coin-flipped universes, and within each universe  delete the added node itself, and all nodes that can be reached from it (BFS), from "zones of influence" for each node.

The expected running time of this version is near-linear with the number of simulations: O(N_nodes ∙ N_simulations + two much smaller terms). We lost the number of edges as a factor under O(∙), and that's because now we consider only a fixed number of edges (just enough to find the biggest )

In practice, simulations show that sketch-base algorithm achieves high performance, but is orders of magnitude faster.

# Refs

Domingos, P., & Richardson, M. (2001, August). Mining the network value of customers. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 57-66).
http://dm.cs.washington.edu/papers/mnvc.pdf
The paper that introduced the concept, and offered early analysis. 3k citations. Very influential model.

**Proofs that the greedy algorithm works:**
* Nemhauser, G. L., Wolsey, L. A., & Fisher, M. L. (1978). An analysis of approximations for maximizing submodular set functions—I. Mathematical programming, 14(1), 265-294.
* Fisher, M. L., Nemhauser, G. L., & Wolsey, L. A. (1978). An analysis of approximations for maximizing submodular set functions—II. In Polyhedral combinatorics (pp. 73-87). Springer, Berlin, Heidelberg.

**Now-classic Greedy algorithm:**
Kempe, D., Kleinberg, J., & Tardos, É. (2003, August). Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 137-146).
http://www.theoryofcomputing.org/articles/v011a004/v011a004.pdf
7k citations!

**Sketch-based fast algorithm:**
Cohen, E., Delling, D., Pajor, T., & Werneck, R. F. (2014, November). Sketch-based influence maximization and computation: Scaling up with guarantees. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management (pp. 629-638).
https://arxiv.org/pdf/1408.6282.pdf

Lecture sides from CW224W:
http://web.stanford.edu/class/cs224w/slides/14-influence.pdf
video:
https://www.youtube.com/watch?v=oajYN9HKWhI