# Mind and Consciousness

("Mind" is just a much shorter and pleasant to type word than "Consciousness" :)

Parents: [[neuro]]
See also: [[symbolic]]

#meaning #psych


# My thoughts

What we perceive as "conscious" is a very specific anthropocentric calculation. It's 1) fairly fancy, 2) the subject hesitates (realizes, reflects), and 3) drastically changes the strategy. So what feels "conscious" to us is when system 2 (GOFAI, or Marcus-style [[symbolic]] logic) detects that system 1 (intuition, more FF NNs) fails, and corrects it. Like, forcing a change in strategy, inferring info (mirror test, theory of mind) etc.

The problem with this definition is that it's obviously not transferrable across architectures and scales of hardware efficiency. Like, Alpha Go almost does it, but we don't perceive it as conscious as it's too "simple" to us. Conversely a good enough ANN may do it without relying on symbolic logic (GPT almost did it, in a way, producing what almost felt like "artifacts of consciousness" without any deliberate self-reflection)

In public debates on consciousness and the philosophy of mind, it is extremely common that people just talked through each other, as if they partiicipated in a bunch of non-interacting monologues instead of a dialog. Which possibly, at the meta level, points at the fact that the "problem of consciousness" is not really about consciousness being somehow hard, but about people not being able to talk on this topic productively. The problem is not in the phenomenon itself, but in the way that we engage with it. In other words, maybe it's not that "consciousness is an illusion", but rather that "the problem of consciousness" is an illusion: it's a non-problem masquerading as a problem, tricking smartest people into trying to crack something that's uncrackable by design, as the problem is just not there at all. 