# Reservoir computing

#echo #rnn #graph #bib

Parents: [[07_RNNs]]
See also: [[criticality]], [[bib_stdp]], [[cellular_automata]]

# Echo-State Networks

#todo

# Liquid State Machines

Similar to ESN, but on spiking neurons? #todo

# Papers

* [[Jaeger2004harness]] - Harnessing Nonlinearity - first description? (CHECK!) #halfthere

# To-read

Paassen, B., & Schulz, A. (2020). Reservoir memory machines. arXiv preprint arXiv:2003.04793.
https://arxiv.org/abs/2003.04793
https://gitlab.ub.uni-bielefeld.de/bpaassen/reservoir-memory-machines

Jaeger, H. (2007). Echo state network._scholarpedia_,_2_(9), 2330.

Rodan, A., & Tino, P. (2010). Minimum complexity echo state network._IEEE transactions on neural networks_,_22_(1), 131-144.

Lukoševičius, M. (2012). A practical guide to applying echo state networks. In_Neural networks: Tricks of the trade_(pp. 659-686). Springer, Berlin, Heidelberg.

Sussillo, D. and Abbott, L. F. (2009). Generating Coherent Patterns of Activity from Chaotic Neural Networks. Neuron, 63(4):544–557.
https://www.sciencedirect.com/science/article/pii/S0896627309005479

Sussillo, D., & Barak, O. (2013). Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks. Neural computation, 25(3), 626-649.
https://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00409

Manjunath, G. (2020). Memory-Loss is Fundamental for Stability and Distinguishes the Echo State Property Threshold in Reservoir Computing & Beyond._arXiv preprint arXiv:2001.00766_.
[https://arxiv.org/pdf/2001.00766.pdf](https://arxiv.org/pdf/2001.00766.pdf)
Looks very relevant!!

Tanaka, G., Yamane, T., Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., ... & Hirose, A. (2019). Recent advances in physical reservoir computing: a review. Neural Networks.
https://www.sciencedirect.com/science/article/pii/S0893608019300784
High priority #todo

Pogodin, R., Corneil, D., Seeholzer, A., Heng, J., & Gerstner, W. (2019). Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks. arXiv preprint arXiv:1910.10559. 
https://arxiv.org/pdf/1910.10559.pdf 
Reservoir computer + a "working memory network"

Rotermund, D., & Pawelzik, K. R. (2019). Biologically plausible learning in a deep recurrent spiking network. bioRxiv, 613471.
https://www.biorxiv.org/content/10.1101/613471v1.full

Stimberg, M., Goodman, D. F., & Nowotny, T. (2019). [Brian2GeNN: accelerating spiking neural network simulations with graphics hardware](https://www.nature.com/articles/s41598-019-54957-7). Scientific Reports.
Spiking network simulator that is apparently so optimized that it runs very fast. Check before writing any custom spiking models.

Sussillo, D., & Abbott, L. F. (2012). Transferring learning from external to internal weights in echo-state networks with sparse connectivity._PLoS One_,_7_(5).

Lukoševičius, M., & Uselis, A. (2019, September). Efficient Cross-Validation of Echo State Networks. In_International Conference on Artificial Neural Networks_(pp. 121-133). Springer, Cham.