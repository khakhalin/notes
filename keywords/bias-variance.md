# Bias-Variance Tradeoff

#variance

Parents: [[02_Regression]]
Related: [[confusion]], [[regularization]]

Total prediction error (L2 loss, [[l2]]) consists of two parts: **bias** and **variance**. 

Say, we regress y by x for a dataset (x,y), generated by a distribution P(x,y). For a given x, we can find P(y|x), and then it will be true that $P(x,y) = ∫ P(y|x)P(x) dx$. ML estimator h(x) is supposed to predict the expected y(x), or Ey(x). By definition Ey(x) = ∫y P(y|x) dy.

The main task of ML is to guess (predict) Ey(x) well. Say, h(x) is our guess for Ey (x), produced by some method A, based on a training set D. The expected total error for this estimation h is: J = E (h-y)² = ∬ P(x,y)(h(x)-y)² dx dy. 

Now, h itself is a random variable, as it's a function of a random sample D that was sampled from a true distribution P(x,y). So we can consider expected value: Eh, by integrating over all possible training sets D. We can estimate it by training lots of times and averaging. Or, if your dataset is finite, you could also just apply method A to the full dataset, obtaining the best h possible.

Now, the expected error for the method A itself (across all possible estimators h it could possibly produce) is the estimator error integrated over all possible estimators h (and so, indirectly, over all possible Ds), so we have a triple-integral (by D, x, y): εA = $\int_D \int_x \int_y (h(x)-y)^2 P(D) P(x,y)\,dD\,dx\,dy$

Add and subtract Eh (mean h) inside the square; get (h-Eh) and (Eh-y); open squares. (h-Eh)² becomes **variance**. 2(h-Eh)(Eh-y) dies out. Here's why: for a fixed x, the distribution of (Eh-y) is the same for every D. The only thing that depends on D in this integral is h, and this term doesn't have it: Eh(x) is a result of integration over D, and thus doesn't depend on D, and y is governed by the actual distribution P(x,y), not anything sampled. So for each D, x, we deal with the same (Eh-y). The first term (h-Eh) is different for different D, but once we integrate by D, we get 0, by definition of Eh: ∫_D (h-Eh) = 0. 

The term (Eh-y)² will eventually become **bias**, but it needs some massaging first. Similar to what we did before, subtract and add Ey, then open squares. We're left with (Eh-Ey)² and (Ey-y)². The term 2(Eh-Ey)(Ey-y) dies off at int_y step. (Just make sure to integrate by y at the deepest level, to get 0 before int_x has a chance to see it).

So we ended up with 3 terms:
* **Variance** (of the classifier): (h-Eh)² - how far each trained classified h will be from the ideal classifier Eh
* **Bias**: (Eh-Ey)² - how far each individual classifier will be from its trained data
* **Irreducable error**, aka noise, aka intrinsic variance of the data: (Ey-y)²

It seems to be somewhat similar to the **precision / recall** situation (see [[confusion]]). If you try to minimize bias (make your classifier cling to the data), you fall at mercy of your training set, and so increase variance. Each particular classifier, understood as an instance produced by some particular set of training data, will cling to this testing data, so all classifiers will be slightly different, if you train on different data. If however you believe that the underlying solution has some constraints to it, you can restrain your classifier, even at a cost of accepting higher bias (Eh-Ey), to ensure that it does not change too widely for different subsets of your testing data. **Regularization** and the use of simplified, constrained models, achieve that. In this case, for each given training set you'll get higher bias, but lower variance (of your classifier, across all possible training sets), as all models will be more similar to each other (and hopefully also closer to the "ground truth"). A parsimony principle: this logic is guided by an assumption that simpler, more constrained models are better.

> So, to got it right, one could think of bias, as bias away from the data, and towards the expected ideal classifier (in some sense). Increasing bias of this type of estimators (towards some Platonic notion) decreases variance of this type of estimators (relative to this platonic notion). Right? Unbiased estimators always agree well with the training data, but are highly variable from one training set to another. So essentially the way to relate to it, is to think of bias and variance of the approach: not even each individual estimator h, but the algorithm A itself.

Whether this effect will be manifest IRL depends on whether it is exacerbated by overfitting (problems of interpolation and extrapolation). When a constrained model tries to cling to the data, it may also diverge in parts of the landscape that have no or few observed training values, driving variance up. (Imagine a textbook picture of a polynomial overfitting here). In this case, low bias ⇒ high variance ⇒ great fit on the training set ⇒  horrible fit on parts of the testing set. This is because variance can be further split into **variance due to sampling** and **variance due to optimization**, and it is the strength of the 2nd component of variance that matters.

There are two typical graphical illustrations for it. One shows how training and validation error rates change as you increase the complexity of the model. Both training and testing losses first go down, but at some point overfitting kicks in and turns the test curve up.

Another canonical illustration looks like a parabola (total error) lying on two meeting hyperbolas (left horn for bias, right horn for variance): as you change model complexity, first error goes down because the bias goes down, but then increases again as variance starts to rapidly increase.

> But look, it would only work if the ground truth is actually simple, no? Why does it actually work? Why would the ground truth be simple, in practice? Is it some expected statistical property that the world actually follows, similar to how the central limit theorem shows that the world follows certain statistical patterns? Or is it because a typical practical problem has certain properties, by virtue of being practical (some sort of selection bias)? What's the philosophy behind that?

# Related concepts

**Prediction bias**: a very simple measure of model validity: average value of all predictions - average value of all learning points. In a reasonable model, prediction bias should be close to 0. A way to assess it: build a **calibration plot** - bucket values, calculate predictions, then plot mean(predictions) against mean(values). May help to find areas where the model misbehaves.

# Refs

ESL p24, p37

lecture by K Weinberger: https://www.youtube.com/watch?v=zUJbRO0Wavo

[[Neal2019blog]].