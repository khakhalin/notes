# To-read: Neuro

Se also: [[12_Neuro]] (the official hub age)

#neuro #bib #todo


üî•üî• Zenke ‚Ä¶ Goodman. Visualizing a joint future of neuroscience and neuromorphic engineering. 2020
https://www.sciencedirect.com/science/article/pii/S089662732100009X?dgcid=coauthor

üü¢ Suarez, L. E., Richards, B. A., Lajoie, G., & Misic, B. (2020). Learning function from structure in neuromorphic networks. bioRxiv. https://www.biorxiv.org/content/biorxiv/early/2020/11/11/2020.11.10.350876.full.pdf

Perez-Nieves, N., Leung, V. C., Dragotti, P. L., & Goodman, D. F. (2020). Neural heterogeneity promotes robust learning. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.12.18.423468v1

Predictive coding is an emergent property of input-driven RNNs trained to be energy efficient
https://www.biorxiv.org/content/10.1101/2021.02.16.430904v1
Tweetprint: https://twitter.com/TimKietzmann/status/1361673150828838913

Cornford, J., Kalajdzievski, D., Leite, M., Lamarquette, A., Kullmann, D. M., & Richards, B. A. (2020). Learning to live with Dale's principle: ANNs with separate excitatory and inhibitory units. bioRxiv.
https://www.biorxiv.org/content/biorxiv/early/2020/11/03/2020.11.02.364968.full.pdf
Brief summary: üí° #todo see Twitter

Hasson, U., Nastase, S. A., & Goldstein, A. (2020). Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks. Neuron, 105(3), 416-434.
https://www.cell.com/neuron/pdf/S0896-6273(19)31044-X.pdf
An opinion (perspective) on how we could, and should, use deep models to understand psychology and neuroscience, as they are not that different than the results of evolution, after all.

On backprop in the brain:
* https://blog.evjang.com/2021/02/backprop.html
* https://psychology.stackexchange.com/questions/16269/is-back-prop-biologically-plausible

Haber, A., & Schneidman, E. (2020). Learning the architectural features that predict functional similarity of neural networks. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.04.27.057752v1

# General, top, and fun

üî• Cammarata, et al., "Thread: Circuits", Distill, 2020.
https://distill.pub/2020/circuits/
A series of short papers trying to understand and describe how a DLL vision model works, by looking at its tuning, neuron by neuron. (Would it make a good paper for a neuro class?)

Raman, D. V., Rotondo, A. P., & O‚ÄôLeary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.
https://www.pnas.org/content/116/21/10537.short

Some general opinion on how to research in ML:
https://distill.pub/2017/research-debt/

Ramsauer, H., Sch√§fl, B., Lehner, J., Seidl, P., Widrich, M., Gruber, L., ... & Hochreiter, S. (2020). Hopfield networks is all you need. arXiv preprint arXiv:2008.02217. https://arxiv.org/abs/2008.02217
See also this methodical guide through it (1 hour tho!): https://www.youtube.com/watch?v=nv6oFDp6rNQ

# Topology and development

Related to: [[echo]], [[synfire]]

Hassan, B. A., & Hiesinger, P. R. (2015). Beyond molecular codes: simple rules to wire complex brains. Cell, 163(2), 285-291. https://www.cell.com/cell/fulltext/S0092-8674(15)01193-9
üíé Developmental biology bordering fractals and graphs (maybe? not sure, but judging from the pics) - a nice review-like paper (perspective); worth a priority read :)

Lazar, A., Pipa, G., & Triesch, J. (2006, April). The combination of STDP and intrinsic plasticity yields complex dynamics in recurrent spiking networks. In ESANN (pp. 647-652).

Curto, C., & Morrison, K. (2019). Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience. Current opinion in neurobiology, 58, 11-20.
https://www.sciencedirect.com/science/article/pii/S0959438819300443 
Review. Exactly what it should be (networks, motifs, dynamics)

Morrison, A., Aertsen, A., and Diesmann, M. (2007). Spike-timing dependent plasticity in balanced random networks. Neural Computation, 19:1437‚Äì1467. üí°

Ozturk, I., & Halliday, D. M. (2016, December). Mapping spatio-temporally encoded patterns by reward-modulated STDP in Spiking neurons. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series on (pp. 1-8). IEEE. üí°

Encoding innate ability through a genomic bottleneck. Alexei Koulakov, Sergey Shuvaev, Anthony Zador. 2021. https://www.biorxiv.org/content/10.1101/2021.03.16.435261v1

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning. Cerebral cortex, 24 (3), 677-690. üí°

Gilson, M., Burkitt, A., & van Hemmen, J. L. (2010). STDP in recurrent neuronal networks. Spike-timing dependent plasticity, 271.

Naud√©, J., Cessac, B., Berry, H., & Delord, B. (2013). Effects of cellular homeostatic intrinsic plasticity on dynamical and computational properties of biological recurrent neural networks. Journal of Neuroscience, 33(38), 15032-15043.

Gilson M, Burkitt AN, Grayden DB, Thomas DA, van Hemmen JL. Emergence of network structure due to spike-timing-dependent plasticity in recurrent neuronal networks III: Partially connected neurons driven by spontaneous activity. Biol Cybern. 2009;101(5‚Äì6):411‚Äì26.

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning. Cerebral Cortex, 24, 677-690.

Gutig R, Aharonov R, Rotter S, Sompolinsky H. Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity. The Journal of neuroscience: the official journal of the Society for Neuroscience. 2003;23(9):3697‚Äì714. Epub 2003/05/09.

Morrison A, Diesmann M, Gerstner W. Phenomenological models of synaptic plasticity based on spike timing. Biol Cybern. 2008;98(6):459‚Äì78. Epub 2008/05/21.

Fauth, M., & Tetzlaff, C. (2016). Opposing effects of neuronal activity on structural plasticity. Frontiers in neuroanatomy, 10, 75.
https://www.frontiersin.org/articles/10.3389/fnana.2016.00075/full
Review, looking how synapse potentiation (aka "activity") and elimination (aka "structural plasticity") compete and shape micro-connectivity.

Rabinovich, M., Volkovskii, A., Lecanda, P., Huerta, R., Abarbanel, H. D. I., & Laurent, G. (2001). Dynamical encoding by networks of competing neuron groups: winnerless competition. Physical review letters, 87(6), 068102.	
https://pdfs.semanticscholar.org/a0e4/249b0ab6e34699b12d49ad0d0a4c5e121886.pdf

Lagzi, F., & Rotter, S. (2015). Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state. PloS one, 10(9), e0138947.
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138947

Gutkin, B. S., Laing, C. R., Colby, C. L., Chow, C. C., & Ermentrout, G. B. (2001). Turning on and off with excitation: the role of spike-timing asynchrony and synchrony in sustained neural activity. Journal of computational neuroscience, 11(2), 121-134.
Apparently, sorta seminal paper in this narrow field. But check out their later papers as well:
https://scholar.google.com/citations?user=Q2aFVoYAAAAJ&hl=en&oi=sra

# Synfire chains, Sequences

üî• Zheng, P., & Triesch, J. (2014). Robust development of synfire chains from multiple plasticity mechanisms. Frontiers in computational neuroscience, 8, 66.
https://www.frontiersin.org/articles/10.3389/fncom.2014.00066/full

üî• Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of spiking model neurons. Nature Neuroscience, 19(3):350‚Äì355.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4928643/
Sounds like may be a neat review or opinion piece.

Gilra, A. and Gerstner, W. (2017). Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. Elife, 6(e28295):1‚Äì38.

Park, Y., Choi, W., & Paik, S. B. (2017). Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories. Scientific reports, 7(1), 5671.

Lee, H., Choi, W., Park, Y., & Paik, S. B. (2020). Distinct role of flexible and stable encodings in sequential working memory. Neural Networks, 121, 419-429.

Nicola, W. and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE training. Nature Communications, 8(1):1‚Äì15.

Nicola, W. and Clopath, C. (2019). A diversity of interneurons and Hebbian plasticity facilitate rapid compressible learning in the hippocampus. Nature Neuroscience, 22(7):1168‚Äì1181.

Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. Journal of Neuroscience, 33(23):9565‚Äì9575.

Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., and Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101(March):1‚Äì12.

Time perception in brain networks: may be related?
Integrating time from experience in the lateral entorhinal cortex
https://www.nature.com/articles/s41586-018-0459-6
Activity in perceptual classification networks as a basis for human subjective time perception
https://www.nature.com/articles/s41467-018-08194-7
Changing temporal context in human temporal lobe promotes memory of distinct episodes
https://www.nature.com/articles/s41467-018-08189-4
Also a blog post summary: https://www.quantamagazine.org/how-the-brain-creates-a-timeline-of-the-past-20190212/

Bobier, B., Stewart, T. C., & Eliasmith, C. (2014). A unifying mechanistic model of selective attention in spiking neurons. PLoS computational biology, 10(6).
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003577
Model of the cortex.

# Single-neuron calculations

See [[dendritic_comp]]

Abraham, W. C., Jones, O. D., & Glanzman, D. L. (2019). Is plasticity of synapses the mechanism of long-term memory storage?. NPJ science of learning, 4(1), 1-10.
https://www.ncbi.nlm.nih.gov/pubmed/31285847
A review on alternatives (non-synaptic) ways too store memories in the brain.

Dendritic action potentials and computation in human layer 2/3 cortical neurons
Albert Gidon1, Timothy Adam Zolnik1, Pawel Fidzinski2,3, Felix Bolduan4, Athanasia Papoutsi5, Panayiota Poirazi5, Martin Holtkamp2, Imre Vida3,4, Matthew Evan Larkum. Science  03 Jan 2020:
https://science.sciencemag.org/content/367/6473/83
Apparently show that individual cortical piramidal neurons can do XOR.
#dendritic

Unifying Long-Term Plasticity Rules for Excitatory Synapses by Modeling Dendrites of Cortical Pyramidal Neurons
C Ebner, C Clopath, P Jedlicka, H Cuntz - Cell Reports, 2019
https://www.sciencedirect.com/science/article/pii/S2211124719315591
Compartments, NMDA, all sorts of STDP stuff.

# Bio-plausible DL

üü¢ Guerguiev, J., Lillicrap, T. P., & Richards, B. A. (2017). Towards deep learning with segregated dendrites. ELife, 6, e22901. https://elifesciences.org/articles/22901.pdf

Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2019). A solution to the learning dilemma for recurrent networks of spiking neurons. bioRxiv, 738385.
https://www.biorxiv.org/content/10.1101/738385v3
Something like backpropagation in spiking networks.

Lawrence, C., Sztyler, T., & Niepert, M. (2020). Explaining Neural Matrix Factorization with Gradient Rollback. arXiv preprint arXiv:2010.05516.
https://arxiv.org/abs/2010.05516a

üî• Payeur, A., Guerguiev, J., Zenke, F., Richards, B., & Naud, R. (2020). Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.03.30.015511v1

üü¢ Lindsey, J., & Litwin-Kumar, A. (2020). Learning to Learn with Feedback and Local Plasticity. arXiv preprint arXiv:2006.09549.
https://arxiv.org/abs/2006.09549
Also related to "continuing learning"

Sezener, E., Grabska-Barwinska, A., Kostadinov, D., Beau, M., Krishnagopal, S., Budden, D., ... & Latham, P. (2021). A rapid and efficient learning rule for biological neural circuits. bioRxiv.
https://www.biorxiv.org/content/10.1101/2021.03.10.434756v1

Aljadeff, J., D'amour, J., Field, R. E., Froemke, R. C., & Clopath, C. (2019). Cortical credit assignment by Hebbian, neuromodulatory and inhibitory plasticity. arXiv preprint arXiv:1911.00307.
[https://arxiv.org/pdf/1911.00307.pdf](<https://arxiv.org/pdf/1911.00307.pdf>)

G√ºtig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing‚Äìbased decisions. Nature neuroscience, 9(3), 420.

Zenke, F., Poole, B., & Ganguli, S. (2017, August). Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3987-3995). JMLR. org.
https://arxiv.org/pdf/1703.04200.pdf
Abstract synapses (not biological) that somehow prioritize what to learn based on some information? Claim to be biologically inspired. Well cited.

Krieg, D., & Triesch, J. (2014). A unifying theory of synaptic long-term plasticity based on a sparse distribution of synaptic strength. Frontiers in synaptic neuroscience, 6, 3.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941589/
Try to marry Hebbian with gradient descent and something else? Lots of math, not that well cited.

Rolfe, Jason Tyler (2012) Intrinsic Gradient Networks. Dissertation (Ph.D.), California Institute of Technology.
https://thesis.library.caltech.edu/6953/
PhD thesis that was never published on biologically plausible gradient descend in the brain? Check it out.

Kunin, D., Nayebi, A., Sagastuy-Brena, J., Ganguli, S., Bloom, J., & Yamins, D. L. (2020). Two Routes to Scalable Credit Assignment without Weight Symmetry. arXiv preprint arXiv:2003.01513.	
https://arxiv.org/abs/2003.01513
Classical backprop uses same weights there and back; aka "weight symmetry". They seem to claim that by making the rules only slightly non-local, they can solve stuff.
Code: https://github.com/neuroailab/neural-alignment

# DL and RL in the brain

#rl

üü¢ Richards, B. A., & Lillicrap, T. P. (2019). Dendritic solutions to the credit assignment problem. Current opinion in neurobiology, 54, 28-36. http://linclab.org/wp-content/uploads/2020/06/2019-Dendritic-Solutions-to-the-Credit-Assignment-Problem.pdf

Dabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R., & Botvinick, M. (2020). A distributional code for value in dopamine-based reinforcement learning. Nature, 1-5.
https://www.nature.com/articles/s41586-019-1924-6
(obv behind paywall)
DeepMind. Some cover of it:
https://www.technologyreview.com/s/615054/deepmind-ai-reiforcement-learning-reveals-dopamine-neurons-in-brain/

Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., ... & Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21(6), 860.
https://www.nature.com/articles/s41593-018-0147-8

Fremaux N, Sprekeler H, Gerstner W. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS computational biology. 2013;9(4):e1003024.

# DL models of brains

#deepneuro

üî• Yaoda Xu, Maryam Vaziri-Pashkam (2020) Limited correspondence in visual representation between the human brain and convolutional neural networks
https://www.biorxiv.org/content/10.1101/2020.03.12.989376v1 
Essentially, some critique of deepneuro!

https://www.biorxiv.org/content/10.1101/838383v1
Training deep neural density estimators to identify mechanistic models of neural dynamics
Gon√ßalves .. Macke 2019
About how to use deep learning to guess neuronal parameters to fit the actual activity of the network (?) They seem to be looking at actual V(t) though.

Richards, B. A., Xia, F., Santoro, A., Husse, J., Woodin, M. A., Josselyn, S. A., & Frankland, P. W. (2014). Patterns across multiple memories are identified over time. Nature neuroscience, 17(7), 981.
https://www.nature.com/articles/nn.3736

‚ùì Lillicrap, T. P., & Scott, S. H. (2013). Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics. Neuron, 77(1), 168-179.
https://www.sciencedirect.com/science/article/pii/S0896627312009920

Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS computational biology, 10(11), e1003915.
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915
Very well cited, this one.

Kording, K. P., Kayser, C., Einhauser, W., & Konig, P. (2004). How are complex cell properties adapted to the statistics of natural stimuli?. Journal of neurophysiology, 91(1), 206-212.
https://www.ncbi.nlm.nih.gov/m/pubmed/12904330/

Sorscher, B., Mel, G., Ganguli, S., & Ocko, S. (2019). A unified theory for the origin of grid cells through the lens of pattern formation. In Advances in Neural Information Processing Systems (pp. 10003-10013).
https://papers.nips.cc/paper/9191-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation
Supposedly, explains the development of grid cells, synthesizing two existing theories (recurrent with lateral inhibition and spontaneous development during navigation?)

‚ùì Michaels, J. A., Schaffelhofer, S., Agudelo-Toro, A., & Scherberger, H. (2019). A neural network model of flexible grasp movement generation. bioRxiv, 742189.
https://www.biorxiv.org/content/10.1101/742189v1
Modeled vision-controlled grasping (based on monkey data) as convolutional network (based on [[Alexnet]], it seems) ‚Üí RNN (fully connected) ‚Üí bottlneck of 8 neurons ‚Üí another RNN ‚Üí another bottleneck ‚Üí another RNN ‚Üí output. They call these bottlenecks "sparsity", which is interesting. I wonder whether it is fair to call it brain-inspired. Is it something that works, as it forces the learning of representations, and then only claim that it is brain-inspired, to give it some conceptual validity? Or is there actual evidence in the brain that connections between motor systems are throttled like that? Something to explore. Also they mention some "feedback connections" that may be connections between RNN modules, but I'd need to read the methods section carefully to see what's happening there. One of the most important aspects of this paper for me may be the neuro-inspired analysis of model performance that they do, and the very attitude to validation that they have. I wonder if that's something that can be applied to spiking networks.

Doerig, A., Schmittwilken, L., Sayim, B., Manassi, M., & Herzog, M. H. (2019). Capsule Networks as Recurrent Models of Grouping and Segmentation. BioRxiv, 747394.
https://www.biorxiv.org/content/10.1101/747394v2
Something on capsule networks better reproducing vision?

# Memory storage

Goode, T. D., Tanaka, K. Z., Sahay, A., & McHugh, T. J. (2020). An Integrated Index: Engrams, Place Cells, and Hippocampal Memory. Neuron.
https://www.cell.com/neuron/fulltext/S0896-6273(20)30528-6

Masse, N. Y., Yang, G. R., Song, H. F., Wang, X. J., & Freedman, D. J. (2019). Circuit mechanisms for the maintenance and manipulation of information in working memory. Nature neuroscience, 1.
https://www.biorxiv.org/content/biorxiv/early/2018/05/21/305714.full.pdf

Gonz√°lez, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E., & Bazhenov, M. (2020). Can sleep protect memories from catastrophic forgetting?. Elife, 9, e51005.
https://elifesciences.org/articles/51005
Model in which they show that sleep-reply can help to disentangle memories in a recurrent network, improving memory recall.

Dentate gyrus circuits for encoding, retrieval and discrimination of episodic memories. 2020. Thomas Hainmueller & Marlene Bartos. Nature Neuroscience. Review. May be interesting!
https://www.nature.com/articles/s41583-019-0260-z?WT.mc_id=TWT_NatRevNeurosci
It appears that they didn't pre-print it though, so paywalled, at least for now.

Context-modular memory networks support high-capacity, flexible, and robust associative memories. (2020). William F Podlaski,  Everton J Agnes,  Tim P Vogels
https://www.biorxiv.org/content/10.1101/2020.01.08.898528v1
About Hopfield limit. It seems they use an architecture in which large groups neurons or synapses can be modulated (via gating, inhibition) in a context-dependent manner. Show an increase in memory capacity; changes in active components (states), robustness to noise, memory search (??), memory stability. Gating of memories. Substrate for continuous learning? Related to the problem of [[catastrophic_forgetting]], including [this paper](https://www.pnas.org/content/115/44/E10467.short).
There's also a [tweetprint](https://twitter.com/TPVogels/status/1215572496570896384).

# Bottom-up validation

üî• Geiger, F., Schrimpf, M., Marques, T., & DiCarlo, J. (2020). Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.06.08.140111v1
Protoid: Geiger2020ventralstream

Misra, D. (2019). Mish: A Self Regularized Non-Monotonic Neural Activation Function. arXiv preprint arXiv:1908.08681.
https://arxiv.org/abs/1908.08681
Claim that if you make activation function non-monotonic (almost like a sigmoid, but with slight overshoots), it actually helps in some ways.
Github: https://github.com/digantamisra98/Mish
May be a curious parallel to this recent finding in Neuro, about XOR computation by dendrites of an individual cell Gidon, A., Zolnik, T. A., Fidzinski, P., Bolduan, F., Papoutsi, A., Poirazi, P., ... & Larkum, M. E. (2020). Dendritic action potentials and computation in human layer 2/3 cortical neurons. Science, 367(6473), 83-87. (no free pdf)

Lappalainen, J., Herpich, J., & Tetzlaff, C. (2019). A theoretical framework to derive simple, firing-rate-dependent mathematical models of synaptic plasticity. Frontiers in computational neuroscience, 13, 26.
https://www.frontiersin.org/articles/10.3389/fncom.2019.00026/full

Lim, S. et al. Inferring learning rules from distributions of fring rates in cortical neurons. Nat. Neurosci. 18, 1804‚Äì1810 (2015).

Doerig, A., Bornet, A., Choung, O. H., & Herzog, M. H. (2019). Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines. bioRxiv, 744268.
https://www.sciencedirect.com/science/article/pii/S0042698919302299
Seem to be claiming that humans process images fundamentally differently than convolutional networks, because responding to perturbations follows a different logic. Sounds interesting!

Deep neuroethology of a virtual rodent
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence √ñlveczky
https://arxiv.org/abs/1911.09451
Apparently create a vidrual 3D rodent (like, with muscles, joints and what not), make it move in virtual environment, learn to move, then study its network using neuro methods.

Li, Z., Brendel, W., Walker, E., Cobos, E., Muhammad, T., Reimer, J., ... & Tolias, A. (2019). Learning from brains how to regularize machines. In Advances in Neural Information Processing Systems (pp. 9525-9535).
https://arxiv.org/abs/1911.05072

Whiteway, M. R., & Butts, D. A. (2019). The quest for interpretable models of neural population activity. Current opinion in neurobiology, 58, 86-93.

Feather, J., Durango, A., Gonzalez, R., & McDermott, J. (2019). Metamers of neural networks reveal divergence from human perceptual systems. In Advances in Neural Information Processing Systems (pp. 10078-10089).
https://papers.nips.cc/paper/9198-metamers-of-neural-networks-reveal-divergence-from-human-perceptual-systems.pdf
Metameres: in this case, different stimuli that cause identical activation in a part of a network. They seem to be claiming that there's a difference between humans and ANNs here.

Calhoun, A. J., Pillow, J. W., & Murthy, M. (2019). Unsupervised identification of the internal states that shape natural behavior. Nature Neuroscience, 1-10.
https://www.nature.com/articles/s41593-019-0533-x
Use unsupervise learning to identify internal (latent) states in a fly; then correlate these states with activity of individual neurons.

Spoerer, C. J., Kietzmann, T. C., & Kriegeskorte, N. (2019). Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition. bioRxiv, 677237.
https://www.biorxiv.org/content/10.1101/677237v3.full
Recurrent convolutional network  works better than a feed-forward convolutional network. It's slower, but better. Claim that it's similar to primate vision somehow.

Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504. https://www.biorxiv.org/content/10.1101/133504v2.abstract
Overview of how DL may be a model for the brain. At a first glance, seems less convinsing than Kording or Richards tbh.

# Dynamics

üî• Merel, J., Botvinick, M., & Wayne, G. (2019). Hierarchical motor control in mammals and machines. Nature Communications, 10(1), 1-12.
https://www.nature.com/articles/s41467-019-13239-6
Opinion? Seems short and general, so give it priority.

Maheswaranathan, N., Williams, A., Golub, M., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks. In_Advances in neural information processing systems_(pp. 15603-15615).

Recurrence is required to capture the representational dynamics of the human visual system
Kleinman
https://www.pnas.org/content/116/43/21854
Visual representation, clustering objects, cortex, primates

Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales
Kaplan .. Zimmer
https://www.cell.com/neuron/fulltext/S0896-6273(19)30932-8
C elegans, activation dynamics. How fixed action patterns emerge from (are encoded by) the nervous system.

Wilson‚ÄìCowan Equations for Neocortical Dynamics (2016)
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4733815/

# Decision-making

https://www.biorxiv.org/content/10.1101/798553v1 
Recurrent neural network models of multi-area computation underlying decision-making Michael Kleinman, Chandramouli Chandrasekaran, Jonathan C Kao
It seems that they try recurrent networks (RNN) running in parallel, and compare their decision times to that from monkey cortex. Then show that one RNN doesn't match the distribution of times, but if you have several running in parallel, and then synthesizing info, you get similar results? Claim that different modes of distributed computation are experimentally testable like that.

Zoltowski, D. M., Pillow, J. W., & Linderman, S. W. (2020). Unifying and generalizing models of neural dynamics during decision-making. arXiv preprint arXiv:2001.04571.
https://arxiv.org/abs/2001.04571

# Variability, Stochastic Behaviors

#behav

Brembs, B. (2011). Towards a scientific concept of free will as a biological trait: spontaneous actions and decision-making in invertebrates. Proceedings of the Royal Society B: Biological Sciences, 278(1707), 930-939. https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.2325

Noble, R., & Noble, D. (2018). Harnessing stochasticity: How do organisms make choices?. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(10), 106309.
https://aip.scitation.org/doi/full/10.1063/1.5039668#.XuDOjHNMscw.twitter

Murakami, M., Shteingart, H., Loewenstein, Y., & Mainen, Z. F. (2016). Distinct sources of deterministic and stochastic components of action timing decisions in rodent frontal cortex. bioRxiv, 088963.

Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., & Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature neuroscience, 22(12), 2066-2077. https://www.biorxiv.org/content/biorxiv/early/2018/10/11/439885.full.pdf

# CogSci and Development

üî• Chance, F. S., Aimone, J. B., Musuvathy, S. S., Smith, M. R., Vineyard, C. M., & Wang, F. (2020). Crossing the Cleft: Communication Challenges Between Neuroscience and Artificial Intelligence. Frontiers in Computational Neuroscience, 14, 39.
https://www.frontiersin.org/articles/10.3389/fncom.2020.00039/full

Mandler, J. M. (1988). How to build a baby: On the development of an accessible representational system. Cognitive development, 3(2), 113-136. [link](https://s3.amazonaws.com/academia.edu.documents/48154476/How_to_build_a_baby_On_the_development_o20160818-28052-ns7nel.pdf?response-content-disposition=inline%3B%20filename%3DHow_to_build_a_baby_On_the_development_o.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20200223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200223T181627Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11b1618e368d462df1af3b595c4c4c0775001e9a19e536a709c9b6f6d93ca8b1)
Quite popular (500 citations), strongly endorsed by Melanie Mitchell.
And part 2, with even more citations (1.6k):
Mandler, J. M. (1992). How to build a baby: II. Conceptual primitives. Psychological review, 99(4), 587.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.460.5280&rep=rep1&type=pdf

# Other

Piccinini, G., & Shagrir, O. (2014). Foundations of computational neuroscience. Current opinion in neurobiology, 25, 25-30.
(available on academia and researchgate). More of an opinion piece, and philosophy rather than neuro. Discuss things like "What is information?", "What is computation?", does a stone perform computations (kinda?), and other fun stuff. Only 5 pages.

On whether complexity of organisms increases in the course of evolution. #evolution
Krakauer, D., Bertschinger, N., Olbrich, E., Ay, N., & Flack, J. C. (2014). The information theory of individuality. arXiv preprint arXiv:1412.2447.
Also this discussion: https://twitter.com/WiringTheBrain/status/1357004097665306629

https://www.biorxiv.org/content/10.1101/2020.03.24.006775v1
The mouse cortico-tectal projectome

Rahmani, P., Peruani, F., & Romanczuk, P. (2019). Flocking in complex environments--attention trade-offs in collective information processing. arXiv preprint arXiv:1907.11691.
https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1007697
If you have a school of fish, with limited computational capacity, and only some individuals are "informed" (of danger, for example), what would be the optimal flocking strategy? A modeling paper.

Suzuki, M., & Larkum, M. E. (2020). General Anesthesia Decouples Cortical Pyramidal Neurons. Cell, 180(4), 666-676.
https://www.cell.com/action/showPdf?pii=S0092-8674%2820%2930105-7
About how general anesthesia actually retains FF signalling, but disrupts feedback signalling, and it is enough to disrupt consciousness. In all cortical neurons. Seems fun, and  important to understand the logic of cortical processing.

Harnessing behavioral diversity to understand neural computations for cognition Simon Musall, Anne E Urai, David Sussillo, Anne K Churchland
[https://www.sciencedirect.com/science/article/pii/S0959438819300285]
Potentially the most important section: "Relating rich behavior to neural activity by studying ANNs"

Learning predictive structure without a teacher: decision strategies and brain routes
Zoe Kourtzi, Andrew E Welchman
https://www.sciencedirect.com/science/article/pii/S0959438818302393 

The language of the brain: real-world neural population codes J Andrew Pruszynski, Joel Zylberberg
https://www.sciencedirect.com/science/article/pii/S0959438818302137 
Mostly (only?) about motor cortex, and how representation works there. Seems useful and accessible.

Data-driven models in human neuroscience and neuroengineering Bingni W. Brunton, Michael Beyeler. 
A review of current ML solutions for this general problem; still may be useful.

An argument for hyperbolic geometry in neural circuits
Tatyana O Sharpee
Interesting, but not immediately clear. But intriguing. Not much math, so should be very readable.

Mind the last spike ‚Äî firing rate models for mesoscopic populations of spiking neurons Tilo Schwalger, Anton V Chizhov
https://www.sciencedirect.com/science/article/pii/S095943881930039X 
Review

https://www.biorxiv.org/content/10.1101/837567v1 
Interrogating theoretical models of neural computation with deep inference
Bittner .. Cunningham 2019
Apparently a very similar paper (to Goncalves 2019), but independent.

Balleine, B. W. (2019). The Meaning of Behavior: Discriminating Reflex and Volition in the Brain. *Neuron*, *104*(1), 47-62. 
Review. Potentially an interesting paper on free will, behaviorally (and computationally?) defined.

Classes of dendritic information processing Alexandre Payeur, Jean-Claude B√©√Øque, Richard Naud

Constraining computational models using electron microscopy wiring diagrams Ashok Litwin-Kumar, Srinivas C Turaga

Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2, 4.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/
1.5k citations; seminal conceptual work apparently. Important for ML?

# ML of brain data

.
