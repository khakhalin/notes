# Courses, books, and links
#book

# Key resources
* http://www.arxiv-sanity.com/ - AI search for arxiv

# Books

Given in the subjective order of my gradual reading.

* How to think like a computer scientist by Allen B. Downey: ([site](https://greenteapress.com/wp/think-python-2e/)) - best intro to Python ever
* VLMS (Introduction to Applied Linear Algebra) by Stephen Boyd. [full pdf](http://vmls-book.stanford.edu/)
* Algorithms by Sedgewick - uses Java, but is exceptionally clearly written
* ISLR: Introduction to Statistical Learning with R, by James, Witten, Hastie, Tibshirani. ([web with pdf](http://faculty.marshall.usc.edu/gareth-james/ISL/)). See this for a fan-made translation of the code Python: [JW](https://github.com/JWarmenhoven/ISLR-python).
* Hands-On M with Scikit-Learn, Keras, and TensorFlow, by Aurélien Géron. Python obviously. [All labs on github](https://github.com/ageron/handson-ml2), but no full version of the book online.
* "The 100 pages ML book" by Andriy Burkov. [full pdf](http://themlbook.com/wiki/doku.php) 
* MML (Mathematics for Machine Learning) by M.P. Deisenroth: [full pdf](https://mml-book.github.io/)
* Programming in Python: Fluent Python, by Luciano Ramalho (no online version)
* Deep Learning by Ian Goodfellow et al. [full pdf](http://www.deeplearningbook.org/)

Second set, to advance, or get a new perspective:
* ESL (The Elements of Statistical Learning) by Hastie, Tibshirani, Friedman. ([full pdf](https://web.stanford.edu/~hastie/ElemStatLearn/)) A more encyclopaedic version of ISLR: extensive, no code, and with most proofs and derivations "left to the reader" ([ref](https://www.quora.com/How-do-I-learn-the-book-Elements-of-Statistical-Learning-What-books-materials-would-help-beef-up-my-foundations-so-that-I-will-be-able-to-comprehend-the-book-easily)). Great book to revisit and integrate the material you already know.
* On Programming Well, by Robert C. Martin (no online version)
* Speech and Language Processing by D. Jurafsky & JH Martin ([draft pdf](https://web.stanford.edu/~jurafsky/slp3/))
* Pattern recognition and ML by Christopher Bishop ([full pdf](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)) - Has a more Bayesian perspective. [This set of notebooks](https://github.com/ctgk/PRML) implements this book, chapter by chapter.
* [Dive into Deep Learning](http://d2l.ai/) - an interactive introduction to deep learning, based on NumPy, apparently!
* Think Complexity, by Allen B. Downey ([full pdf](https://greenteapress.com/wp/think-complexity-2e/)) - network sci, automata, all in Python
* Foundations of ML by M. Mohri et al ([full pdf](https://cs.nyu.edu/~mohri/mlbook/)) - more mathy?
* ML book by K.P. Murphy - ([site](https://www.cs.ubc.ca/~murphyk/MLbook/), but no full pdf)
* Non-linear Dynamics and Chaos, by Steven Strogatz ([full pdf](http://www.hds.bme.hu/~fhegedus/Strogatz%20-%20Nonlinear%20Dynamics%20and%20Chaos.pdf))
* Morgan, S. L., & Winship, C. (2015). Counterfactuals and causal inference (pdfs are googlable)
* [Notes on dynamical systems](https://people.maths.bris.ac.uk/~macpd/ads/) ([pdf](https://people.maths.bris.ac.uk/~macpd/ads/bnotes.pdf)) from Carl Dettmann. Not a textbook, but close; worth scanning
* Bayesian Statistics: Statistical Rethinking, by Richard McElreath
* Artificial Intelligence: A Modern Approach, by Russel and Norvig. ([site](http://aima.cs.berkeley.edu/), but no online)

Refresher books
* Applied Predictive Modeling by Kuhn Johnson (no free pdf online). Code in R.

Third priority books:
* Algorithms by Erickson
* Design Patterns for Python by Norvig: https://norvig.com/design-patterns/

Books that are not on my reading list, but keeping here just in case:
* Problem solving with data structures in Python ([full text, web](https://runestone.academy/runestone/books/published/pythonds/index.html))
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) by Michael Nielsen
* Competitive Programmer's Handbook by Antti Laaksonen: [pdf](https://cses.fi/book/book.pdf) (C++) - great idea, but the descriptions of algorithms are not clear, while implementations seem ideosyncratic…

Coding problems
* https://www.byte-by-byte.com/google-interview/
* https://leetcode.com/
* https://codingbat.com/java
* https://codegolf.stackexchange.com/

Job-search specific materials:
* Cracking the code interview (a book)
* [[Huyen2019book]] - a short book on data science / ML job interviewing
* [Numbeo "cost of living" indices](https://www.numbeo.com/cost-of-living/rankings.jsp) - to scale salary expectations for the location
* See also: [[ml_questions]] - an unstructured list of random ML and math questions

# Courses
* [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/) - a really good introduction for complete noobs 
* [Stanford, linear dynamical systems](http://stanford.edu/class/ee363/lectures.html) (Stephen Boyd) - includes stochastic control, Kalman filter, Lyapunov theory
* [Linear algebra by Gilbert Strang](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8) - Youtube from MIT. Everybody loves it, as it achieves a beautiful balance between staying extremely practical, concise, but at the same time giving many proofs (or "almost proofs", extreme-physics style, like for good cases, with simplifying assumptions etc). MIT also has [notes for each lecture](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/syllabus/) (not in one pdf, but easy to find).
* [Matrix methods in data science](https://www.youtube.com/watch?v=Cx5Z-OslNWE&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k) (also Gilbert Strang)- YouTube from MIT. May be good for refreshing things.
* Good collection of short tutorials (both ML and snippets of Python code) by Chris Albon: https://chrisalbon.com/
* [Maximum Entropy Network Ensembles](http://www.maths.qmul.ac.uk/~gbianconi/LTCCModule) - a series of lectures
* [End-to-end machine learning](https://end-to-end-machine-learning.teachable.com) - sorta like a course from Brandon Rohrer
* https://github.com/hsayama/PyCX - a set of notebooks on complexity (looks like weekly labs?)

# Link aggregators

* [A very nice annotated list of book recommendations](https://towardsdatascience.com/beyond-the-mooc-a-bookworms-guide-to-data-science-e87271cb0572)
* [Machine learning for humans, 80/20 reading list](https://medium.com/machine-learning-for-humans/ai-reading-list-c4753afd97a) - claims to be curated. Sections on ML, Deep, RL, and then various general readings about the meaning of it all.
* [Scientific visualization with matplotlib](https://github.com/rougier/scientific-visualization-book) - an open-source book project in the works that hadn't been released yet, but the previews look lovely

# Temp teaching links
Three books:
* Think Python: https://greenteapress.com/wp/think-python-2e/
* Think Complexity: http://greenteapress.com/complexity2/thinkcomplexity2.pdf
* Modeling and simulation in Python: https://greenteapress.com/wp/modsimpy/

On agent-based modeling, from "The Nature of Code" by Daniel Shiffman:
* [Chapter 4, on particle systems](https://natureofcode.com/book/chapter-4-particle-systems/)
* [Chapter 6, on autotonomous agents](https://natureofcode.com/book/chapter-6-autonomous-agents/) (he uses steering vehicles as an example)

Other materials:
* An introduction into the logic (not formulas or programming, but the ideas) of agent-based modeing (a short chapter by Nigel Gilbert): http://epubs.surrey.ac.uk/1580/1/fulltext.pdf
* [Good exploration of logistic map and chaos](https://geoffboeing.com/2015/03/chaos-theory-logistic-map/)
* [A collection of Jupyter models on topics of complexity](https://github.com/hsayama/PyCX)
* Examples of good student projects from Downey's course: [1](https://github.com/kdy304g/ComplexLizards-CA/blob/master/reports/final_report.md), [2](https://github.com/jzerez/swarm_classification/blob/master/reports/Final_Report.md)