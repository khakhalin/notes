# Texts and Language
#text

**Toolbox of methods and concepts:**
* [[ngram]] - N-grams: low-level feature for text analysis, beyond single words. "Bags" of several (2-3) words.
* [[tfidf]] - Text Frequency - Inverse Document Frequency. A basic ranking approach for text relevance.
* [[perplexity]] - main measure of language model quality, as well as a great objective function
* [[beam_search]] - a way to go beyound a level of "one word at a time" by tree exploration
* [[stupid_back_off]] - a simplistic Markovian approach to text generation

# To read:
https://ruder.io/unsupervised-cross-lingual-learning/index.html

https://ruder.io/state-of-transfer-learning-in-nlp/index.html

https://www.aclweb.org/anthology/P19-1334/ 

Two posts about BERT by Jesse Vig:
* https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8
* https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1


McCoy, R. T., Pavlick, E., & Linzen, T. (2019). Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. arXiv preprint arXiv:1902.01007.
https://www.aclweb.org/anthology/P19-1334/
Criticism of DL text prediction models.
https://thegradient.pub/nlps-clever-hans-moment-has-arrived/