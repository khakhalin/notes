# Regression

## Linear Regression
**L2 loss**: mean squared distance. Nice illustration from [Google crash course](https://developers.google.com/machine-learning/crash-course/): 2 outliers of 2 are worse than 4 outliers of 1, as 4<8. Real sensitive to outliers.

**Hyperparameters**: those somewhat arbitrary values that define the type of solution the model is looking for, and the process of descent. Examples: learning rate, batch size.

**Learning rate**: Goldilocks principle - the best learning rate should "magically" put you in the minimum in a very few steps. Large learning rate leads to noisy oscillations after what looked like a convergence. It may even break everything (unstable).

**Mini-batch**: process >1 (usually 10-1000) points at a time. Somewhere in between fully stochastic descent (1 point at a time) and math-optimal (all points every time).

h(x) = estimation for y = ∑θ_i x_i = θ'∙x . Here we assume that x is a vector length n+1: x_0 = 1 (intercept), followed by n variables. Introduce loss: J(θ) = 1/2∙∑(h-y)^2 across all points i. Now minimize it by differentiating by θ (partial derivative for each of the coordinates). For one point:
∂J(θ)/∂θ_j = definition ∂/∂θ_j 1/2 (h(θ,x)-y)^2 = simple chain rule (h(θ,x)-y) ∙ ∂/∂ h = by formula for h = (h-y)∙x_j . Now we can update θ by gradient descent by doing θ := θ + α(h-y)x . As this loss is a convex quadratic function, it has only one minimum, and so always converges.

## Bias-Variance Tradeoff

An estimate for the number of training examples needed ot learn problems of certain dimensionality.

Generalization error consists of two parts: bias and variance.

**Derivation for a linear system:** (ref: [Some lecture](https://www.youtube.com/watch?v=zUJbRO0Wavo) by Kilian Weinberger)

Say, regression x→y on a dataset (x,y) generated by a distribution P(x,y).
For a given x, we can have P(y|x), and P(x,y) = ∫ P(y|x)P(x) by x.
Regression predicts expected y(x), or Ey (x). 
By definition Ey (x) = ∫y P(y|x) , integrated across all possible x.
ML is about guessing (predicting) E y(x) well using some algorithm.
Say, h(x) is our guess for Ey (x), produced by some method A, based on a training set D.
The expected error for this estimation h is: E (h-y)^2 = ∬ P(x,y)(h(x)-y)^2 by both x and y.
Now, h itself is a random variable, as it's a function of D, which is a random sample from (x,y).
So we can consider expected value: Eh, by integrating over all possible training sets D.
We can estimate it by training lots of times and averaging. Or, if you are only interested in E(h), you could also just apply A to the full dataset, obtaining the best h possible.
Now, the expected error for the algorithm (across all possible classifiers it could possibly produce) is the error integrated over all possible h (and so, over all Ds), so we have a triple-integral (by D, x, y):
$\int_D \int_x \int_y (h(x)-y)^2 P(D) P(x,y)\,dD\,dx\,dy$
Add and subtract Eh (mean h) inside the square; get (h-Eh) and (Eh-y); open squares.
(h-Eh)^2 becomes variance.
2(h-Eh)(Eh-y) dies out because for each fixed x and D you get a fixed (Eh-y), and int_D (h-Eh) = 0 by def of Eh.
The term (Eh-y)^2 will eventually become bias, but it needs some massaging first. Similar to what we did before, subtract and add Ey, then open squares.
We're left with (Eh-Ey)^2 and (Ey-y)^2. The term 2(Eh-Ey)(Ey-y) dies off at int_y step. 
(Just make sure to integrate by y at the deepest level, to get 0 before int_x has a chance to see it).
So we ended up having 3 terms:
* Variance of the classifier: (h-Eh)^2
* Bias: (Eh-Ey)^2
* Variance of the data, aka noise, or irreducable error: (Ey-y)^2

> **My current understanding** is that it is actually close to the precision / recall situation. The point is that if you try to minimize bias (make your classifier cling to the data), you fall at mercy of your training set, and so increase variance. Each particular classifier, understood as an instance produced by some particular set of training data, will cling to this testing data, so all classifiers will be slightly different, if you train on different data. If you believe that the underlying solution has some constraints to it, you may want to restrain your classifier, even at cost of accepting higher bias (Eh-Ey), so that it would not change that widely for different subsets of your testing data. Regularization does that. Then for each given training set you'll get higher bias, but you'll reduce variance (of your classifier, across all possible training set), as all models will be more similar to each other, and hopefully also closer to the "ground truth". Essentially, a parsimony principle: an assumption that simpler models are better.

> So it's directly related to overfitting. Low bias = high variance = great fit on the training set, but horrible fit on te testing set.

> But look, it would only work if the ground trugh is actually simple. Why does it work? Why is the ground truth actually simple? Is it some expected statistical property that the world actually follows?.. Or is it because a typical practical problem has certain properties? Seems like that; see below.

## Ridge regression

Seems to be an old name for **L2 regularization** (see DL chapter). Another name: **Tikhonov regularization**. 

Motivation: Consider Ax = b, and x doesn't exist. In a most typical practical case, it gives a superposition of an overdetermined problem Ax' = b for the component of x that is in the row-space, and so is affected by the matrix (x' = projection of x into row-space), and an underdetermined problem A(x-x') = 0 for the components x-x' that are in the null-space of A, and so can be chosen arbitrary without any effect on Ax. Everything that in the null-space cannot bring Ax closer to b, so we are free to pick them in some "nice" fashion, for example, to satisfy some priors, or minimize complexity.

With Tikhonov regularization, we minimize not squared Euqledian norm |Ax-b|^2 but |Ax-b|^2 + |Γx|^2 where Γ is some matrix; often identity matrix I multiplied by a coefficient, in which case we just have a sum of squared x_i (L2 regularization) multiplied by a coeff (usually denoted k). 

This is especially important in case of **Multicollinearity**, when you're trying to predict y from many variables a_i, in a way Ax = y (observations of variables a_i for different training points become columns of A, while regression coefficients form x), but some of a_i are strongly correlated. In this case trying to painfully minimize y-Ax would be counter-productive, as we'd fit noise in y with noise in a_i. Imagine an extreme case: all columns of A are the same (rank=1), but are observed with noise, and noise is independent (so formally rank = N). What we actually need is only one (doesn't matter which one) x_i>0, and all others 0. But what will happen, is that we'll fit noise in y with noise in A.

The name "ridge" comes from a visual example of what is describe above. Imagine that only part of the solution is well defined, and the rest is close ot null-space of A. Then the "true solution" is a "generalized cylinder" made by the true solution (in those coordinates that make sense), arbitrarily extended across the "irrelevant coordinates". Small changes in training data (right side of the Ax=y equation) would sway the solution along this "ridge". By adding regularization we change a "ridge" into a "peak" (lines turn into parabolas), which stabilizes the solution to perturbations in both A and y. ([Source: stackexchange](https://stats.stackexchange.com/questions/118712/why-does-ridge-estimate-become-better-than-ols-by-adding-a-constant-to-the-diago/119708#119708))

How to pick the coefficient k? One method: **Ridge trace**: plot found coefficients as a function of k, and eyeball value at which they stop oscillating, and start converging (not in the sense of becoming const, but in the sense of of monotone almost-linear change).

## Logistic Regression

**Logistic function**: $\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{1+e^x}$.

The inverse function is called **logit**: logit(p) $=\ln \frac{p}{1-p}$

Assumes that **log-odds** $\log \frac{p}{1-p}=a+bx$ , so is a linear function of x (that is generally a vector). It means that p/(1-p)=exp(ax+bx), which leads to p = σ(ax+b). It can be said that a logistic regression is just a linear regression of log-odds.

To find a solution, minimize **logloss**: Loss = $-\sum (y\cdot\log(\tilde y)+(1-y)\cdot\log(1-\tilde y))$, or, using p for y estimations, -∑( y∙log(p) + (1-y)∙log(1-p) ). As p→0, -log(p)→inf, so huge punishment for near-zero p. Because of that, if the data is too clean and some areas contain only points of one type, weights may explode (it's hard to fit infinity), making proper regularization extremely important.

For high-D non-linear data, works great if features cross-products are included into the set as synthetic features (see [[07_Features]]).

## Stepwise Regression
https://en.wikipedia.org/wiki/Stepwise_regression
More of a statistical procedure (not sure if even applicable to big data): automated selection of features based on the quality of fit, and a complexity penalty (regularization). Two main approaches: Forward selection and Backward selection (or a combination of both).
See also: [[Guyon2003variable]].

The maxim about stepwise regression is that it is useless for hypothesis testing, OK but questionable for prediction, and good for variable selection (see [[07_Features]]). 
* It can't be used for hypotheses, as p-values after this procedure are meaningless, and arguably shouldn't even be reported, as their direct probabilistic intepretation is impossible, and adjustment is too complicated (p-values aren't independent, so you cannot do FDR), making adjustment impossible in practice.
* It can be used for prediction, but usually full models (or, assumably, probably regularized models) are more powerful anyways.
* It's an OK method to select variables tho.

Some people (like Thompson cited below) are really vehemently against them tho. That's because they are usually too optimistic (the very concept of degrees of freedom becomes problematic after multiple testing), yet are strictly less powerful than proper regularization, and also typically don't replicate, as the sequence of descent depends on the data, so performing stepwise regression on different subsets of data is much more likely to yield different results, compared to a more sophisticated model.

Refs:
* Steyerberg, E. W., Eijkemans, M. J., Harrell Jr, F. E., & Habbema, J. D. F. (2001). Prognostic modeling with logistic regression analysis: in search of a sensible strategy in small data sets. Medical Decision Making, 21(1), 45-56.
* Thompson, B. (2001). Significance, effect sizes, stepwise methods, and other issues: Strong arguments move the field. The Journal of Experimental Education, 70(1), 80-93. 
