# Linear algebra, Basics

## Linear and affine

**Linear** means f(ax) = a f(x), and f(a+b) = f(a)+f(b). Examples of linear: negation, reversal, running sum, demeaning (subtract mean(x) from all x_i elements of x).

*Th*: If f(x) is linear, it can be represented as a matrix multiplication Ax
Proof: x = sum x_i e_i , and as f is linear, f(x) = sum x_i f(e_i) where f(e_i) is in out-space (codomain) of f. If we write coordinates of each f(e_i) in this space, we get a matrix with columns of f(e_i), and f(x) = Ax. QED 

What most people think of as "linear" is actually **affine**: Ax+b (linear + const). This representation is unique, and A = (f(x)-f(0)) - because -f(0) kills the "b" part, and then we have f(x)-b linear, so see above.

Examples:
* Cumulative sum, as applied to a vector length n, and producing vector lengths n. A lower triangular matrix of ones.
* Difference matrix: the opposite of that, and not square, as takes n-long x, and outputs n-1-long with elements x2-x1 etc. Elements of this matrix are like -1 1 shaped into a diagonal, which fits fine as the matrix has a shape of (n-1)n.
* First-order Taylor approximation: f(x) ~ f(x0) + Df(x0)(x-x0) where D = Jacobian, or matrix of all possible derivatives of coordinates of f over coordinates of x: Df_ij = df_i/dx_j . Obviously affine.

### Linear equations
Ax = b

Interesting examples:
* Ax = b means that b is a linear combination of columns in A. If x like that exists, then b can be represented via columns of A.
* **Polynomical interpolation**: $y_i = \sum c_k x_i^k$ . If we consider a matrix A with elements a_ij = x_i^j , it turns into a matrix equation. This A is called a *Vandermonde matrix*.

**Markov model**, aka linear dynamics model : states in next moment of time x(t+1) = Ax(t). A is a matrix of transitions. (There may also be an input with matching dimensions). Example of a model: pandemics (not as a network model, but a basic model of transitions: health -> infected -> either recovered or dead).

Same approach can be used to numerically solve DE, as x = x + x'dt, so A = (1 dt ; 0 1) where dt is an integration step for Euler integration; would take (x, dx/dt) and produce a new pair.

_Th:_ A composition f(g(x)) of two linear functions is also linear. That's obvious.
_Th:_ A composition of affine, is also affine. Also obvious.

Example: **Second difference matrix**: a multiplication of two different matrices D_{n-1}D_n . Matrix form is obvious from matrix multiplication. Similarly, chain rule: if h=f(g(x)) and everything is multivariable, then $dh_i/dx_j =  \sum_k dh/dy_k (g(x)) \, dg_k/dx_j$ that totally looks like a vector (or rather matrix, as it's true for every i) equation. A matrix of all possible derivatives of elements of f alogn dimensions of g is multiplied by a matrix of all possible coordinates of g over dimensions of x. Dh() = Df(g()) Dg(). 
If h() yields a scalar, a vector of dh/dy is just gradient ∇h, so we get ∇h = (∇f^T ∇g)^T = ∇g^T ∇f.
If g() is affine Ax+b, then ∇g = $A^\top$, and so ∇h = $A^\top$∇f(). (vlms p. 185)

Note: how come ∇(Ax) = A^T? Because here we actually have to assume a certain **Layout convention** ([wiki](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)): namely what happens if y and x are vectors, and we're considering $\partial y/\partial x$. For consistency, a derivative of scalar by a vector, and a derivative of vector by a scalar should have opposite orientations (if one is row, the other one shoudl be a column). If you want gradient of a scalar be a normal vector (column), it's called _denominator layout_, or _Hessian formulation_. But then derivative of a vector by scalar is transposed compared to the vector. If on the other hand you want the devative of a vector have the same shape as the vector, then it's called _numerator layout_, or _Jacobian formulation_. Then gradient is a flipped vector.

If your matrix of derivatives y over x looks like $d_{ij} = \frac{\partial y_i}{\partial x_j}$ then d/dx(Ax) = $A^\top$ , meaning that it's a denominator layout. vlms uses this layout.

