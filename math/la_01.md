# Linear algebra, Basics

## Linear and affine

**Linear** means f(ax) = a f(x), and f(a+b) = f(a)+f(b). Examples of linear: negation, reversal, running sum, demeaning (subtract mean(x) from all x_i elements of x).

*Th*: If f(x) is linear, it can be represented as a matrix multiplication Ax
Proof: x = sum x_i e_i , and as f is linear, f(x) = sum x_i f(e_i) where f(e_i) is in out-space (codomain) of f. If we write coordinates of each f(e_i) in this space, we get a matrix with columns of f(e_i), and f(x) = Ax. QED 

What most people think of as "linear" is actually **affine**: Ax+b (linear + const). This representation is unique, and A = (f(x)-f(0)) - because -f(0) kills the "b" part, and then we have f(x)-b linear, so see above.

Examples:
* Cumulative sum, as applied to a vector length n, and producing vector lengths n. A lower triangular matrix of ones.
* Difference matrix: the opposite of that, and not square, as takes n-long x, and outputs n-1-long with elements x2-x1 etc. Elements of this matrix are like -1 1 shaped into a diagonal, which fits fine as the matrix has a shape of (n-1)n.
* First-order Taylor approximation: f(x) ~ f(x0) + Df(x0)(x-x0) where D = Jacobian, or matrix of all possible derivatives of coordinates of f over coordinates of x: Df_ij = df_i/dx_j . Obviously affine.

### Examples of linear equations
Ax = b

Interesting examples:
* Ax = b means that b is a linear combination of columns in A. If x like that exists, then b can be represented via columns of A.
* **Polynomial interpolation**: $y_i = \sum c_k x_i^k$ . If we consider a matrix A with elements a_ij = x_i^j , it turns into a matrix equation. This A is called a *Vandermonde matrix*.

**Markov model**, aka linear dynamics model : states in next moment of time x(t+1) = Ax(t). A is a matrix of transitions. (There may also be an input with matching dimensions). Example of a model: pandemics (not as a network model, but a basic model of transitions: health -> infected -> either recovered or dead).

Same approach can be used to numerically solve DE, as x = x + x'dt, so A = (1 dt ; 0 1) where dt is an integration step for Euler integration; would take (x, dx/dt) and produce a new pair.

_Th:_ A composition f(g(x)) of two linear functions is also linear. That's obvious.
_Th:_ A composition of affine, is also affine. Also obvious.

Example: **Second difference matrix**: a multiplication of two different matrices $D_{n-1}D_n$ . The shape of this matrix is obvious from matrix multiplication. Consider chain rule for h=f(g(x)) , where everything is multivariable. Then $dh_i/dx_j =  \sum_k dh/dy_k (g(x)) \, dg_k/dx_j$ , which totally looks like a vector equation. Or rather a matrix, as this is true for every coordinate i . A matrix of all possible derivatives of elements of f, taken along each of the dimensions (coordinates) of g, is multiplied by a matrix of all possible coordinates of g, taken over dimensions of x. Dh() = Df(g()) Dg().

If h() yields a scalar, then a vector of dh/dy is just the gradient ∇h, in which case we get:
∇h = (∇f^T ∇g)^T = ∇g^T ∇f.

If g() is affine Ax+b, then ∇g = $A^\top$, and so ∇h = $A^\top$∇f(). (vlms p. 185)

Note: how come ∇(Ax) = A^T? Because to use derivative in a vector space, we actually have to assume a certain **Layout convention** ([wiki](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)): namely what happens if y and x are vectors, and we're considering ∂y/∂x. For consistency, a derivative of scalar by a vector (gradient), and a derivative of vector by a scalar (like, speed?) should have opposite orientations (if one is a row, the other one should be a column). Or you won't be able to work with vector fields (as one way or another, you need to get a matrix when you differentiate vector by a vector). If you want gradient of a scalar be a normal vector (column), it's called _denominator layout_, or _Hessian formulation_. But then derivative of a vector by scalar is transposed compared to the vector, which feels super-wrong (only think of speed!). If on the other hand you want the devative of a vector have the same shape as the vector itself, then it's called _numerator layout_, or _Jacobian formulation_. Then gradient is a flipped vector (row), making  ∇^T a normal vector (column-vector). As a consequence, it gives ∇(Ax) = A^T. And if your matrix of derivatives y over x looks like $d_{ij} = \frac{\partial y_i}{\partial x_j}$ then ∂/∂x(Ax) = $A^\top$ , meaning that it's a denominator layout. VLMS book uses this layout (ref: wiki)

## QR factorization

**Gram matrix** or **Gramian**: for a set of vectors {v_i}, a matrix of all possible inner products: G_ij = v_i ∙ v_j. Sometimes people use the word "Gramian" for the determinant of this matrix though. (ref: wiki)
