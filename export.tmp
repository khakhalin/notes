# Courses, books, and links


# Key resources
* http://www.arxiv-sanity.com/ - AI search for arxiv

# Books

(Given in subjective order of my gradual reading)
* How to think like a computer scientist by Allen B. Downey: ([site](https://greenteapress.com/wp/think-python-2e/)) - best intro to Python ever
* VLMS (Introduction to Applied Linear Algebra) by Stephen Boyd. [full pdf](http://vmls-book.stanford.edu/)
* Algorithms by Sedgewick - uses Java, but is exceptionally clearly written
* ESL (The Elements of Statistical Learning) by Hastie, Tibshirani, Friedman. ([full pdf](https://web.stanford.edu/~hastie/ElemStatLearn/)) Very extensive, no code, most math is stated but not derived, poor exposition. If I could go back in time, I'd bumped ISLR and "Hands-On ML in Python" to this position, and moved ESL to the "Deeper Refresher" section ([ref](https://www.quora.com/How-do-I-learn-the-book-Elements-of-Statistical-Learning-What-books-materials-would-help-beef-up-my-foundations-so-that-I-will-be-able-to-comprehend-the-book-easily)). It's probably a great book to revisit and integrate the material you already know.
* "The 100 pages ML book" by Andriy Burkov. [full pdf](http://themlbook.com/wiki/doku.php) 
* Deep Learning by Ian Goodfellow et al. [full pdf](http://www.deeplearningbook.org/)
* Programming in Python: Fluent Python, by Luciano Ramalho (no online version)
* MML (Mathematics for Machine Learning) by M.P. Deisenroth: [full pdf](https://mml-book.github.io/)
* Hands-On M with Scikit-Learn, Keras, and TensorFlow, by Aurélien Géron. Python obviously. [All labs on github](https://github.com/ageron/handson-ml2), but no full version of the book online.
* Speech and Language Processing by D. Jurafsky & JH Martin ([draft pdf](https://web.stanford.edu/~jurafsky/slp3/))

Second priority books:
* On Programming Well, by Robert C. Martin (no online version)
* Pattern recognition and ML by Christopher Bishop ([full pdf](https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/)) - apparently, has a more Bayesian perspective
* [Dive into Deep Learning](http://d2l.ai/) - an interactive introduction to deep learning, based on NumPy, apparently (no TF, no PyTorch, ha! Is it true?) 
* Think Complexity, by Allen B. Downey ([full pdf](https://greenteapress.com/wp/think-complexity-2e/)) - network sci, automata, all in Python
* Foundations of ML by M. Mohri et al ([full pdf](https://cs.nyu.edu/~mohri/mlbook/)) - more mathy?
* ML book by K.P. Murphy - ([site](https://www.cs.ubc.ca/~murphyk/MLbook/), but no full pdf)
* Morgan, S. L., & Winship, C. (2015). Counterfactuals and causal inference (pdfs are googlable)
* [Notes on dynamical systems](https://people.maths.bris.ac.uk/~macpd/ads/) ([pdf](https://people.maths.bris.ac.uk/~macpd/ads/bnotes.pdf)) from Carl Dettmann. Not a textbook, but close; worth scanning
* Bayesian Statistics: Statistical Rethinking, by Richard McElreath
* Artificial Intelligence: A Modern Approach, by Russel and Norvig. ([site](http://aima.cs.berkeley.edu/), but no online)

Refresher books
* ISLR: Introduction to Statistical Learning with R, by James, Witten, Hastie, Tibshirani - seems like a more practical and simpler version of ESLII? ([web with pdf](http://faculty.marshall.usc.edu/gareth-james/ISL/)). There are some fan-made translations of the code from R to Python: [JW](https://github.com/JWarmenhoven/ISLR-python).
* Applied Predictive Modeling by Kuhn Johnson (no free pdf online). Code in R.

Third priority books:
* Algorithms by Erickson
* Design Patterns for Python by Norvig: https://norvig.com/design-patterns/

Books that are not on my reading list, but keeping here just in case:
* Problem solving with data structures in Python ([full text, web](https://runestone.academy/runestone/books/published/pythonds/index.html))
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html) by Michael Nielsen
* Competitive Programmer's Handbook by Antti Laaksonen: [pdf](https://cses.fi/book/book.pdf) (C++) - great idea, but the descriptions of algorithms are not clear, while implementations seem ideosyncratic...

Links for the computation course I'm teaching this spring:
* Modeling and Simulation in Python, by Allen B. Downey: [link](https://greenteapress.com/wp/modsimpy/)
* Examples of good student projects from Downey's course: [1](https://github.com/kdy304g/ComplexLizards-CA/blob/master/reports/final_report.md), [2](https://github.com/jzerez/swarm_classification/blob/master/reports/Final_Report.md)

Job-search specific materials:
* Cracking the code interview
*  - a short book on data science / ML job interviewing
* [Numbeo "cost of living" indices](https://www.numbeo.com/cost-of-living/rankings.jsp) - to scale salary expectations for the location

# Courses
* [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/) - a really good introduction for complete noobs 
* [Stanford, linear dynamical systems](http://stanford.edu/class/ee363/lectures.html) (Stephen Boyd) - includes stochastic control, Kalman filter, Lyapunov theory
* [Linear algebra by Gilbert Strang](https://www.youtube.com/playlist?list=PLE7DDD91010BC51F8) - Youtube from MIT. Everybody loves it, as it achieves a beautiful balance between staying extremely practical, concise, but at the same time giving many proofs (or "almost proofs", extreme-physics style, like for good cases, with simplifying assumptions etc). MIT also has [notes for each lecture](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/syllabus/) (not in one pdf, but easy to find).
* [Matrix methods in data science](https://www.youtube.com/watch?v=Cx5Z-OslNWE&list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k) (also Gilbert Strang)- YouTube from MIT. May be good for refreshing things.
* [Maximum Entropy Network Ensembles](http://www.maths.qmul.ac.uk/~gbianconi/LTCCModule) - a series of lectures
* [End-to-end machine learning](https://end-to-end-machine-learning.teachable.com) - sorta like a course from Brandon Rohrer
* https://github.com/hsayama/PyCX - a set of notebooks on complexity (looks like weekly labs?)

**Systems design**
* https://github.com/donnemartin/system-design-primer
* https://github.com/checkcheckzz/system-design-interview

# Link aggregators

* [A very nice annotated list of book recommendations](https://towardsdatascience.com/beyond-the-mooc-a-bookworms-guide-to-data-science-e87271cb0572)
* [Machine learning for humans, 80/20 reading list](https://medium.com/machine-learning-for-humans/ai-reading-list-c4753afd97a) - claims to be curated. Sections on ML, Deep, RL, and then various general readings about the meaning of it all.
* [Scientific visualization with matplotlib](https://github.com/rougier/scientific-visualization-book) - an open-source book project in the works that hadn't been released yet, but the previews look lovely

# Temp teaching links
Three books:
* Think Python: https://greenteapress.com/wp/think-python-2e/
* Think Complexity: http://greenteapress.com/complexity2/thinkcomplexity2.pdf
* Modeling and simulation in Python: https://greenteapress.com/wp/modsimpy/

Other materials:
* An introduction into the logic (not formulas or programming, but the ideas) of agent-based modeing (a short chapter by Nigel Gilbert): http://epubs.surrey.ac.uk/1580/1/fulltext.pdf
* [Good exploration of logistic map and chaos](https://geoffboeing.com/2015/03/chaos-theory-logistic-map/)
* [A collection of Jupyter models on topics of complexity](https://github.com/hsayama/PyCX)



# To-read: ML and AI

 - a tag for papers that were added to the database, but that I haven't really read well. Potentially, papers to revisit!

# Intros and top choices

* Move old  recommendatinos from Sven down here
* Double-check that all Eric's recommendations are here

More info on decision trees, pruning
https://towardsdatascience.com/the-complete-guide-to-decision-trees-28a4e3c7be14

Try to get the gist of it, even if just to participate in the discussion. File, even if math turns to be harder than comfortable:
Automatic Differentiation via Contour Integration
Jan 16, 2020. Aidan Rocke
https://keplerlounge.com/neural-computation/2020/01/16/complex-auto-diff.html
Aidan claims tha this math somehow helps to explain calculations in a dendritic tree, but it has lots of calculus, and seems like something one has to actually think about. It like has math and stuff! Complex functions, poles, and what not.

Reflections on Innateness in Machine Learning
Thomas G. Dietterich. Mar 6, 2018.
https://medium.com/@tdietterich/reflections-on-innateness-in-machine-learning-4eebefa3e1af
Another must-do short read.

The Unreasonable Effectiveness of Recurrent Neural Networks
http://karpathy.github.io/2015/05/21/rnn-effectiveness/

An overview of gradient descent optimization algorithms.  19 Jan 2016 (updated in 2018). By Sebastian Ruder.
https://ruder.io/optimizing-gradient-descent/
Adagrad, adam, and others like that.

Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F. A., ... & Courville, A. (2018). On the spectral bias of neural networks. arXiv preprint arXiv:1806.08734.
https://arxiv.org/abs/1806.08734
On what neural network can encode easily - seems to be a very useful read, potentially.

Challenging Common Deep Learning Conventions – Part 1: Rethinking Layer-wise Feature Amounts in Convolutional Neural Network Architectures. Martin Mundt. Jan 2020
http://martin-mundt.com/rethinking-cnn-feature-amounts/

Neural Reparameterization Improves Structural Optimization, by Sam Greydanus
https://greydanus.github.io/2019/12/15/neural-reparam/
A really cool post on Neural reparameterization (ANN as a way to optimize search for new structures; like ANNs in the middle of a formal algorithms; the one with bridges for example - super-fun!)

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).
http://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf
36k citations, haha. The famous residual networks paper.

Identity Crisis: Memorization and Generalization Under Extreme Overparameterization 
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Michael C. Mozer, Yoram Singer
https://openreview.net/forum?id=B1l6y0VFPr
https://openreview.net/pdf?id=B1l6y0VFPr
Identity mapping (just training the output be exactly like the input).

Berseth, G., Geng, D., Devin, C., Finn, C., Jayaraman, D., & Levine, S. (2019). SMiRL: Surprise Minimizing RL in Dynamic Environments. arXiv preprint arXiv:1912.05510.
https://arxiv.org/abs/1912.05510
Blog:
https://bair.berkeley.edu/blog/2019/12/18/smirl/
Sounds extremely interesting!!

Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). ACM.
https://arxiv.org/abs/1602.04938
Seems to be a classic text, on model interpretatibility (that is often critical for production)

Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky. https://arxiv.org/abs/1912.03263

Metz, L., Maheswaranathan, N., Cheung, B., & Sohl-Dickstein, J. (2018). Meta-Learning Update Rules for Unsupervised Representation Learning. arXiv preprint arXiv:1804.00222.
https://arxiv.org/abs/1804.00222

Finn, C., Abbeel, P., & Levine, S. (2017, August). Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 1126-1135). JMLR. org.
https://arxiv.org/pdf/1703.03400.pdf
About few-shots learning, and generalizing from a very limited number of labels? More than 1000 citations!

Schroff, F., Kalenichenko, D., & Philbin, J. (2015). Facenet: A unified embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 815-823).
https://arxiv.org/abs/1503.03832 
 triplet_loss

Van Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353.
https://arxiv.org/abs/1802.10353
Unsupervized discovery of objects: seems interesting

Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 7132-7141).
https://arxiv.org/abs/1709.01507
Another paper with 1600 citations.

On evolving network architectures:
https://ai.googleblog.com/2018/03/using-evolutionary-automl-to-discover.html

Klein, B., & Hoel, E. (2019). Uncertainty and causal emergence in complex networks. arXiv preprint arXiv:1907.03902.
https://arxiv.org/pdf/1907.03902.pdf
Analyze how some basic network properties (?) can be derived by very basic information principles (?).

Chen, T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. K. (2018). Neural ordinary differential equations. In Advances in neural information processing systems (pp. 6571-6583).
https://arxiv.org/abs/1806.07366
Insteresting postscript a year later (20 min video from Neurips 2019; [youtube](https://www.youtube.com/watch?v=YZ-_E7A3V2w)). Lists items that were ovehyped, and cites follow-up studies that show limitations of this approach. Also a nice story of how they gradually updated the paper, to reflect the critique (including citing older and related work). Recommends as a replacement for resnets and time-series (in the future? once they make them stochastic?).

Advani, M. S., & Saxe, A. M. (2017). High-dimensional dynamics of generalization error in neural networks. arXiv preprint arXiv:1710.03667.
https://arxiv.org/abs/1710.03667

Allamanis, M., Barr, E. T., Devanbu, P., & Sutton, C. (2018). A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR), 51(4), 81.
https://arxiv.org/pdf/1709.06182.pdf
A longish review (30 pages) on using ML to analyze programming code.

# Why networks work?

Milne, T. (2019). Piecewise strong convexity of neural networks. In Advances in Neural Information Processing Systems (pp. 12953-12963).
https://arxiv.org/abs/1810.12805

Yang, G. (2019). Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes. In Advances in Neural Information Processing Systems (pp. 9947-9960).
https://arxiv.org/abs/1910.12478


Belkin, M., Hsu, D., Ma, S., & Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32), 15849-15854.
https://arxiv.org/abs/1812.11118

Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural Networks and the Bias/Variance Dilemma. Neural Computation.
Classical work everybody reference, even though now people are saying it may not be completely correct?

Neal, B. (2019). On the Bias-Variance Tradeoff: Textbooks Need an Update. arXiv preprint arXiv:1912.08286.
https://arxiv.org/abs/1912.08286
Blog post to accompany it: 

Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., & Mitliagkas, I. (2018). A modern take on the bias-variance tradeoff in neural networks. arXiv preprint arXiv:1810.08591.
https://arxiv.org/abs/1810.08591

Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., & Sutskever, I. (2019). Deep Double Descent: Where Bigger Models and More Data Hurt. arXiv preprint arXiv:1912.02292.
https://mltheory.org/deep.pdf
Blog description:
https://openai.com/blog/deep-double-descent/

Arora, S., Du, S. S., Li, Z., Salakhutdinov, R., Wang, R., & Yu, D. (2019). Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks. arXiv preprint arXiv:1910.01663.
https://arxiv.org/pdf/1910.01663.pdf

Ba, J., & Caruana, R. (2014). Do deep nets really need to be deep?. In Advances in neural information processing systems (pp. 2654-2662).
https://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf
Seems to be one of the original network distillation papers (800 refs)

# Interpretability

Gilboa, D., & Gur-Ari, G. (2019). Wider Networks Learn Better Features. arXiv preprint arXiv:1909.11572.
https://arxiv.org/pdf/1909.11572.pdf
train a network (they used MNIST), do UMAP of activations, find groups of neurons that together encode useful features, visualize them. Claim it to be a reasonable approach to understanding networks.

# Tickets, distillation, transfer, curriculum


Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635.
https://arxiv.org/abs/1803.03635
Original paper presenting the lottery ticket hypothesis.

Zhou, H., Lan, J., Liu, R., & Yosinski, J. (2019). Deconstructing lottery tickets: Zeros, signs, and the supermask. arXiv preprint arXiv:1905.01067.
https://arxiv.org/abs/1905.01067

Tian, Y., Jiang, T., Gong, Q., & Morcos, A. (2019). Luck Matters: Understanding Training Dynamics of Deep ReLU Networks. arXiv preprint arXiv:1905.13405.
https://arxiv.org/pdf/1905.13405.pdf
To understand how "lucky tickets" work, they train a larger network from a smaller network, then study activation.

Individual differences among deep neural network models (2020)
Johannes Mehrer, Courtney J. Spoerer, Nikolaus Kriegeskorte, Tim C Kietzmann
https://www.biorxiv.org/content/10.1101/2020.01.08.898288v1
Neuro-inspired: approach ANNs as one would approach individual animals. Use "Representation Similarity analysis" from neuro, changing nothing but weight init. Describe differences (pretty pics!). Claim that dropout helps.
Tweetprint: https://twitter.com/TimKietzmann/status/1215620270679044096

Morcos, A. S., Yu, H., Paganini, M., & Tian, Y. (2019). One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773.
https://arxiv.org/abs/1906.02773
New paper about whether "lucky tickets" generalize across datasets.

Hooker, S., Courville, A., Dauphin, Y., & Frome, A. (2019). Selective Brain Damage: Measuring the Disparate Impact of Model Pruning. arXiv preprint arXiv:1911.05248.
https://weightpruningdamage.github.io/

Maheswaranathan, N., Williams, A., Golub, M., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks. In Advances in Neural Information Processing Systems (pp. 15603-15615).
https://arxiv.org/abs/1907.08549
They trained the same network (or similar networks) thousands of times, and checked how similar the profiles are. Kind of related to both O'Leary neuro idea, and the ticket hypothesis. An interesting follow-up would be something like performance outside of the preferred distributions, and how this performance (individuality?) is distributed itself, and whether some architectures tend to produce clusters rather then smooth distributions (personality?)
Tweetprint:
https://twitter.com/SussilloDavid/status/1153427790672171009

Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., & Rastegari, M. (2019). What's Hidden in a Randomly Weighted Neural Network?. arXiv preprint arXiv:1911.13299.
https://arxiv.org/pdf/1911.13299.pdf
They do some math, and some modeling, trying to estimate how easy it is to find a subnetwork, within a randomly weighted wide network, that would work great without any training. A combination of ticket story, and weight-agnostic networks (see ). Allen institute.

 

Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.
https://arxiv.org/pdf/1503.02531.pdf
The splashy "original distillation paper" (2500 refs)

Furlanello, T., Lipton, Z. C., Tschannen, M., Itti, L., & Anandkumar, A. (2018). Born-again neural networks. arXiv preprint arXiv:1805.04770.
https://arxiv.org/pdf/1805.04770.pdf
Top read!

Self-training with Noisy Student improves ImageNet classification Qizhe Xie, Eduard Hovy, Minh-Thang Luong, Quoc V. Le
https://arxiv.org/abs/1911.04252
Something weird semi-supervised learning with noisy teachers and distillation. Essentially, it seems that a badly labeled large dataset is better than a well-labeled small dataset, so it's better to train one model on a small dataset, then have it label a huge dataset (even tho many labels will be wrong), and then use this large dataset to train the next model. Or something like that. Weird.

von Oswald, J., Henning, C., Sacramento, J., & Grewe, B. F. (2019). Continual learning with hypernetworks. arXiv preprint arXiv:1906.00695.
https://arxiv.org/abs/1906.00695
If it get it right from the abstract, this can be called meta-networks as well: a network that learns to predict weights for a network that would follow a task; so one step above learning the weights for each individual task. Essentially, instead of remembering all possible network configurations for all possible tasks, to reset the network each time, they use this hypernetwork to interpolate in the space of parameters.



Fort, S., Hu, H., & Lakshminarayanan, B. (2019). Deep Ensembles: A Loss Landscape Perspective. arXiv preprint arXiv:1912.02757. 
https://arxiv.org/pdf/1912.02757.pdf
How Bayesian networks help to understand ensemble effects in deep learning (as they learn the distributino of parameters instead of instances?). Some lovely, inspiring pictures.



Wang, T., Zhu, J. Y., Torralba, A., & Efros, A. A. (2018). Dataset Distillation. arXiv preprint arXiv:1811.10959.
https://arxiv.org/pdf/1811.10959.pdf
10 images give 94% accuracy on MNIST.

Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data.
Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth O. Stanley, Jeff Clune
https://arxiv.org/abs/1912.07768
Blog post with a rather detailed description and commentary:
https://eng.uber.com/generative-teaching-networks/

Panagiotatos, G., Passalis, N., Iosifidis, A., Gabbouj, M., & Tefas, A. (2019, September). Curriculum-based Teacher Ensemble for Robust Neural Network Distillation. In 2019 27th European Signal Processing Conference (EUSIPCO) (pp. 1-5). IEEE.
https://ieeexplore.ieee.org/abstract/document/8903112
Something similar: auto-generated curriculum. Not available online for some reason.

**MNIST for small experiments**

Kim, T. H., & Choi, J. (2018). ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks. arXiv preprint arXiv:1801.00904.
https://arxiv.org/pdf/1801.00904.pdf
Not a popular one (9 citations since 2018), but may have some points relevant for me specifically.

Reduced MNIST: how well can machines learn from small data? By Michael Nielsen. Nov 15, 2017
http://cognitivemedium.com/rmnist
Blog post on learning on super-small subsets of MNIST (not an official pub, so never cited). One example of each digit apparently brings accuracy to 42% for a naive NN (vs 97% for full), and ~56% (vs 99% for full) for a pretrained conv net (2 layers, not trained on mnist specifically), followed by 2 fully connected. Regularization becomes extremely important, as a way to fight overfitting, and even switching to convnet for this may be considered a type of regularization, I think.

# Architecture search


Liu, H., Simonyan, K., Vinyals, O., Fernando, C., & Kavukcuoglu, K. (2017). Hierarchical representations for efficient architecture search. arXiv preprint arXiv:1711.00436.
https://arxiv.org/pdf/1711.00436.pdf
Summary by Connor Shorten. Sep 12 2019.
https://towardsdatascience.com/hierarchical-neural-architecture-search-aae6bbdc3624
How to optimize network architectures if training each model is so ridiculously expensive? By defining blocks, and then using them recursively to create larger networks… Not sure how it helps, but apparently it helps a lot? Read.

Elsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: A survey. arXiv preprint arXiv:1808.05377.
http://www.jmlr.org/papers/volume20/18-598/18-598.pdf


# Self-supervised learning

Jing, L., & Tian, Y. (2019). [Self-supervised visual feature learning with deep neural networks: A survey](https://arxiv.org/pdf/1902.06162.pdf). arXiv preprint arXiv:1902.06162.

Kolesnikov, A., Zhai, X., & Beyer, L. (2019). [Revisiting self-supervised visual representation learning](http://openaccess.thecvf.com/content_CVPR_2019/papers/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.pdf). arXiv preprint arXiv:1901.09005.

Roads, B. D., & Love, B. C. (2020). Learning as the unsupervised alignment of conceptual systems. Nature Machine Intelligence, 1-7.
https://arxiv.org/abs/1906.09012

He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2019). Momentum Contrast for Unsupervised Visual Representation Learning. arXiv preprint arXiv:1911.05722. https://arxiv.org/pdf/1911.05722.pdf

Misra, I., & van der Maaten, L. (2019). Self-Supervised Learning of Pretext-Invariant Representations. arXiv preprint arXiv:1912.01991. https://arxiv.org/abs/1912.01991 

Hénaff, O. J., Razavi, A., Doersch, C., Eslami, S. M., & Oord, A. V. D. (2019). Data-efficient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272. https://arxiv.org/abs/1905.09272v2

What's the current view of greedy training of autoencoders? Is it an abandoned technique, or are people still working on it? Sample seminal papers:
* Bengio, Y., Lamblin, P., Popovici, D., & Larochelle, H. (2007). Greedy layer-wise training of deep networks. In Advances in neural information processing systems (pp. 153-160).
* Hinton, G. E, Osindero, S., and Teh, Y. W. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18:1527-1554.
* http://scholarpedia.org/article/Deep_belief_networks

Zhai, X., Oliver, A., Kolesnikov, A., & Beyer, L. (2019). S4L: Self-Supervised Semi-Supervised Learning. arXiv preprint arXiv:1905.03670.
https://arxiv.org/abs/1905.03670

# Exploration, curiosity, RL

Ortega, P. A., Wang, J. X., Rowland, M., Genewein, T., Kurth-Nelson, Z., Pascanu, R., ... & Jayakumar, S. M. (2019). Meta-learning of Sequential Strategies. arXiv preprint arXiv:1905.03030.
https://arxiv.org/pdf/1905.03030.pdf

Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, et al. Noisy networks for exploration. arXiv preprint arXiv:1706.10295, 2017
https://arxiv.org/pdf/1706.10295.pdf
A weak (not good enough?) way to add exploration to AI playing videogames, by target-injecting noise into the network (?). Sometimes (?) elevates the performance from "sub-human to super-human" level (funny turn of a phrase).

Lázaro-Gredilla, M., Lin, D., Guntupalli, J. S., & George, D. (2018). Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs. arXiv preprint arXiv:1812.02788.
https://robotics.sciencemag.org/content/4/26/eaav3150.full?ijkey=9p/p9D23WW2Ek&keytype=ref&siteid=robotics
Research article on imagination, recursion, and abstraction. Guessing context via transfer from prior experience.

Dauphin, Y. N., & Schoenholz, S. (2019). MetaInit: Initializing learning by learning to initialize. In Advances in Neural Information Processing Systems (pp. 12624-12636).
https://papers.nips.cc/paper/9427-metainit-initializing-learning-by-learning-to-initialize



Leibo, J. Z., Hughes, E., Lanctot, M., & Graepel, T. (2019). Autocurricula and the emergence of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv preprint arXiv:1903.00742.
https://arxiv.org/pdf/1903.00742.pdf
Something related to their multi-agent simulation (hide-and-seek). Seems promising.

Proximal Policy Optimization by OpenAI:
https://openai.com/blog/openai-baselines-ppo/
Something for agent training?

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., ... & Oh, J. (2019). Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 1-5.
No open link?

Freeman, D., Ha, D., & Metz, L. (2019). Learning to Predict Without Looking Ahead: World Models Without Forward Prediction. In Advances in Neural Information Processing Systems (pp. 5380-5391).
https://arxiv.org/abs/1910.13038
If you limit agents ability too observe real world to "key frames", making it more sporadic, it develops some predicting capabilities, even tho you never told it do?

Raghu, A., Raghu, M., Bengio, S., & Vinyals, O. (2019). Rapid learning or feature reuse? towards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157.
https://arxiv.org/abs/1909.09157

Behavior Regularized Offline Reinforcement Learning
Yifan Wu, George Tucker, Ofir Nachum
https://arxiv.org/abs/1911.11361
A paper about the most horrible type of learning: offline (model of the world instead of the real world) and reinforcement. Some sort of a review?

Yu, H., Edunov, S., Tian, Y., & Morcos, A. S. (2019). Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP. arXiv preprint arXiv:1906.02768.
https://arxiv.org/abs/1906.02768
Lottery tickets in RL domain.

## RNNs, Attention


Kitaev, N., Kaiser, Ł., & Levskaya, A. (2020). Reformer: The Efficient Transformer. arXiv preprint arXiv:2001.04451.
https://arxiv.org/pdf/2001.04451.pdf
Potentially a newer link:
https://openreview.net/forum?id=rkgNKkHtvB
Google research. Claim that somehow by two changes in logic they improved  so much that now they would be O(L log L) instead of O(L²) efficient, which is like a big deal.

https://distill.pub/2019/memorization-in-rnns/
How LSTM networks remember text: a visual intro.

Hafner, D., Irpan, A., Davidson, J., & Heess, N. (2017). Learning hierarchical information flow with recurrent neural modules. In Advances in Neural Information Processing Systems (pp. 6724-6733).
https://papers.nips.cc/paper/7249-learning-hierarchical-information-flow-with-recurrent-neural-modules.pdf
ThalNet, which like a virtual thalamus. Only cited by 4 since 2017, so doesn't look like it caught on.

Jaderberg, M., Simonyan, K., & Zisserman, A. (2015). Spatial transformer networks. In Advances in neural information processing systems (pp. 2017-2025).
https://arxiv.org/pdf/1506.02025.pdf
2000 citations. Seems important.

Single Headed Attention RNN: Stop Thinking With Your Head. S Merity. (2019)
https://arxiv.org/abs/1911.11423
He tried to diversify the field by deliberately sticking to a non-mainstream approach (something that is NOT transformers), and got good performance. Also the writing is wild.

Yang, Y., Sautière, G., Ryu, J. J., & Cohen, T. S. (2019). Feedback Recurrent AutoEncoder. arXiv preprint arXiv:1911.04018.
https://arxiv.org/abs/1911.04018
Qualcomm AI Research.

On the Relationship between Self-Attention and Convolutional Layers (2020)
Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi
https://openreview.net/forum?id=HJlnC1rKPB
Describe that attention networks spontaneiously develop convolution?

Lipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural networks for sequence learning. arXiv preprint arXiv:1506.00019.
[https://arxiv.org/pdf/1506.00019.pdf](<https://arxiv.org/pdf/1506.00019.pdf>)
Great review for learning about RNNs (could be a textbook) 700 refs.

Press, O., Smith, N. A., & Levy, O. (2019). Improving Transformer Models by Reordering their Sublayers. arXiv preprint arXiv:1911.03864.
https://ofir.io/sandwich_transformer.pdf
An interesting tiny paper where all they did, I think, is play with the sequence of 2 common blocks, to show that the optimal sequence is not what everyone expected. May be neat. But I prob need to understand how transformers work to get it.

Arabshahi, F., Lu, Z., Singh, S., & Anandkumar, A. (2019). Memory Augmented Recursive Neural Networks. arXiv preprint arXiv:1911.01545.
https://arxiv.org/abs/1911.01545

Compressive Transformers for Long-Range Sequence Modelling
by Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Timothy P. Lillicrap (at DeepMind)
https://arxiv.org/abs/1911.05507

# AEs, GANs, Generation


A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications. Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, Jieping Ye. 20 Jan 2020
https://arxiv.org/abs/2001.06937
Most recent review of the field.

PCGRL: Procedural Content Generation via Reinforcement Learning. Ahmed Khalifa, Philip Bontrager, Sam Earle, Julian Togelius. 24 Jan 2020.
https://arxiv.org/abs/2001.09212
Independent researchers. Treat game level generation as a "game"; then try 3 different approaches (?) to solve this game with Markovian processes and .

Oord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., ... & Kavukcuoglu, K. (2016). Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499.
https://arxiv.org/abs/1609.03499
1.4k citations: famous really successful realtime audio generation network from Google DeepMind.

Video tutorial on GANs from Ian Goodfellow (2016):
https://www.youtube.com/watch?v=HGYYEUSm-0Q

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680).
https://arxiv.org/pdf/1406.2661v1.pdf
Original paper with 14000 citations.

Original StyleGan (is it?)
https://arxiv.org/abs/1812.04948
Karras, T., Laine, S., & Aila, T. (2019). A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4401-4410).

Reimplementation of StyleGAN from scratch in TF 2.0. Seem to be well-written, and worth studying:
https://github.com/manicman1999/StyleGAN-Tensorflow-2.0

Contrastive Learning of Structured World Models
Thomas Kipf, Elise van der Pol, Max Welling
[https://arxiv.org/abs/1911.12247](<https://arxiv.org/abs/1911.12247>)
Not sure if filed correctly, but they claim to identify objects in an unsupervised manner using some tricks from graph networks. What??

Lin, Z., Thekumparampil, K. K., Fanti, G., & Oh, S. (2019). InfoGAN-CR: Disentangling Generative Adversarial Networks with Contrastive Regularizers. arXiv preprint arXiv:1906.06034.
https://arxiv.org/abs/1906.06034
Separating latent vectors (aka "disentanglement"). Apparently something that we know how to do in VAE, here applied to GANs?

https://arxiv.org/abs/1905.01164 
SinGAN: Learning a Generative Model from a Single Natural Image Tamar Rott Shaham, Tali Dekel, Tomer Michaeli

Reinke, C., Etcheverry, M., & Oudeyer, P. Y. (2019). Intrinsically Motivated Exploration for Automated Discovery of Patterns in Morphogenetic Systems. arXiv preprint arXiv:1908.06663.
arxiv.org/abs/1908.06663
Apparently used an intrinsically motivated AI to explore Game Of Life to find cool patterns. The most interesting part (judging from the abstract) is that it developed a non-obvious representation, just looking for novelty (or whatever it used: looking for something _interesting_). Would be super-cool to try it for my networks, huh?

Hoyer, S., Sohl-Dickstein, J., & Greydanus, S. (2019). Neural reparameterization improves structural optimization. arXiv preprint arXiv:1909.04240.
https://arxiv.org/pdf/1909.04240.pdf
They are trying to create some fancy mechanical structures, and instead of directly optimizing the parameters of these structures, they optimize the parameters of an ANN that output these structures. Claim that it works better. Looks fun.

Makhzani, A. (2018). Implicit autoencoders. arXiv preprint arXiv:1805.09804.
https://arxiv.org/abs/1805.09804
Sort of GAN-like architecture that tries to optimize the latent space directly? Interesting, but hard to get from the abstract.

Find original Neurips GAN paper from 2016?

# Graphical
Bacciu, D., Errica, F., Micheli, A., & Podda, M. (2019). A Gentle Introduction to Deep Learning for Graphs. arXiv preprint arXiv:1912.12693.
https://arxiv.org/abs/1912.12693
Must read (seems short and gentle indeed)

Battaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M., ... & Gulcehre, C. (2018). Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261.
https://arxiv.org/abs/1806.01261
Was described as practical and particularly well-written.

David Belli (2019). Generative graph transformer
https://davide-belli.github.io/generative-graph-transformer.html
Blog post with math and pretty pics: investigate!

Dehmamy, N., Barabási, A. L., & Yu, R. (2019). Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology. arXiv preprint arXiv:1907.05008.
https://arxiv.org/abs/1907.05008

Bojchevski, A., Shchur, O., Zügner, D., & Günnemann, S. (2018). Netgan: Generating graphs via random walks. arXiv preprint arXiv:1803.00816.
https://arxiv.org/pdf/1803.00816.pdf

# Bayesian

The Case for Bayesian Deep Learning
Andrew Gordon Wilson. January 11, 2020
https://cims.nyu.edu/~andrewgw/caseforbdl/
Serious blog post with some (non-scary) math.

A Sober Look at Bayesian Neural Networks. January 17, 2020
by Carles Gelada and Jacob Buckman
https://jacobbuckman.com/2020-01-17-a-sober-look-at-bayesian-neural-networks/
A response to Wilson 2020 above! :) Best type of scientific exchange! Also a blog post, also with math.

# Alternative network designs

**Helmholtz Machines**

Kevin G. Kirby (2006). A Tutorial on Helmholtz Machines 
https://www.nku.edu/~kirby/docs/HelmholtzTutorialKoeln.pdf

Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The helmholtz machine. Neural computation, 7(5), 889-904.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.34.1800&rep=rep1&type=pdf
Original paper by Hinton, 1k citations.

# CogSci and Meta


Chollet, F. (2019). The Measure of Intelligence. arXiv preprint arXiv:1911.01547.
https://arxiv.org/abs/1911.01547
Seems to be endorsed by many good people. Bump it up on the list!

Turing, A. M. (1950). Computing machinery and intelligence. In Parsing the Turing Test (pp. 23-65). Springer, Dordrecht. 2005.
http://www.geielettronica.it/images/pdf/turing.pdf
Said to be a really nicely written paper, seminal, good examples, and with 13k citations.

Do language network learn what they think they learn?
https://thegradient.pub/nlps-clever-hans-moment-has-arrived/

Sinz, F. H., Pitkow, X., Reimer, J., Bethge, M., & Tolias, A. S. (2019). Engineering a less artificial intelligence. Neuron, 103(6), 967-979.
http://xaqlab.com/wp-content/uploads/2019/09/LessArtificialIntelligence.pdf
Review / Opinion piece on differences between intelligence as it operates in biological brains, and what modern AI offers. Very neuro-style, but it's more about AI than about neuro, so I file it here.

van Rooij, I., Wright, C. D., Kwisthout, J., & Wareham, T. (2018). Rational analysis, intractability, and the prospects of ‘as if’-explanations. Synthese, 195(2), 491-510.
On how humans (and thus AI) could handle intractable problems.
Also these 2 blog posts that summarize and comment on this research: [one](https://experiencemachines.wordpress.com/2019/11/13/five-ways-the-mind-does-not-solve-computationally-intractable-problems-and-one-way-it-does/), and [two](https://irisvanrooijcogsci.com/2020/01/01/sampling-cannot-make-hard-work-light/)

Mesulam, M. M. (1998). From sensation to cognition. Brain: a journal of neurology, 121(6), 1013-1052.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.8687&rep=rep1&type=pdf
Review with 3k citations, which is a ton for this area. Something about modular organization of brain, and apparently so well written that highly influential?

Ullman, S. (1987). Visual routines. In Readings in computer vision (pp. 298-328). Morgan Kaufmann.
https://www.cs.cmu.edu/afs/cs/academic/class/15494-s12/readings/ullman-visual-routines.pdf
Some influential piece with 1.2k citations. Some take on the psychology of visual perception?
And a commentary / computer implementation of it (a presentation from 2008)
https://www.cs.cmu.edu/afs/cs/academic/class/15494-s08/lectures/visual_routines.pdf

Barsalou, L. W. (1999). Perceptual symbol systems. Behavioral and brain sciences, 22(4), 577-660.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.4.5511&rep=rep1&type=pdf
A hugely influential conceptual piece: 7k citations! Something about how knowledge is all about predicting your perception, and perceiving your actions? Lots of philosophy, followed by sensorimotor examples, neural representations. Then back-and-forth exchange with other cognitive scientists.

Marcus, G. (2018). Innateness, alphazero, and artificial intelligence. arXiv preprint arXiv:1801.05667.
[https://arxiv.org/ftp/arxiv/papers/1801/1801.05667.pdf](<https://arxiv.org/ftp/arxiv/papers/1801/1801.05667.pdf>)

Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2), 3-71.
[pdf on rutgers edu](http://ruccs.rutgers.edu/images/personal-zenon-pylyshyn/proseminars/Proseminar13/ConnectionistArchitecture.pdf)
Seminal old paper (5k refs) against neural networks :) that everybody reference when then want to question the supposed universality of deep learning.

Cardon, D., Cointet, J. P., & Mazieres, A. (2018). Neurons spike back: The Invention of Inductive Machines and the Artificial Intelligence Controversy.
[https://neurovenge.antonomase.fr/NeuronsSpikeBack.pdf](<https://neurovenge.antonomase.fr/NeuronsSpikeBack.pdf>)

Cisek, P. (2019). Resynthesizing behavior through phylogenetic refinement. Attention, Perception, & Psychophysics, 1-23.
https://link.springer.com/article/10.3758/s13414-019-01760-1
On evolution of mamallian brain, and how you can draw a link between behavioral (computational?) complexity, and hierarchical organization of a vertebrate brain. Tries to ambitiously draw parallels between evolution, development, and behavior.

# Symbolic and related

Yoshua Bengio’s short reading list:
* BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop, Chevalier-Boisvert et al.,
2018: https://arxiv.org/abs/1810.08272v2.
* A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms, Bengio et al., 2019:
https://arxiv.org/abs/1901.10912.
* Learning Neural Causal Models from Unknown Interventions, Ke et al., 2019: https://arxiv.org/abs/1910.
01075.
* Recurrent Independent Mechanisms, Goyal et al., 2019: https://arxiv.org/abs/1909.10893.
* The Consciousness Prior, Bengio et al., 2017: https://arxiv.org/abs/1709.08568.

Paul Smolensky. Next-generation architectures bridge gap between neural and symbolic representations with neural symbols. Microsoft Research blog. December 12, 2019.
https://www.microsoft.com/en-us/research/blog/next-generation-architectures-bridge-gap-between-neural-and-symbolic-representations-with-neural-symbols/

# Complexity


Corominas-Murtra, B., Seoane, L. F., & Solé, R. (2018). Zipf’s law, unbounded complexity and open-ended evolution. Journal of the Royal Society Interface, 15(149), 20180395.
https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2018.0395
General patterns of increase in complexity during evolution, using several natural and artificial examples (texts, proteins, logic circuits, and even combinations of Legos). Information and string complexity.

Zenil, H., & Villarreal-Zapata, E. (2013). Asymptotic behavior and ratios of complexity in cellular automata. International Journal of Bifurcation and Chaos, 23(09), 1350159.
https://arxiv.org/pdf/1304.2816.pdf
Some measures of complexity (Shannon's block entropy and Kolmogorov) applied to 1D cellular automata. They try to make some general statements. Give a look.

# To-read: Network Science

Clune, J., Mouret, J. B., & Lipson, H. (2013). The evolutionary origins of modularity. Proceedings of the Royal Society b: Biological sciences, 280(1755), 20122863.
https://royalsocietypublishing.org/doi/pdf/10.1098/rspb.2012.2863
500 citations: pretty cool for a bio paper

Go through publications of D. Krioukov (Northeastern):
https://web.northeastern.edu/dima/pub/
He has some publications (older, down the page) on percolation on networks.

Video: The physics of brain network architecture, function, and control
Lecture by Danielle Bassett
https://www.youtube.com/watch?v=u-l-rdKvd8U

# Spectral techniques

Tao, T. (2013). Outliers in the spectrum of iid matrices with bounded rank perturbations. Probability Theory and Related Fields, 155(1-2):231–263.

P. Van Mieghem. Graph Spectra for Complex Networks. Cambridge University Press, Cambridge, UK, 2013.

# Cliques and ensembles

P. Bonacich. Factoring and weighting approaches to clique identification. J. Math. Sociol.,
2:113–120, 1972.

# Network topology analysis

da Fontoura Costa, L. (2018). What is a Complex Network?(CDT-2).
[ResearchGate link](https://www.researchgate.net/profile/Luciano_Da_F_Costa/publication/324312765_What_is_a_Complex_Network_CDT-2/links/5aca49ba4585151e80a91abd/What-is-a-Complex-Network-CDT-2.pdf)
Hmm. A very short illustrated mini-review, and the topic is good, but is it a good paper? Check.

# Processes on networks

Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of ‘small-world’networks. nature, 393(6684), 440.
Seminal, very short (2 pages) work. 40k refs!! That's because this particular type of rewiring became known as the **Watts-Strogatz model**, and everyone who used it, referenced this original paper.

R. Pastor-Satorras, C. Castellano, P. Van Mieghem, and A. Vespignani. Epidemic processes
in complex networks. Rev. Mod. Phys., 87:925–979, 2015.
Sounds like a seminal review...

M. Asllani, D. M. Busiello, T. Carletti, D. Fanelli, and G. Planchon. Turing patterns in
multiplex networks. Phys. Rev. E, 90:042814, 2014.

N. E. Kouvaris, S. Hata, and A. Díaz-Guilera. Pattern formation in multiplex networks. Sci.
Rep., 5:10840, 2015.

N. Masuda, M. A. Porter, and R. Lambiotte. Random walks and diffusion on networks. Phys.
Rep., 716–717:1–58, 2017.

M. O. Jackson and Y. Zenou. Games on networks. In P. Young and S. Zamir, editors,
Handbook of Game Theory Vol. 4, pages 95–163. Elsevier, 2014.

# Graph-producing automata

Eckel, W. (2015). Creating an Artificial World with a New Kind of Cellular Automata. arXiv preprint arXiv:1507.05789.
https://arxiv.org/pdf/1507.05789.pdf

Praba, B., & Saranya, R. (2020). Application of the graph cellular automaton in generating languages. Mathematics and Computers in Simulation, 168, 111-121.

Mukhopadhyay, D. (2012, September). Generating Expander Graphs Using Cellular Automata. In International Conference on Cellular Automata (pp. 52-62). Springer, Berlin, Heidelberg.

Hassan, B. A., & Hiesinger, P. R. (2015). Beyond molecular codes: simple rules to wire complex brains. Cell, 163(2), 285-291.
A review of how simple developmental rules (in this case actual dendrites in 3D, growing and stopping) can produce different stochastic patterns, depending on the encoded rules.

# Dynamic on dynamic

Zambra, M., Testolin, A., & Maritan, A. (2019). [Emergence of Network Motifs in Deep Neural Networks](https://arxiv.org/pdf/1912.12244.pdf). arXiv preprint arXiv:1912.12244.

N. Perra, B. Gonçalves, R. Pastor-Satorras, and A. Vespignani. Activity driven modeling of time varying networks. Sci. Rep., 2:469, 2012. 

# Computations on graphs

Farhoodi, R., Filom, K., Jones, I. S., & Kording, K. P. (2019). On functions computed on trees. arXiv preprint arXiv:1904.02309. https://arxiv.org/abs/1904.02309

# Centrality measures


J. Flores and M. Romance. On eigenvector-like centralities for temporal networks: Discrete
vs. continuous time scales. J. Comp. App. Math., 330:1041–1051, 2018.
Introduces eigenvector centrality for dynamic networks

S. Praprotnik and V. Batagelj. Spectral centrality measures in temporal networks. Ars Math.
Contemp., 11(1):11–33, 2015.

P. Grindrod and D. J. Higham. A dynamical systems view of network centrality. Proc. Royal
Soc. A, 470:20130835, May 2014.
Introduces Katz and Bonacich centrality measures for dynamic networks

K. Lerman, R. Ghosh, and J. H. Kang. Centrality metric for dynamic networks. In Proc.
Eighth Workshop on Mining and Learning with Graphs, pages 70–77. ACM, 2010.
Introduces Katz and Bonacich centrality measures for dynamic networks

P. Grindrod and D. J. Higham. A matrix iteration for dynamic network summaries. SIAM
Rev., 55:118–128, Jan 2013.
Introduces "communicatibility centrality"

Fagiolo, G. (2007). Clustering in complex directed networks. Physical Review E, 76(2), 026107.

# Process complexity

Toker, D., Sommer, F. T., & D’Esposito, M. (2020). [A simple method for detecting chaos in nature](https://www.nature.com/articles/s42003-019-0715-9). Communications Biology, 3(1), 1-13.
Seems practical, and reasonable. They first make sure the data is ready to be analyzed (noise, downsampling). Takes considerable part of the text. Then they use a "0-1 test", resulting in one value K that is 0 for periodical, and approaches 1 for chaotic. Seems to be based on predictions for brownian motion, but need to look more closely.

F. A. Rodrigues, T. K. DM. Peron, P. Ji, and J. Kurths. The Kuramoto model in complex
networks. Phys. Rep., 469:1–98, 2016

A Arenas, A Díaz-Guilera, J Kurths, Y Moreno, and C Zhou. Synchronization in complex
networks. Phys. Rep., 469:93–153, 2008.

O’Keeffe, K. P., Hong, H., & Strogatz, S. H. (2017). Oscillators that sync and swarm. Nature communications, 8(1), 1504.
https://www.nature.com/articles/s41467-017-01190-3
About collective self-organized behaviors. Can be useful for the modeling class maybe?

# Neuromorphic computing

Kendall, J. D., & Kumar, S. The building blocks of a brain-inspired computer. Applied Physics Reviews, 7(1), 011305.
https://aip.scitation.org/doi/10.1063/1.5129306
14 January 2020. A rather long review on the next steps in neuromorhic computing. With a review of what aspects of the brain are reflected, which ones aren't, and what to do with it?

# Misc

On teaching math
https://www.uni-muenster.de/Physik.TP/~munsteg/arnold.html

To-read: Neuro
===


**Bibliography collections:**
*  - On reconstructing biological connectivity from spike-trains and alike

Merel, J., Botvinick, M., & Wayne, G. (2019). Hierarchical motor control in mammals and machines. Nature Communications, 10(1), 1-12.
https://www.nature.com/articles/s41467-019-13239-6
Opinion? Seems short and general, so give it priority.

# General and top

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.
https://www.pnas.org/content/116/21/10537.short

# Topology and development

Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience Carina Curto, Katherine Morrison
https://www.sciencedirect.com/science/article/pii/S0959438819300443 
Review. Exactly what it should be (networks, motifs, dynamics)

Morrison, A., Aertsen, A., and Diesmann, M. (2007). Spike-timing dependent plasticity in balanced random networks. Neural Computation, 19:1437–1467.

Lazar, A., Pipa, G., & Triesch, J. (2006, April). The combination of STDP and intrinsic plasticity yields complex dynamics in recurrent spiking networks. In ESANN (pp. 647-652).

Ozturk, I., & Halliday, D. M. (2016, December). Mapping spatio-temporally encoded patterns by reward-modulated STDP in Spiking neurons. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series on (pp. 1-8). IEEE.

Emergence of complex computational structures from chaotic neural networks through reward-modulated hebbian learning. GM Hoerzer, R Legenstein, W Maass, Cerebral Cortex, 24, 677-690, 2014

Gilson, M., Burkitt, A., & van Hemmen, J. L. (2010). STDP in recurrent neuronal networks. Spike-timing dependent plasticity, 271.

Naudé, J., Cessac, B., Berry, H., & Delord, B. (2013). Effects of cellular homeostatic intrinsic plasticity on dynamical and computational properties of biological recurrent neural networks. Journal of Neuroscience, 33(38), 15032-15043.

Gutig R, Aharonov R, Rotter S, Sompolinsky H. Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity. The Journal of neuroscience: the official journal of the Society for Neuroscience. 2003;23(9):3697–714. Epub 2003/05/09.

Morrison A, Diesmann M, Gerstner W. Phenomenological models of synaptic plasticity based on spike timing. Biol Cybern. 2008;98(6):459–78. Epub 2008/05/21.

Gilson M, Burkitt AN, Grayden DB, Thomas DA, van Hemmen JL. Emergence of network structure due to spike-timing-dependent plasticity in recurrent neuronal networks III: Partially connected neurons driven by spontaneous activity. Biol Cybern. 2009;101(5–6):411–26.

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning. Cerebral Cortex, 24, 677-690.

Fauth, M., & Tetzlaff, C. (2016). Opposing effects of neuronal activity on structural plasticity. Frontiers in neuroanatomy, 10, 75.
https://www.frontiersin.org/articles/10.3389/fnana.2016.00075/full
Review, looking how synapse potentiation (aka "activity") and elimination (aka "structural plasticity") compete and shape micro-connectivity.

Rabinovich, M., Volkovskii, A., Lecanda, P., Huerta, R., Abarbanel, H. D. I., & Laurent, G. (2001). Dynamical encoding by networks of competing neuron groups: winnerless competition. Physical review letters, 87(6), 068102.	
https://pdfs.semanticscholar.org/a0e4/249b0ab6e34699b12d49ad0d0a4c5e121886.pdf

Lagzi, F., & Rotter, S. (2015). Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state. PloS one, 10(9), e0138947.
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138947

Gutkin, B. S., Laing, C. R., Colby, C. L., Chow, C. C., & Ermentrout, G. B. (2001). Turning on and off with excitation: the role of spike-timing asynchrony and synchrony in sustained neural activity. Journal of computational neuroscience, 11(2), 121-134.
Apparently, sorta seminal paper in this narrow field.

# Criticality, inhibition, dynamics

Popular account, harvest all links:
https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/

25 years of criticality in neuroscience — established results, open controversies, novel concepts J Wilting, V Priesemann
[https://www.sciencedirect.com/science/article/pii/S0959438819300248]
Very short, so a must read.

Mechanisms underlying gain modulation in the cortex (2020)
Katie A. Ferguson & Jessica A. Cardin 
https://www.nature.com/articles/s41583-019-0253-y?WT.mc_id=TWT_NatRevNeurosci
New review in Nature; ppl seem to like

Sussillo, D. and Abbott, L. F. (2009). Generating Coherent Patterns of Activity from Chaotic Neural Networks. Neuron, 63(4):544–557.

Universality and individuality in neural dynamics across large populations of recurrent networks Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, David Sussillo
https://arxiv.org/abs/1907.08549 
 
Mastrogiuseppe, F. and Ostojic, S. (2018). Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks. Neuron, 554 99(3):609–623.e29.

Cortical computations via metastable activity Giancarlo La Camera, Alfredo Fontanini, Luca Mazzucato
https://www.sciencedirect.com/science/article/pii/S0959438818302198 
Interesting; includes a section of spiking models of those. It's like criticality, but better, and also super-relevant.

Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., & Gerstner, W. (2011). Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks. *Science*, *334*(6062), 1569-1573. 
About what tunes inhibitory neurons, that in turn make excitatory activity properly sparse.

Schaub, M. T., Billeh, Y. N., Anastassiou, C. A., Koch, C., and Barahona, M. (2015). Emergence of Slow-Switching Assemblies in Structured Neuronal Networks. PLoS Computational Biology, 11(7):1–28.

Kim, J. K., & Fiorillo, C. D. (2017). Theory of optimal balance predicts and explains the amplitude and decay time of synaptic inhibition. Nature communications, 8, 14566.
https://www.nature.com/articles/ncomms14566
How ideally balanced inhibition can emerge from anti-hebbian plasticity.

Kanders, K., Lee, H., Hong, N., Nam, Y., & Stoop, R. (2020). Fingerprints of a second order critical line in developing neural networks. Communications Physics, 3(1), 1-13.
https://www.nature.com/articles/s42005-019-0276-8
Transitions between two levels of criticality in neuronal culture.
 
# Reservoir computing

Tanaka, G., Yamane, T., Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., ... & Hirose, A. (2019). Recent advances in physical reservoir computing: a review. Neural Networks.
https://www.sciencedirect.com/science/article/pii/S0893608019300784
High priority 

Pogodin, R., Corneil, D., Seeholzer, A., Heng, J., & Gerstner, W. (2019). Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks. arXiv preprint arXiv:1910.10559. 
https://arxiv.org/pdf/1910.10559.pdf 
Reservoir computer + a "working memory network"

Rotermund, D., & Pawelzik, K. R. (2019). Biologically plausible learning in a deep recurrent spiking network. bioRxiv, 613471.
https://www.biorxiv.org/content/10.1101/613471v1.full

Stimberg, M., Goodman, D. F., & Nowotny, T. (2019). [Brian2GeNN: accelerating spiking neural network simulations with graphics hardware](https://www.nature.com/articles/s41598-019-54957-7). Scientific Reports.
Spiking network simulator that is apparently so optimized that it runs very fast. Check before writing any custom spiking models.

# Synfire chains, Sequences

Zheng, P. and Triesch, J. (2014). Robust development of synfire chains

Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of spiking model neurons. Nature Neuroscience, 19(3):350–355.
Sounds like may be a neat review or opinion piece.

Gilra, A. and Gerstner, W. (2017). Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. Elife, 6(e28295):1–38.

Park, Y., Choi, W., & Paik, S. B. (2017). Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories. Scientific reports, 7(1), 5671.

Lee, H., Choi, W., Park, Y., & Paik, S. B. (2020). Distinct role of flexible and stable encodings in sequential working memory. Neural Networks, 121, 419-429.

Nicola, W. and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE training. Nature Communications, 8(1):1–15.

Nicola, W. and Clopath, C. (2019). A diversity of interneurons and Hebbian plasticity facilitate rapid compressible learning in the hippocampus. Nature Neuroscience, 22(7):1168–1181.

Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. Journal of Neuroscience, 33(23):9565–9575.

Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., and Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101(March):1–12.

Time perception in brain networks: may be related?
Integrating time from experience in the lateral entorhinal cortex
https://www.nature.com/articles/s41586-018-0459-6
Activity in perceptual classification networks as a basis for human subjective time perception
https://www.nature.com/articles/s41467-018-08194-7
Changing temporal context in human temporal lobe promotes memory of distinct episodes
https://www.nature.com/articles/s41467-018-08189-4
Also a blog post summary: https://www.quantamagazine.org/how-the-brain-creates-a-timeline-of-the-past-20190212/

# Single-neuron calculations

Hidden Computational Power Found in the Arms of Neurons. Jordana Cepelewicz. Jan 14 2020.
https://www.quantamagazine.org/neural-dendrites-reveal-their-computational-power-20200114/

Abraham, W. C., Jones, O. D., & Glanzman, D. L. (2019). Is plasticity of synapses the mechanism of long-term memory storage?. NPJ science of learning, 4(1), 1-10.
https://www.ncbi.nlm.nih.gov/pubmed/31285847
A review on alternatives (non-synaptic) ways too store memories in the brain.

Dendritic action potentials and computation in human layer 2/3 cortical neurons
Albert Gidon1, Timothy Adam Zolnik1, Pawel Fidzinski2,3, Felix Bolduan4, Athanasia Papoutsi5, Panayiota Poirazi5, Martin Holtkamp2, Imre Vida3,4, Matthew Evan Larkum. Science  03 Jan 2020:
https://science.sciencemag.org/content/367/6473/83
Apparently show that individual cortical piramidal neurons can do XOR.

David, Beniaguev, Segev Idan, and London Michael. "Single Cortical Neurons as Deep Artificial Neural Networks." bioRxiv (2019): 613141. https://www.biorxiv.org/content/10.1101/613141v1.full.pdf

Unifying Long-Term Plasticity Rules for Excitatory Synapses by Modeling Dendrites of Cortical Pyramidal Neurons
C Ebner, C Clopath, P Jedlicka, H Cuntz - Cell Reports, 2019
https://www.sciencedirect.com/science/article/pii/S2211124719315591
Compartments, NMDA, all sorts of STDP stuff.

# Credit assignment

Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2019). A solution to the learning dilemma for recurrent networks of spiking neurons. bioRxiv, 738385.
https://www.biorxiv.org/content/10.1101/738385v3
Something like backpropagation in spiking networks.

Aljadeff, J., D'amour, J., Field, R. E., Froemke, R. C., & Clopath, C. (2019). Cortical credit assignment by Hebbian, neuromodulatory and inhibitory plasticity. arXiv preprint arXiv:1911.00307.
[https://arxiv.org/pdf/1911.00307.pdf](<https://arxiv.org/pdf/1911.00307.pdf>)

Gütig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience, 9(3), 420.

Zenke, F., Poole, B., & Ganguli, S. (2017, August). Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3987-3995). JMLR. org.
https://arxiv.org/pdf/1703.04200.pdf
Abstract synapses (not biological) that somehow prioritize what to learn based on some information? Claim to be biologically inspired. Well cited.

Krieg, D., & Triesch, J. (2014). A unifying theory of synaptic long-term plasticity based on a sparse distribution of synaptic strength. Frontiers in synaptic neuroscience, 6, 3.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941589/
Try to marry Hebbian with gradient descent and something else? Lots of math, not that well cited.

Rolfe, Jason Tyler (2012) Intrinsic Gradient Networks. Dissertation (Ph.D.), California Institute of Technology.
https://thesis.library.caltech.edu/6953/
PhD thesis that was never published on biologically plausible gradient descend in the brain? Check it out.

# RL in the brain


Dabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R., & Botvinick, M. (2020). A distributional code for value in dopamine-based reinforcement learning. Nature, 1-5.
https://www.nature.com/articles/s41586-019-1924-6
(obv behind paywall)
DeepMind. Some cover of it:
https://www.technologyreview.com/s/615054/deepmind-ai-reiforcement-learning-reveals-dopamine-neurons-in-brain/

Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., ... & Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21(6), 860.
https://www.nature.com/articles/s41593-018-0147-8

Fremaux N, Sprekeler H, Gerstner W. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS computational biology. 2013;9(4):e1003024.

# DL models of brains

https://www.biorxiv.org/content/10.1101/838383v1
Training deep neural density estimators to identify mechanistic models of neural dynamics
Gonçalves .. Macke 2019
About how to use deep learning to guess neuronal parameters to fit the actual activity of the network (?) They seem to be looking at actual V(t) though.

Richards, B. A., Xia, F., Santoro, A., Husse, J., Woodin, M. A., Josselyn, S. A., & Frankland, P. W. (2014). Patterns across multiple memories are identified over time. Nature neuroscience, 17(7), 981.
https://www.nature.com/articles/nn.3736

Lillicrap, T. P., & Scott, S. H. (2013). Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics. Neuron, 77(1), 168-179.
https://www.sciencedirect.com/science/article/pii/S0896627312009920

Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS computational biology, 10(11), e1003915.
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915
Very well cited, this one.

Kording, K. P., Kayser, C., Einhauser, W., & Konig, P. (2004). How are complex cell properties adapted to the statistics of natural stimuli?. Journal of neurophysiology, 91(1), 206-212.
https://www.ncbi.nlm.nih.gov/m/pubmed/12904330/

Sorscher, B., Mel, G., Ganguli, S., & Ocko, S. (2019). A unified theory for the origin of grid cells through the lens of pattern formation. In Advances in Neural Information Processing Systems (pp. 10003-10013).
https://papers.nips.cc/paper/9191-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation
Supposedly, explains the development of grid cells, synthesizing two existing theories (recurrent with lateral inhibition and spontaneous development during navigation?)

Michaels, J. A., Schaffelhofer, S., Agudelo-Toro, A., & Scherberger, H. (2019). A neural network model of flexible grasp movement generation. bioRxiv, 742189.
https://www.biorxiv.org/content/10.1101/742189v1
Modeled vision-controlled grasping (based on monkey data) as convolutional network (based on Alexnet, it seems) → RNN (fully connected) → bottlneck of 8 neurons → another RNN → another bottleneck → another RNN → output. They call these bottlenecks "sparsity", which is interesting. I wonder whether it is fair to call it brain-inspired. Is it something that works, as it forces the learning of representations, and then only claim that it is brain-inspired, to give it some conceptual validity? Or is there actual evidence in the brain that connections between motor systems are throttled like that? Something to explore. Also they mention some "feedback connections" that may be connections between RNN modules, but I'd need to read the methods section carefully to see what's happening there. One of the most important aspects of this paper for me may be the neuro-inspired analysis of model performance that they do, and the very attitude to validation that they have. I wonder if that's something that can be applied to spiking networks.

# Memory storage

Context-modular memory networks support high-capacity, flexible, and robust associative memories. (2020). William F Podlaski,  Everton J Agnes,  Tim P Vogels
https://www.biorxiv.org/content/10.1101/2020.01.08.898528v1
About Hopfield limit. It seems they use an architecture in which large groups neurons or synapses can be modulated (via gating, inhibition) in a context-dependent manner. Show an increase in memory capacity; changes in active components (states), robustness to noise, memory search (??), memory stability. Gating of memories. Substrate for continuous learning? Related to the problem of catastrophic forgetting, including [this paper](https://www.pnas.org/content/115/44/E10467.short).
There's also a [tweetprint](https://twitter.com/TPVogels/status/1215572496570896384).

Masse, N. Y., Yang, G. R., Song, H. F., Wang, X. J., & Freedman, D. J. (2019). Circuit mechanisms for the maintenance and manipulation of information in working memory. Nature neuroscience, 1.
https://www.biorxiv.org/content/biorxiv/early/2018/05/21/305714.full.pdf

# Bottom-up validation

Misra, D. (2019). Mish: A Self Regularized Non-Monotonic Neural Activation Function. arXiv preprint arXiv:1908.08681.
https://arxiv.org/abs/1908.08681
Claim that if you make activation function non-monotonic (almost like a sigmoid, but with slight overshoots), it actually helps in some ways.
Github: https://github.com/digantamisra98/Mish
May be a curious parallel to this recent finding in Neuro, about XOR computation by dendrites of an individual cell Gidon, A., Zolnik, T. A., Fidzinski, P., Bolduan, F., Papoutsi, A., Poirazi, P., ... & Larkum, M. E. (2020). Dendritic action potentials and computation in human layer 2/3 cortical neurons. Science, 367(6473), 83-87. (no free pdf)

Lappalainen, J., Herpich, J., & Tetzlaff, C. (2019). A theoretical framework to derive simple, firing-rate-dependent mathematical models of synaptic plasticity. Frontiers in computational neuroscience, 13, 26.
https://www.frontiersin.org/articles/10.3389/fncom.2019.00026/full

Lim, S. et al. Inferring learning rules from distributions of fring rates in cortical neurons. Nat. Neurosci. 18, 1804–1810 (2015).

Doerig, A., Bornet, A., Choung, O. H., & Herzog, M. H. (2019). Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines. bioRxiv, 744268.
https://www.sciencedirect.com/science/article/pii/S0042698919302299
Seem to be claiming that humans process images fundamentally differently than convolutional networks, because responding to perturbations follows a different logic. Sounds interesting!

Deep neuroethology of a virtual rodent
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Ölveczky
https://arxiv.org/abs/1911.09451
Apparently create a vidrual 3D rodent (like, with muscles, joints and what not), make it move in virtual environment, learn to move, then study its network using neuro methods.

Li, Z., Brendel, W., Walker, E., Cobos, E., Muhammad, T., Reimer, J., ... & Tolias, A. (2019). Learning from brains how to regularize machines. In Advances in Neural Information Processing Systems (pp. 9525-9535).
https://arxiv.org/abs/1911.05072

Whiteway, M. R., & Butts, D. A. (2019). The quest for interpretable models of neural population activity. Current opinion in neurobiology, 58, 86-93.

Feather, J., Durango, A., Gonzalez, R., & McDermott, J. (2019). Metamers of neural networks reveal divergence from human perceptual systems. In Advances in Neural Information Processing Systems (pp. 10078-10089).
https://papers.nips.cc/paper/9198-metamers-of-neural-networks-reveal-divergence-from-human-perceptual-systems.pdf
Metameres: in this case, different stimuli that cause identical activation in a part of a network. They seem to be claiming that there's a difference between humans and ANNs here.

Calhoun, A. J., Pillow, J. W., & Murthy, M. (2019). Unsupervised identification of the internal states that shape natural behavior. Nature Neuroscience, 1-10.
https://www.nature.com/articles/s41593-019-0533-x
Use unsupervise learning to identify internal (latent) states in a fly; then correlate these states with activity of individual neurons.

Spoerer, C. J., Kietzmann, T. C., & Kriegeskorte, N. (2019). Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition. bioRxiv, 677237.
https://www.biorxiv.org/content/10.1101/677237v3.full
Recurrent convolutional network  works better than a feed-forward convolutional network. It's slower, but better. Claim that it's similar to primate vision somehow.

# Large-scale dynamics

Recurrence is required to capture the representational dynamics of the human visual system
Kleinman
https://www.pnas.org/content/116/43/21854
Visual representation, clustering objects, cortex, primates

Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales
Kaplan .. Zimmer
https://www.cell.com/neuron/fulltext/S0896-6273(19)30932-8
C elegans, activation dynamics. How fixed action patterns emerge from (are encoded by) the nervous system.

# Decision-making

https://www.biorxiv.org/content/10.1101/798553v1 
Recurrent neural network models of multi-area computation underlying decision-making Michael Kleinman, Chandramouli Chandrasekaran, Jonathan C Kao
It seems that they try recurrent networks (RNN) running in parallel, and compare their decision times to that from monkey cortex. Then show that one RNN doesn't match the distribution of times, but if you have several running in parallel, and then synthesizing info, you get similar results? Claim that different modes of distributed computation are experimentally testable like that.

Zoltowski, D. M., Pillow, J. W., & Linderman, S. W. (2020). Unifying and generalizing models of neural dynamics during decision-making. arXiv preprint arXiv:2001.04571.
https://arxiv.org/abs/2001.04571

# Variability

Brembs, B. (2011). Towards a scientific concept of free will as a biological trait: spontaneous actions and decision-making in invertebrates. Proceedings of the Royal Society B: Biological Sciences, 278(1707), 930-939.
https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.2325

Murakami, M., Shteingart, H., Loewenstein, Y., & Mainen, Z. F. (2016). Distinct sources of deterministic and stochastic components of action timing decisions in rodent frontal cortex. bioRxiv, 088963.

Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., & Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature neuroscience, 22(12), 2066-2077.
https://www.biorxiv.org/content/biorxiv/early/2018/10/11/439885.full.pdf

# Free energy


Gershman, S. J. (2019). What does the free energy principle tell Us about the brain?. arXiv preprint arXiv:1901.07945.
https://arxiv.org/abs/1901.07945

The Markov blankets of life: autonomy, active inference and the free energy principle
Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston and Julian Kiverstein
Published:17 January 2018
https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0792

Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature reviews neuroscience, 11(2), 127.
3k references! So people do find it useful after all?

https://www.frontiersin.org/articles/10.3389/fnsys.2019.00042/full 
The Dialectics of Free Energy Minimization Evert A. Boonstra and Heleen A. Slagter
Seems to be a review explaining the free energy minimization principle for an organism. It's not math though; looks almost like philosophy?

# Other

Harnessing behavioral diversity to understand neural computations for cognition Simon Musall, Anne E Urai, David Sussillo, Anne K Churchland
[https://www.sciencedirect.com/science/article/pii/S0959438819300285]
Potentially the most important section: "Relating rich behavior to neural activity by studying ANNs"

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.

Learning predictive structure without a teacher: decision strategies and brain routes
Zoe Kourtzi, Andrew E Welchman
https://www.sciencedirect.com/science/article/pii/S0959438818302393 

The language of the brain: real-world neural population codes J Andrew Pruszynski, Joel Zylberberg
https://www.sciencedirect.com/science/article/pii/S0959438818302137 
Mostly (only?) about motor cortex, and how representation works there. Seems useful and accessible.

Data-driven models in human neuroscience and neuroengineering Bingni W. Brunton, Michael Beyeler. 
A review of current ML solutions for this general problem; still may be useful.

An argument for hyperbolic geometry in neural circuits
Tatyana O Sharpee
Interesting, but not immediately clear. But intriguing. Not much math, so should be very readable.

Mind the last spike — firing rate models for mesoscopic populations of spiking neurons Tilo Schwalger, Anton V Chizhov
https://www.sciencedirect.com/science/article/pii/S095943881930039X 
Review

https://www.biorxiv.org/content/10.1101/837567v1 
Interrogating theoretical models of neural computation with deep inference
Bittner .. Cunningham 2019
Apparently a very similar paper (to Goncalves 2019), but independent.

Balleine, B. W. (2019). The Meaning of Behavior: Discriminating Reflex and Volition in the Brain. *Neuron*, *104*(1), 47-62. 
Review. Potentially an interesting paper on free will, behaviorally (and computationally?) defined.

Classes of dendritic information processing Alexandre Payeur, Jean-Claude Béïque, Richard Naud

Constraining computational models using electron microscopy wiring diagrams Ashok Litwin-Kumar, Srinivas C Turaga

Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2, 4.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/
1.5k citations; seminal conceptual work apparently. Important for ML?

# ML of brain data

Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504.
https://www.biorxiv.org/content/10.1101/133504v2.abstract
A review.



# General To-Dos


Glance through last few lectures of Google Course (are they all on production?): https://developers.google.com/machine-learning/crash-course/production-ml-systems 

Matrix calculus for deep learning
https://explained.ai/matrix-calculus/index.html

Awesome collection of short tutorials (both ML and snippets of Python code) by Chris Albon:
https://chrisalbon.com/
Look through, decide the priority, and how to file.

Interesting practical intro to ML projects; look through:
https://github.com/chiphuyen/machine-learning-systems-design

ESL progress. 
Current position: p290

Parked parts that need to be revisited:
* p161 to p218 - Smoothing, wavelets, smoothing kernels, local regression, kernel size choice
* p219 - a whole chapter on Model selection, Bias-Variance Tradeoff, effective number of parameters, cross-validation, and bootstrapping
* p261 - another whole chapter. Model averaging, max likelihood, some Bayesian inference, more bootstrapping, the EM algorithm (Expectation-Maximization), Gibbs samping, bagging, MCMC

# Words and topics to look up

**Classic ML and friends**
* gradient boosting
* the difference between tSNE and UMAP
* what's the difference between hash-sets, hash-maps, and hash-tables?
* Latent Dirichlet Allocation ([ref](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0))
* radix-sort 
* that visual blog post about types of DL optimizers
* make sure all graph algorithms are covered
* Hungarian algorithm
* Bloom filter
* Kalman filter (and its friends)
* Expectation-Maximization
* Bayes Information Criterion - will be naturually described once I cover model selection?
* Bayesian PCA
* more on Gaussian processes
* Graphical models
* Hidden markov models
* Markov random fields (precursor of modern graph models?)
* Variational inference
* Influence functions ([this pdf may help](https://arxiv.org/pdf/1810.03260.pdf))
* Resampling techniques: SMOTE, isotonic regression, Platt scaling
* Principal variables
* Mahalanobis distance
* Particle filter

**Practical stuff:**
* Checkpoints
* Keras hyperparameter optimization

**Deep Learning:**
* What's so eager about eager mode
* How to tell the receptive field of a neuron in a deep network? Do they gradient descend on it?
* Word2vec - how does it work? And does it come pretrained or what?
* residual blocks - why are they even a good idea? What's the deal here? How can it possibly help?
* Alexnet - some sort of efficient visual net that people use as a module. Is it pre-trained?
* Maximum Mean Discrepancy trick
* Replica trick and log-partition function (??)
* Gumbel-max trick
* Russian roulette trick
* Bayesian conjugacy trick
* Reparameterization trick
* Duality trick, easier Fenchel
* Doubly Reparameterized Gradient Estimators
* Do people use leakyReLus often? Do they prefer them for pruning, or is it ignored?
* Xavier initialization of weights - what's cool about it? Also, is it true that variance-scaled init is better? ([claim](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/))
* inception blocks
* noise-contrastive estimation
* self-supervised boosting
* Wavenet - that famous convolution network that generates audio on-the-fly

**Technical stuff**
* regex in Python
* JSON - how does it work, why everybody love it, and is it the present or the future? ([wiki](https://en.wikipedia.org/wiki/JSON))
* YAML - some standard for keeping stuff that's like XML, but human-readable. Who uses it? Is it popular? ([wiki](https://en.wikipedia.org/wiki/YAML))
* bash
* Python closures, why they are a thing, what's dangerous about them, and how to use them
* Jax: [Baysian NN in Jax](https://colab.research.google.com/drive/1gMAXn123Pm58_NcRldjSuGYkbrXTUiN2) - collab notebook 

**Other math**
* How to calculate single-value-decomposion and eigenvector decomposition in practice? How do they code it for numpy, for example?
* FFT
* Lagrange multiplyier - is there an easy (not formal, but intuitive) proof that it works?
* Hamiltonians and how dynamical systems invoke them
* Perron–Frobenius theorem - something related to centralities and or network graph eigenvectors
* kuramoto oscillations



# Tools, coding, project management

**Cheatsheets:**
*  - GIT cheat sheet
*  - SQL cheat sheet
*  - Pandas obviously :)

**Advice on ML projects:**
*  - a collection of rules of thumb about everything Deep Learning (which defaults to pick etc.)

Other (s):
*  - how to properly create and support this knowledge base
* [How to do citations with Zettlr](https://docs.zettlr.com/en/academic/citations/) - , this must be nice! 

# Python

**References:**
* Tips from Chip Huyen: https://github.com/chiphuyen/python-is-cool
* Nice [list of Python gotchas](https://www.toptal.com/python/top-10-mistakes-that-python-programmers-make) from Martin Chilikian
* f-strings: http://zetcode.com/python/fstring/ , and [this specification](https://docs.python.org/3/library/string.html#format-specification-mini-language) (mini-language!)

**Random Python tips:**
* If in a module you start a method name with one underscore, like `_helper`, this method isn't imported on `from module import *`. Unfortunately it is still accessible if you do `import module`and address it as `module._helper()`.
* Conventions: underscore for variables and functions; all-caps for constants, capitalized camelcase for objects, leading-underscore for local methods.
* Nested comprehensions: same syntax as in writing nested loops (even tho it looks unformulaic), e.g. `[j for i in range(5) for j in range(i)]
* To add += 1 to a dict when a key may not exist, use `get()` as it has a default value: `a[i] = a.get(i,0)+1`
* To get some (or rather, first) key from a dict: `next(iter(a.keys()))`
* Objects (including empty lists `[]`) should never be used as default arguments for functions, as they are evaluated only once per program (during object definition), not when methods are called! Insetad use `x=None`, then `if x is None: x=[]`. It sounds super-cumbersome, but that's just how it is. ([source](https://docs.python-guide.org/writing/gotchas/))
* **Docstring**: First constant in a declaration, starts and ends with triple double quotes `"""`, accessible via `object.__doc__` property. Minimum: one sentence, capitalized, with a full stop at the end, explaining what this function does. Don't include the name, or usage. Any other comments - lower, after an empty line. For modules, similar, at the very beginning, before any declarations. Refs: [1](https://www.python.org/dev/peps/pep-0257/), [2](https://www.pythonforbeginners.com/basics/python-docstrings)
* **F-strings**: `f"bla {x['a']:.2f}"` - this version supports dicts (because diff quotation marks), and formats the output (after `:`).

## Matplotlib
* Brief intro from Brad Solomon: https://realpython.com/python-matplotlib-guide/
* Cheatsheet: [https://github.com/rougier/matplotlib-cheatsheet](<https://github.com/rougier/matplotlib-cheatsheet>)

## Coding habits for data scientists
* Keep code clean (not smelly). Types of **smells** ([ref](https://www.thoughtworks.com/insights/blog/coding-habits-data-scientists)):
    1. Dead code (commented out, inconsequential)
    2. Print statements everywhere
    3. Bad variable names
    4. Functions that do too many things instead of one thing
    5. Code repetition
    6. Magic values
* Smuggle code from Jupyter to classes as soon as possible (Jupyter only for protopying, reporting, and use case)
* Write unit tests ([link to a decent intro](https://www.freecodecamp.org/news/an-introduction-to-testing-in-python/))
* Make small and frequent commits

# TF and Keras
* Links to several tutorials: https://github.com/sayakpaul/TF-2.0-Hacks/blob/master/README.md

Random Notes:
* **Tensor object**:  type, shape, and a bunch of numbers. For example, when working with images, we have a 4D structure: image# × W × H × ColorChannels.
* TF relies on a function that iterates through (features, labels) as tuples. And instead of directly linking to data, it wants to receive a data-generating function (for lazy / parallel execution?)

**Data Pipeline**: TF wants a Python iterable. If data can be loaded in memory, use `tf.data.Dataset.from_tensors()` or `from_tensor_slices()`. Later, a dataset object may be transformed using various TF methods, like `shuffle()` for reshuffling, or `batch(bath_size)` to create a batched dataset.

When building the model, for the first layer, set `input_shape` to a tuple describing the dimensions, assuming that the first (silent, unspecified) dimension will be the batch size. TF can also directly 'cosume' Python generators (functions that produce values lazily, using `yield` instead of `return`), although documentation warns that there are some potential traps with this.

Three different ways to fit:
* `fit` - best choice for small datasets. Takes x and y explicitly; you specify `batch_size` as a parameter, as it is the fit that will do batching. Good if entire set fits in RAM, and there's no data augmentation (no on-the-fly data, so to say). [ref](https://www.pyimagesearch.com/2018/12/24/how-to-use-keras-fit-and-fit_generator-a-hands-on-tutorial/)
* `fit_generator` - accepts a data generator instead of static data, and in this case it is the Dataset generator object (or your custom function) that specifies the shape, including batch size. With `steps_per_epoch`, you can tell it just how many new data points to generate per epoch. Note that with fit_generator, you'll have an extra dimension for batch in the model, at the inputs level, so for prediction you'd either have to use `predict_generator` (which makes sense if you generate data for it on the fly), or expand x with a leading empty axis using numpy (np.reshape(1, … )), and then np.squeeze(y) the output back to normal.
* `train_on_batch` - trains on one batch and updates the mode.

# ML Project Organization


Basic ideas:
1. Define scope, feasibility, targets (accuracy, speed, size?)
2. Define ground truth
3. Validate data, swim in it, write data tests upfront.
4. Version your data
5. Identify baselines (stupid / simplistic models). Fit and overfit with them
7. Is there a SoTA model?
8. Start very simple (aka "Don't be a hero")
9. Develop model by iterating through a cycle ([ref](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)): 
        * add complexity; 
        * debug; 
        * tune hyperparameters, 
        * benchmark
10. Test on test data
11. Write tests for model performance (to be alerted if it unexpectedly drops on new data)

How to fight bad fit:
* Underfitting: bigger model (model capacity); less regularization; advanced architecture; hyperparameters; more features
* Overfitting: more training data; regularization; data augmentation; reduce model size

Refs:
* [Data Science Checklist](https://www.fast.ai/2020/01/07/data-questionnaire/) by Jeremy Howard
* How to tell whether an ML solution is ready for production? 
* Blog of [Jeremy Jordan](https://www.jeremyjordan.me/ml-projects-guide/)
* Blog of [Aseem Bansal](https://medium.com/infinity-aka-aseem/things-we-wish-we-had-known-before-we-started-our-first-machine-learning-project-336d1d6f2184)
* [Presentation by Josh Tobin](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)
* [Andrej Karpathy on training NNs](http://karpathy.github.io/2019/04/25/recipe/)
* [Advice from Chip Huyen](https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf)

# Linear Regression
**Linear model**: h(x) = estimation for y = $∑θ_i x_i$ = xᵀθ, or y ~ h = Xθ in matrix form. Here we assume that xᵀ is a row-vector length p+1, with x_0 = 1 (intercept), followed by n variables. The equation above defines a hyperplane in ℝ^p.

## L2 loss
Loss: J(θ) = ∑(h-y)² across all data points i. Is called **Squared error**, or **squared distance**, or **Mean Squared Error** (MSE), or **Least Squares**. Sensitive to outliers, permissive to small deviations around zero (because of its convex shape). Nice practical illustration from [Google crash course](https://developers.google.com/machine-learning/crash-course/): 2 outliers of 2 are worse than 4 outliers of 1, as 8>4. 

We can minimize this loss by differentiating by θ (partial derivative for each of the coordinates). For one point: ∂J(θ)/∂θ_j = by definition of J: ∂/∂θ_j (h(θ,x)-y)² = simple chain rule: 2(h-y) ∙ ∂h/∂θ_j = by formula for h: 2(h-y)∙x_j . Minimum: dJ/dθ = 0, ⇒ (derivative above written in matrix notation): Xᵀ(Xθ-y) = 0. Here X is a matrix in which each _row_ is an input vector, and y is a column-vector of target outputs. If XᵀX is nonsignular, we can open the brackets, send y to the right, muliply from the left on inverse, get: θ = (XᵀX)⁻¹Xᵀy.

> To sum up **notation choices** (roughly matching ESL): one point is a row-vector, so all points make a column (a column of row- vectors). The values of y are also a column. As data-point is a row, parameters θ are a column, so that dim(y) = N×1 = dim(Xθ). A bit confusing part when writing it all is that X (matrix) of N×k is multiplied by θ from the right (Xθ), but individual points x have to be written as xᵀθ to show that they are row-vectors (by default x would have been a column-vector). As Unicode makes diacritics a hassle, I try to use θ for parameter estimates, and β for true values. x_i is typically a column of X. Subscripts are shortened (xi instead of x_i) if it looks ok. Cdot (∙) may mean both normal and inner product; ⟨x,y⟩ always means inner product. 

Interpretation: if y ~ h = Xθ, then h is in the column-space of X, which forms a subspace of R^N (N data points, p+1 dimensions, assuming x0≡1). To find θ, we project y to col-space using a projection matrix, which obviously minimizes residual (unexplained) variance, by making it orthogonal to col-space.

Another interpretation: 
* for a univariate x, we have a classic projection of vector y (all observations) on x (all input values): y ~ h = xθ = x ∙ xᵀy / xᵀx (by definition of projection). 
* If x is a vector, with several coordinates, but these coordinates are independent (say, in case of orthogonal experiment design design), so columns of X are orthogonal, we can just project to each dimension separately, and the result will be the sum of these projections. That's because once you dot-product y=∑θ_i x_i with x_j, all terms but one would die. Which means that essentially we can pretend that multiple linear regression is just a bunch of univariate regressions. It also matches the general formula, as in this case XᵀX is diagonal. 
* If however columns of X are correlated (not orthogonal), we have to do repetitive elimination ( aka **Gram-Schmidt**) that leads to a full form projection matrix. See below.

Or we can perform a gradient descent: θ := θ + α(h-y)x, with some learning rate α. As this loss is a convex quadratic function, it has only one minimum, and so it always converges. 

## Gram-Smidt
A computationally better alternative to working with inverse matrices. The idea: beak y into x1 and everything else, as if the basis was orthonormal, changing it on the fly. (In practice, x0≡1 goes before x1, but it doesn't change the idea)

Algorithm (without normalization):
1. Start with xi set to x0
2. Replace xi  with its projection to the complement of all previously considered coordinates: qi = xi - ∑proj(xi→qj) = xi - ∑qj∙⟨xi,qj⟩/⟨qj,qj⟩ where j=0 to i-1. Remember all scalar products from this formula as cji = ⟨xi,qj⟩.
3. Find the best projection of y on qi: γi = ⟨y,qi⟩
4. Shift current x from xi to x_i+1. Go to step 2.

This produces a split of X into X = QR where Q with dimensions (N × p+1) is column-orthogonal, and R with dim of (p+1 × p+1) that is upper triangular, with elements cji=⟨xi,qj⟩/⟨qj,qj⟩ (in case or not-normalized basis, 1s on the diagonal). And we get a set of estimates {γ_i} that represent y in the basis Q.

Now as X = QR, our Xθ = y becomes QRθ = y, but we also just found Qγ = y, so Rθ = γ. It gives θ = R⁻¹γ. 

> What I still not quite understand here is why R remains, meaning that we still have to calculate an inverse for it. Could not we somehow manage to tuck it into the calculation as well, to get θ directly? And if not, how do we calculate the inverse in practice? (How to make it computationally effective)? Park for now.

Refs: ESL p54, [some lecture](http://homepages.ulb.ac.be/~majansen/teaching/STAT-F-408/slides01multipleregression_4.pdf)

In a general case, coordinates of X may be dependent, or close to it. Then, XᵀX from a general formula will be singular, and won't have an inverse, meaning that θ is not uniquely defined. If we use orthogonalization, and two columns of X are highly correlated (say, x1~x2), then x1 will get a normal score, but on the next step we'll have to face a vector that is almost pure noise: q = x2-proj(x1→x2) ~ x2-x2 = 0. Then both qᵀy and qᵀq will be very small, and θ_2 = qᵀy/qᵀq will be very unstable. Estimate for variance: var(θ_2) = σ²/qᵀq. Practical consequences for variable selection (dangers of collinearity), conjuctive dim reduction, ridge regression (why not just zero this θ if it's so bad?) etc.

If Y is multivariate as well, but all errors in Y are independent, we just have several independent linear regressions. If errors in Y are correlated, the very definition of RSS is changed (to encorporate the covariance matrix), but we don't go deep into it here. Refs: ESL p56

## Bias-Variance Tradeoff

Total prediction error (L2 loss) consists of two parts: **bias** and **variance**. 

Derivation: Say, we regress y by x on a dataset (x,y) generated by a distribution P(x,y).
For a given x, we can have P(y|x), and P(x,y) = ∫ P(y|x)P(x) by x.
Regression predicts expected y(x), or Ey (x). 
By definition Ey (x) = ∫y P(y|x) , integrated across all possible x.
ML is about guessing (predicting) E y(x) well using some algorithm.
Say, h(x) is our guess for Ey (x), produced by some method A, based on a training set D.
The expected error for this estimation h is: J = E (h-y)² = ∬ P(x,y)(h(x)-y)² by both x and y.
Now, h itself is a random variable, as it's a function of D, which is a random sample from (x,y).
So we can consider expected value: Eh, by integrating over all possible training sets D.
We can estimate it by training lots of times and averaging. Or, if you are only interested in E(h), you could also just apply A to the full dataset, obtaining the best h possible.
Now, the expected error for the algorithm (across all possible classifiers it could possibly produce) is the error integrated over all possible h (and so, over all Ds), so we have a triple-integral (by D, x, y):
$\int_D \int_x \int_y (h(x)-y)^2 P(D) P(x,y)\,dD\,dx\,dy$
Add and subtract Eh (mean h) inside the square; get (h-Eh) and (Eh-y); open squares.
(h-Eh)² becomes variance.
2(h-Eh)(Eh-y) dies out because for each fixed x and D you get a fixed (Eh-y), and int_D (h-Eh) = 0 by def of Eh.
The term (Eh-y)² will eventually become bias, but it needs some massaging first. Similar to what we did before, subtract and add Ey, then open squares.
We're left with (Eh-Ey)² and (Ey-y)². The term 2(Eh-Ey)(Ey-y) dies off at int_y step. 
(Just make sure to integrate by y at the deepest level, to get 0 before int_x has a chance to see it).
So we ended up with 3 terms:
* Variance (of the classifier): (h-Eh)²
* Bias: (Eh-Ey)²
* Irreducable error, aka noise, aka variance of the data: (Ey-y)²

It seems to be somewhat similar to precision / recall situation. If you try to minimize bias (make your classifier cling to the data), you fall at mercy of your training set, and so increase variance. Each particular classifier, understood as an instance produced by some particular set of training data, will cling to this testing data, so all classifiers will be slightly different, if you train on different data. If however you believe that the underlying solution has some constraints to it, you can restrain your classifier, even at a cost of accepting higher bias (Eh-Ey), so that it would not change that widely for different subsets of your testing data. Regularization and the use of simplified, constrained models, do that. Then for each given training set you'll get higher bias, but lower variance (of your classifier, across all possible training sets), as all models will be more similar to each other (and hopefully also closer to the "ground truth"). A parsimony principle: an assumption that simpler models are better.

Whether this effect will be manifest IRL depends on whether it is exacerbated by overfitting. When a constrained model tries to cling to training data, it may also diverge in parts of the landscape that have no or few observed training values, driving variance up. (Imagine a textbook picture of a polynomial overfitting here). In this case, low bias ⇒ high variance ⇒ great fit on the training set ⇒  horrible fit on parts of the testing set. This is because variance can be further split into **variance due to sampling** and **variance due to optimization** , and it is the strength of the 2nd component of variance that matters.

There are two typical graphical illustrations for it. One shows how training and validation error rates change as you increase the complexity of the model. Both training and testing losses first go down, but at some point overfitting kicks in and turns the test curve up. Another canonical illustration looks like a parabola (total error) lying on two meeting hyperbolas (left horn for bias, right horn for variance): as you change model complexity, first error goes down because the bias goes down, but then increases again as variance starts to rapidly increase.

> But look, it would only work if the ground trugh is actually simple. Why does it actually work? Why would the ground truth be simple, in practice? Is it some expected statistical property that the world actually follows, similar to how the central limit theorem shows that the world follows certain statistical patterns? Or is it because a typical practical problem has certain properties, by virtue of being practical (some sort of selection bias)? What's the philosophy behind that?

Refs: ESL p24, p37; [lecture by K Weinberger](https://www.youtube.com/watch?v=zUJbRO0Wavo); .

### Variance of parameter estimations
How well can we guess θ? Assuming that y_i have normal uncorrelated noise with variance σ², we get var(θ) = (XᵀX)⁻¹σ², where σ² can be estimated from σ² ~ ∑ (y-h)² / (N-p-1). (ESL p47) In this case, θ are destributed multivariate-Gaussian, and estimations for σ² are chi-squared with N-p-1 degrees of freedom. Which also means that we can use t-test for particular θ_i.

For **categorical variables** (in X), the situation is a bit different, as each of them is represented by several **dummy variables**  (one for each of the levels), and these dummy variables need to be included or excluded all together. F-statistics (with p1-p0, N-p1-1 degrees of freedom). This is also known as **one-hot encoding**.

### Gauss-Markov theorem
**Theorem:** The least squares estimate for β has the smallest variance of all unbiased linear estimators for true parameters β.

In other words, consider that we actually have a linear system plus some independent errors for each data point: y = Xβ + ε. We're saying that the estimation θ obtained by the least squared process ( θ = (XᵀX)⁻¹Xᵀy ) is unbiased, in the sense that E(θ) = E(β), and crucially, Var(θ) is minimal among all possible unbiased linear estimators ( ξ = Cᵀy ) for β. 

It can be proven through direct matrix substitution into the general formula for least squares. The fact that least squares is unbiased is easy: as Xθ is always linearly fit to y, y-Xθ = 0 for any fixed set of y, so E(y-Xθ) = lim(y-Xθ) for N→∞ also = 0 (it never deviates from it). Or one can directly torture E(ξ) after expressing y through true β, and assuming that C=(XᵀX)⁻¹Xᵀ + some non-zero matrix D, as wiki does it. The key part, goes similarly; uses the fact that Var(Cy) = C∙Var(y)∙Cᵀ, and arrives at Var(ξ) = Var(θ) + DDᵀσ² (?).

> Park for now. I don't like it. Supposely it's somehow related to the triangular inequality, which suggests that there should be some reasonable geometric intuition to it, but for now I cannot find it.

In practice, it means that there are estimators with lower variance, but they are necessarily biased. Which leads to a practical program of introducing some bias in order to reduce variance. For example, if we decide to set (aka "shrink") any particular θ_i to 0, we inevitably get a biased estimate.

Refs: ESL p51, [wiki](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem)

### Dimensionality curse
Now let's consider a special case of bias-variance trade-off in a case of very high dimensions. Say, we have a linear model with error: y = Xᵀθ + ε (aka **additive error model**), where ε is normal noise with variance σ². Try to infer θ from a training set of Y. As θ = (XᵀX)⁻¹Xᵀy, we get it with errors of (XᵀX)⁻¹Xᵀε. Which means that whe we are estimating y as h = Xθ, during training we get errors for these estimations: X(XᵀX)⁻¹Xᵀε . 

J for one point xᵀ (x0 = one row in X) is given by a bias-variance formula: J = E( (h-Eh)² + (Eh-y)² ) = xᵀ(XᵀX)⁻¹xσ² + Bias + σ². Here Bias≡0 because linear fit is unbiased, and σ² is irreducable error.

If training is random, (XᵀX)→N∙Cov(X), so we have:
J ~ E( xᵀCov(X)⁻¹xσ²/N ) + σ²= trace(Cov(X)⁻¹Cov(x))σ²/N + σ² = pσ²/N + σ², where p is the number of dimentions. So J grows as a linear function of the number of dimentions p.

> This trace thing here is pretty annoying, but it works. If we E() the left part, we get a E() of sum_ij (x_i x_j C⁻¹ij), which is p^2 terms. But if you open this trace(C∙C⁻¹), you get the same expression (j coming from trace-sum, and j from internal multiplication). So E() indeed gives the trace(), and then as trace() only runs through p terms, and all of these terms happen to be =1 by def of CC⁻¹, we get what we get.

Which means that the higher the number of dimensions, the worse the error (variance). Now, some methods are worse than others: linear regression is actually faring OK, while some (like KNN regressor) get cursed pretty fast (ESL p26), but the gist is the same for all. If you simulate this, you have diff deterioration curves for different data assumptions. IRL, we try to be in-between these two extremes (linear regression and 1NN) using some info (or assumptions) about the structure of the data. 

# Philosophy of prediction
Returning to bias-variance tradeoff, we consider x and y as random variables, with a joint distribution P(x,y). We're trying to build a predictor f(x) that approximates y. The squared error loss: L = (y-f)² = ∬ (y-f)² P(x,y) dx dy. We can split P(x,y) into P(x)∙P(y|x), and integrate by y first (inside), then by x outside. Then to minimize total L, we can bind f(x) to matching y point-wise: f(x) = E(y|X=x). Which would essentially give us the **nearest neighbor** method (see the very beginning of "Classification" chapter). An alternative to point-wise matching would be a global smooth model, like linear regression. It makes regression and 1NN two opposite extremes of what can be done with the data. Other stuff (like **additive models** where f = ∑ f_j(x)) are kinda in-between.

In spirit, all models are about setting local and global constraints on the behavior of predictor (such as **behavior in the neighborhood**, that is const for KNN, linear for local linear etc.), and the **granularity** of these  neighborhoods. Some methods (kernels, trees) make this granularity parameter explicit, some don't. And high dimensionality is a problem for all methods, regardless of how they work. 

ESL defines 3 broad classes of model smoothing / constraining:
* Roughness penalty
* Kernel methods (aka local regression)
* Basis functions and dictionaries

**Roughness penalty**: Use basic RSS (Residual Sum of Squares) with an explicit penalty: L = RSS(f) + λJ(f), where J grows as f() becomes too rough. For example, defining J as λ∫(f'')²dx leads to **cubic smoothing splines** as an optimal solution. Roughness penalty can be interpreted in a Bayesian way, as a log-prior.

**Kernel methods**: explicitly specify local neighborhoods, and a type of function to fit it locally. The neighborhood is defined by the kernel function K(x0,x) that acts as weights for x around x0. Gaussian Kernel: K = 1/λ exp( - |x-x0|² / 2λ). The simplest way to use kernels, is to calculate a weighted average (aka Nadaraya–Watson kernel regression): f(x0) = ∑ K(x0,x_i)∙y_i / ∑ K(x0,x_i).

Or we can set some sort of smooth f(x), and use kernels for RSS calculations: RSS(x0) = ∑ K(x,x0)∙(y-f(x))² , where ∑ runs through all (x_i, y_i). If we assume f(x) = θ0 + θx, we get **local linear regression**. KNNs (see ) can also be considered a subtype of a kernal method, just with a weird step-wise kernel.

**Basis functions**: includes linear and polynomial regressions, but the idea is that you have a basis of functions on R^n, and project into it: f = ∑_i θ_i h_i(x). Examples: **polynomial splines**; **radial basis functions**  with K(μ, x) defined around several centroids μ that can itself be optimized. Gausssian kernels are still popular. Feed-forward **Deep Learning** actually also belongs here, it just htat basis functions are defined by network design (the space of functions that can be achieved with this particular network depth, activation functions etc.).

**Are there alternatives to L2?** Sure, **L1 norm** = abs(distance), which effectively pushes f(x) towards median(y) rather than the mean(y): sum of distances to 2 points is min when you're exactly between them. Hard to work with, as derivatives are discontinuous.

**Max Likelihood**: In some way, can be considered  an alternative to L2 (MSE) loss. For a data sample y_i, the log-prob of observing it is equal to: L(θ) = ∑ log P(y_i | θ), summed by i (all points). We try to find θ that maximizes ∏P, and thus ∑logP. It is connected to L2 though, as least squares maximizes likelihood for conditional probability P(y|x,θ)=𝒩(h,σ²) where h = Xβ. _ESL has a formula 2.35 for it, but without derivation. I think they may have found a derivative by θ, set it to zero, then integrate the right side by x, but I'm not sure._

# Constrained models
## Best-subset regression
Manually find a combination (subset) of k variables that offers best fit for this k. There's an efficient algorithm for that, called **Leaps and Bounds procedure**. Main ways to decide which one to use: empirical **cross-validation**, and AIC = **Akaike Information Criterion**. 

## Stepwise Regression
A (bad) compromise between performance and speed: automated stepwise selection of features based on the quality of fit, and a complexity penalty (regularization). Two main approaches: **Forward selection** and **Backward selection**. Or a combinationof both (*not sure how?*)

The maxim about stepwise regression is that it is useless for hypothesis testing, questionable for prediction, and OK for variable selection (see ). 
* It can't be used for hypotheses, as p-values after this procedure are meaningless, and arguably shouldn't even be reported, as their direct probabilistic intepretation is impossible, and adjustment is too complicated (p-values aren't independent, so you cannot do FDR), making adjustment impossible in practice.
* It can be used for prediction, but usually full models (or, assumably, probably regularized models) are more powerful anyways.
* It's an OK method to select variables tho.

Some people (like Thompson cited below) are really vehemently against them tho. That's because they are usually too optimistic (the very concept of degrees of freedom becomes problematic after multiple testing), yet are strictly less powerful than proper regularization, and also typically don't replicate, as the sequence of descent depends on the data, so performing stepwise regression on different subsets of data is much more likely to yield different results, compared to a more sophisticated model.

Refs:
* , [wiki](https://en.wikipedia.org/wiki/Stepwise_regression), ESL p59
* Steyerberg, E. W., Eijkemans, M. J., Harrell Jr, F. E., & Habbema, J. D. F. (2001). Prognostic modeling with logistic regression analysis: in search of a sensible strategy in small data sets. Medical Decision Making, 21(1), 45-56.
* Thompson, B. (2001). Significance, effect sizes, stepwise methods, and other issues: Strong arguments move the field. The Journal of Experimental Education, 70(1), 80-93. 

There's also a thing called **Forward-Stagewise Regression** that is based on correlations between remaining features and the current residual, and it is just plan horrible.

## Ridge regression
Also known as **Shrinkage Methods**, closely related **Tikhonov regularization**. Modern DL name: **L2 regularization** (see DL chapter).

Motivation: Consider Ax = b, and x doesn't exist. In a most typical practical case, it gives a superposition of an over-determined (still unsolvable) problem Aξ = b for the component of x that is in the row-space ( ξ = proj(x→RowSpace(A)) ), and so is affected by the actual value of the matrix A, and an under-determined problem A(x-ξ) = 0 for the components x-ξ that are in the null-space of A (orthogonal to the RowSpace(A), and thus projected to 0). The part x-ξ can be chosen arbitrary without any effect on the value of Ax. So we are free to pick this part in some "nice" fashion, to satisfy some priors, for example, or minimize solution complexity. 

Compared to variable selection, this approach is smooth and stable: while with model (variable) selection loss may oscillate pretty wildly as you add more features, with shrinkage, it usually changes monotonously.

This is especially important in case of **Multicollinearity**, when you're trying to predict y from many variables X, in a way Xθ = y (observations of variables x_i for different training points become columns of X, while regression coefficients form θ), but some of x_i are strongly correlated. In this case trying to painfully minimize y-Xθ would be counter-productive, as we'd fit noise in y with noise in x_i. Imagine an extreme case: all p columns of X are the same (rank=1), but are observed with noise, and noise is independent (so formally rank = p). What we actually need is only one (doesn't matter which one) θ_i>0, and all others ≡0. But what will happen, is that we'll fit noise in y with noise in X.

**Ridge regression**: we add a penalty on θ in the form of λ∑θ². Note that this sum runs from 1 to p, as θ0 (intercept) is not penalized. Included in the formula for h, but is not penalized. Alternatively, one can just insist that λ∑θ²<t; because the rest of the loss is defined by {X,y}, for any given task there's an exact match between λ and t. As the values of θ depend on the scale of x_i, standardization of x (de-biasing + normalization) becomes critical.

The name "**ridge**" comes from a visual example of what is describe above. Imagine that only part of the solution is well defined, and the rest is close ot null-space of X. Then the "true solution" is a "generalized cylinder" made by the true solution (in those coordinates that make sense), arbitrarily extended across the "irrelevant coordinates". Small changes in training data (right side of the Xθ=y equation) would sway the solution along this "ridge". By adding regularization we change a "ridge" into a "peak" (lines turn into parabolas), which stabilizes the solution against perturbations in both X and y.

**How to pick  λ?** One method: **Ridge trace**: plot found coefficients as a function of λ, and eyeball value at which they stop oscillating, and switches to converging (not in the sense of becoming const, but in the sense of switching from curving up and down to monotone almost-linear change). A better approach, of course, is **cross-validation**. Also, for any given λ, one can estimate the effective degrees of freedom (as a trace of certain matrix, see ESL p68), so for these plots it's common to use df(λ) for x axis, not λ itself.

Interestingly, if all x_i (columns, variables) are orthonormal, ridge regression has no reasons to penalize any one θ in particular, and just all of them get scaled by 1/(1+λ).

> I am not quite sure why this is the case, even though it feels like something that could be explained visually. It seems that the point where an ellipse and a circle touch doesn't necessarily lie on a straight line between the centers of said circle and ellipse.  : figure it out

**Tikhonov regularization**: a generalization of Ridge. In this case we minimize |Ax-b|² + |Γx|² where Γ is some matrix. If Г=identity matrix I multiplied by a coefficient λ, we have a sum of squared x_i, and so Ridge regression. 

> ESL p67 also seems to suggest that ridge regression is somehow related to PCA, or can be understood in terms of first moving to the PCA space, then doing something, then projecting back, but the math is sketchy, and no good comments, so I'll park it for now.

 : Deconstruct this post that seems to have a more concise and systematic summary of all math involved in parallels between shrinkage and PCA, and in the same system as ESL: https://stats.stackexchange.com/questions/220243/the-proof-of-shrinking-coefficients-using-ridge-regression-through-spectral-dec

Refs:
* [Stackexchange](https://stats.stackexchange.com/questions/118712/why-does-ridge-estimate-become-better-than-ols-by-adding-a-constant-to-the-diago/119708#119708), on visual interpretations
* ESL p61

## Lasso and Elastic Net
**L1 regularization**, aka **basis pursuit**: require ∑|θ_i| < t, or add λ∑|θ_i| penalty to the loss (again summing from 1 to p, meaning no punishment for the intercept θ0). Unlike for L2, it has no closed form solution (because abs() is harder to optimize).

While Ridge tends to scale θ_i values down with a coefficient, lasso tends to slide (shift) them towards zero by a certain value, until they bury in 0 and become 0. For an orthonormal X, this can be proven theoretically (ESL p71).

Visual representation: romboid and a quadratic (elliptic) optimum nearby. A circle (ridge) projected on an elliptic field always has a global minimum. A corner made of two lines (lasso) would either reach a mininum on a line (non-zero optimal θ), or at the corner (zero θ). Graphically, it's easy to see that θ sets to 0 the moment the unpenalized minimum leaves the band made of 2 other sides of the diamond (draw it to see it).

Generalizaton: one can try to use a penalty of λ∑|θ_i|^q, where q is some value in R. Foor q=2 we get ridge, for q=1 lasso, for q in (0,1) - something in-between.

**Elastic net penalty**: An mix of both Ridge and Lasso that can be used to approximate these generalized romboid shapes. It's way easier to calculate, and also shares the ability to set coefficients to exactly 0 (because of pointy points )

## Least Angle Regression
Similar to stepwise regression, but with penalized coefficients, as in shrinkage. The approach:
1. Standardize all variables
2. Set up the residual; initialize it as r=y-intercept
3. Add a variable x_i with a highest correlation to the bag. Set θ_i := 0.
4. Move θ (linearly; not as in gradient descent) towards the least-squares value ⟨x_i , r⟩. Calculate the total correlation of variables in the bag (Xθ) with the residual.
5. Calculate correlations of all runners-up (other x_j currently not in the mix) with the residual r. The moment some x_j becomes as correlated with r as the current Xθ, add x_j to the mix.
6. Repeat 4-5 until least squared is reached.

As a result of this procedure, we get a piecewise-linear change of θ with "iteration time". The results are pretty similar to that for lasso, except for histeresis around θ=0 point.

> Not sure what are the benefits of this approach though.

## Principal Components Regression
1. Standardaze columns of X.
2. Calculate **PCA** of X: Z=XV, where columns of Z are orthonormal.
3. Regress y on Z: Zξ~y. As z_i are orthonormal, it's like running p univariate regressions.
4. Recalculate θ from ξ: θ = Vξ, but only using first m biggest eigenvectors.

Similar to Shrinkage, in the sence that Shrinkage also ends up shrinking components with small eigenvalues more. With the principal components regression, we just zero them altogether. Unlike ridge, it can amplify "unworthy" coefficients in the original X space though, what would have been shrunk by both Ridge and Lasso.

Refs: ESL p79

## Partial Least Squares
1. Standardize columns of X
2. Start cycle i=1
3. Calculate φij = ⟨x_j , y⟩ for each i. Synthesize z_i = ∑ φij xj (by j)
4. Regress (project) y by z_i, get ξ_i.
5. Switch from y to residual (⊥ z_i)
6. Orthogonalize all x_j to z_i (project to ⊥ z_i)
7. Repeat 3-6 m times (m<p)
8. Project ξ back to θ

The idea behind is to let y pull signal from X, regardless of how this signal is encoded (mixed?) in columns of X. ESL claims though that "variance aspect tends to dominate", so in practice this method doesn't behave too differently than Ridge regression. Seems to also be similar in spirit to "Canonical Correlation Analysis" (see  section). Same as PCR above, may inflate weaker dimensions, making it unstable. Which all together is probably why it is not that widely used? 

Refs: ESL p81, [wiki](https://en.wikipedia.org/wiki/Partial_least_squares_regression)

# Classification


# 1NN and KNN
Simplest archetypical approach: **Nearest Neighbor**. Just pick the closest training case. This is an example of a **lazy** approach: instead of generating estimations upfront (that would be called **eager**), we only generate them at retrieval. A better. and more practical, approach: **K nearest neighbors** (aka KNN).

While KNN is lazy, for analysis purposes we can calculate predictions on a grid, and thus identify borders between areas "assigned" to different categories. For k=1 (simple Nearest Neighbor) we get a Voronoi tesselation between all training points (disconnected and jaggedy, as each point gets its own area). Which presents a case of extreme overfitting: all training data is correctly classified, but it looks scary. Higher k: smoother areas, making k a **hyperparameter**. But the **effective number of parameters** for KNN is higher (about N/k), as data points themselves serve as parameters.

**Can we use quadratic loss function for KNN?** No, coz it would go to 0 for k=1, so hyperparameter search would always recommend overfitting. We should use **Max Likelihood** instead: Say, you have data X and a qualitative output G with values g_k denoting several different classes. We can have P(G=g_k|X=x) = P_k(θ,X), and try to maximize ∏P for a given data. It's the same as maximising L = ∑log P(θ , g_i , x_i) in the space of θ.

KNN can be used for numerical predictions as well (beyond simple classification); just use the average of y-values for nearest k x-points (in fact, that's how ESL introduces it).

How to find optimal k? We'll need a training and a testing set, then try different values of k, and find the point when the test error (or some other measure) is lowest for test data (ESL Fig 2.4).

KNN is an archetype for many fancy methods. For example, if instead of "nearest" δi that are all or none we'll use fuzzy weights that are larger when you are close to each data w(distance), it's the same as just regressing on distance, which is a type of a **kernel method**. If we want some dimensions of X matter differently than others, we can use non-round kernels. **Local regression** is also similar in spirit (sorta a combinatino of linear regression and the idea of contextual locality). DL networks also imitate something like that by mixing and mashing linear transformations.

# Model assessement
**Model accuracy**: The most primitive measure = number of true statements / total number of statements. Obvious dependency on class balance (famous example, a statement of "today is not Christmas" has an accuracy of 99.7%). But may work for toy examples with very carefully balanced classes (like  mnist).

**Precision and recall**: Let's concentrate on detection only (positive predictions only). We want something that is proportinal to true positives (TP), but what to put in the denominator? Two options: 

* TP/(all positive predictions) = TP/(TP+FP) - how well can we believe positive predictions. Is called **Prediction**, and is related to false discovery rate (P=1-FDR). From the stats POV, it's about false-positives, or Type I.
* TP/(all actual positives) = TP/(TP+FN) - how well does the model actually detect cases, and how sure can we be that most positive cases were detected, that not much was left behind. **Recall**. From the stats POV, it's about false-negatives, or Type II.

Can we balance them somehow, to make sure the model is reasonably good on both accounts? Most popular approach: **F1 score**: F = 2∙(precision∙recall)/(precision+recall) = harmonic_mean(precision,recall) = inv(mean(inv(precision),inv(recall))).

An even better approach: **AUC** = **Area under the ROC (Receiver Operating Characteristic) curve**. With this approach, you take a threshold-like parameter of the model, and slide it through all possible values from ideally permissive to fully prohibitive. For max permissive we get maximal recall, or TPR=1, but also no true negatives at all: TNR=0 ⇒ FP=1 (that's how false-positives are defined, as 1-TNR). For max prohibitive, we get TPR=0, but also FPR=0, as TNR=1. For random probability, AUC=1/2, for perfect knowledge AUC=1.

Interestingly, AUC = P(score of a true example > score of a wrong example). _I wonder if proving this is hard?_ 

**Prediction bias**: a very simple measure of model validity: average value of all predictions - average value of all learning points. In a reasonable model, prediction bias should be close to 0. A way to assess it: build a **calibration plot** - bucket values, calculate predictions, then plot mean(predictions) against mean(values). May help to find areas where the model misbehaves.

Canonical system of names for **Validation and Testing**:
* **Training dataset** - obviously
* **Validation dataset** - used to calculate validation error; monitored alongside training dataset. The difference between error on validation and training sets is used for model selection, regularization, hyperparameters tuning.
* **Testing dataset** - ideally, whould never be seen by the model until the very end. But at the same time, as models are eventually evaluated based on their performance on the testing dataset, arguably, it indirectly participates in hyperparameter tuning and model selection. Still, try to minimize that.

**Cross-vallidation**: Instead of setting apart one validation dataset, generate lots of those by random splits. Recommendations for split are around 70/30 or 80/20, although extreme versions of 50/50, or "**leave one out**" also exist. Note that repeating random split 10 times is different from **10-fold cross-vallidation**, as there will be overlaps between random splits, while "repeated holdout" carefully cycles through hiding from the model each of the possible equally-sized validation blocks.

What's the current best practice?
* k=3 for big (slow, expensive) models, leave-one-out for small ones - [ref](https://medium.com/@george.drakos62/cross-validation-70289113a072?)
* k=5 or 10 just because feels like reasonable numbers - [ref](https://machinelearningmastery.com/k-fold-cross-validation/)

For unbalanced datasets and multi-class classification, it is honest to used **stratified cross-validation**, when each of the strata is split separately, and then mixed back, to avoid class imbalance.

# Linear separation
**Can regression be used for classification?** Aye, just set a threshold (0.5) for the output value. Aka fitting a **dummy variable**. If h(x) is a hyperplane in (x,y), h=const becomes a 1-d-lower hyperplane (aka **decision boundary**; in 2D case: a line) separating the space of X into 2 parts. **Linear separation**. Apparently, the best we can do for 2 overlapping Gaussians.

What about **multi-class classificaton**; can regression be used then? Also aye, but with a different loss (not L2), summing costs of all misclassifications (when a point from class Gk was erroneously classified as a point in Gl). We'll need a matrix of costs, and from it calculate a matrix of losses L(k,l). Most often though, one cost for all errors, and **Expected Prediction Error** EPE = E(L). We can try to optimize EPE point-wise: G = armin_g ∑ L(Gk counted as g) ∙ P(observing point from Gk | for X=x).

If all errors cost the same, G = argmin_g (1 - P(g | x)), known as likelihood optimization, or **Bayes classifier**: just go for the best guess of conditional probability P(class | observation). The error rate for it is called **Bayes rate**, and it is the lowest possible rate, coming from the variability of data itself, analogous to irreducible error for continuous data: [wiki](https://en.wikipedia.org/wiki/Bayes_error_rate). If data is deterministic (every x↔g), Bayes rate=0, but if same x can produce diff g, it's ≠0, because the model will never be able to perfectly overfit the data.

**Kernel tricks**: to perform separation in of non-linear data, or data in high D, add feature squares and cross-products into the set, as synthetic features (see ). Then perform separation. Linear boundaries will transform to quadratic boundaries.

If each class dummy variable is fit with f_i = Xθ_i, then the decision boundary between 2 classes is a line f_i = f_j, and so X(θ_i - θ_j)=0 is also a line.  So we can take **indicator matrix** Y (each row is a class, one-hot encoded), and try fit each column of it using linear regression: H = XΘ = X(XᵀX)⁻¹XᵀY, where Θ is a matrix composed of many columns θ: one for every column in Y. This H would have a reasonable property of ∑f_k(x) = 1, when summing across all classes g_k, but individual f_k are linear, and so totally go outside of (0,1). This is a problem.

Another problem is **class masking**: imagine 3 separable classes ABC lying roughly on a line. Decision boundaries between AB and BC will be roughly parallel, but when taking together, 3 linear functions will form only one boundary somewhere in the middle, leaving class B completely unresolved. With a quadratic regression one can resolve 3 classes, but cannot resolve 4, etc. To resolve more classes, one has to transform regression values with "increasingly non-linear" functions F(Xθ_k): they should be monotonous and fall down fast enough to offer fine class resolution. refs: [1](https://stats.stackexchange.com/questions/43867/why-does-the-least-square-solution-give-poor-results-in-this-case)

## Logistic Regression
But by Bayes theorem, our best guess about P(g|x) woudl be P(g|x) = f(x)p/∑f(x)p , where p is prior probability, and sum goes through all classes. Not only it looks a more like a probability, but actually leads to the math bemind logistic regression (_does it really?.._).

**Logistic function**: $\sigma(x) = \frac{1}{1+e^{-x}} = \frac{e^x}{1+e^x}$. The inverse function is called **logit**: logit(p) $=\ln \frac{p}{1-p}$. Logistic regerssion assumes that **log-odds** are linear: log p/(1-p) = Xθ, which means that p/(1-p)=exp(Xθ), leading to p = sigmoid(Xθ). This shape is nice and doesn't suffer from class masking.

For many classes, each of log-odds for class g_i is fit linearly: log P(x∈gi)/P(x∈gk) = xᵀθi. Here gk is some class (last one?); it doesn't matter which one exactly; what matters is that with these formulas all ∑P(gi)=1.

To find a solution, use max-likelihood (maximize conditional P(g|x) ), which is equivalent to minimizing **logloss**. For two classes, Loss = -∑(y log(P(g|x)) + (1-y)log(1-P(g|x))), where y∈{0,1}. As p→0, -log(p)→inf, so we have a huge punishment for near-zero p. Because of that, if the data is too clean and some areas contain only points of one type, weights would explode (it's hard to fit infinity), making  **regularization** extremely important.

The expression for loss can be simplified: 
L(θ) = -∑( y log(p) + (1-y)log(1-p) ) =
-∑( y (log(p) - log(1-p)) + log(1-p)) =\
-∑( y log(p/(1-p)) + log(1-p) ) = …

...Now log(p/(1-p)) is just plain Xθ by definition above. For log(1-p), substitute p=sigmoid(xᵀθ) = 1/(1+exp(-xᵀθ)), then calculate 1-p, mutiply both numerator and denominator by exp(xᵀθ), take a log, use log(1/a) = -log(a), resulting in log(1-p) = log(1+exp(xᵀθ)). But both in the formula above, get:

= -∑( yxᵀθ - log(1+exp(1+xᵀθ)) ). 

Differentiate this by θ, set to 0. Get ∂L/∂θ = -∑ (xy - x∙exp(xᵀθ)/(1+exp(xᵀθ))) = -∑x(y-p) = 0. Here x and 0 are vectors length p+1, where p = ndim (our vectors use  one more dim for the intecept $θ_0$).

> ESL p120-121 gives a solution for the updating (descent) procedure that I skip for now. Also a weighted self-consistency formula that ties θ, x, y, and p together, and can apparently be used to achieve some numerical shortcuts.

### Regularized Log Regression
**L1 penalty** (lasso) is good for this: take the previous L=-∑ y log(p) + (1-y) log(1-p), and add to it λ∑|θj| , where the sum runs by dimention (variable). Flip signs if you like maximizing stuff, as ESL does. Apparently, if you do the math, you get the following link between everything: xjᵀ(y-p) = λ∙sign(θj) for each dimension j.

## Discriminant Analysis
As we'll see later, can be considered an alternative to logistic regression. We assume that each class density is a multivariate Gaussian:

$Φ_k(x) = \frac{1}{უ}\exp\big( -½ (x-μ_k)^Tტ^{-1}_k(x-μ_k) \big)$,

where $უ = 2π^{(p/2)}\sqrt{\det(C)}$, and ტ is a covariance matrix of this Gaussian, defining its shape (elongation) and orientation. If we don't put any extra assumptions into it, this approach would be called **Quadratic Discriminant Analysis (QDA)**, but if we assume that all gaussians have the same shape, in the sense that ტ is shared across all classes, we get **Linear Discriminant Analysis (LDA)**.

Apparently if you do log(P_k/P_l) for two classes k and l (decision boundary), you end up with an equation linear in x (ESL p108), making the decision boundary linear, because lots of terms cancel each out. _I don't get the math, but have to park it for now._ Because ტ is not necessarily spherical (σ²I would be spherical), bisectors of classes are not necessarily ⊥ to the lines connecting the centroids.

The decision becomes a simple thersholding of a **linear discriminant function**: δ(x) = ⟨x,ω⟩ against a certain threshold c, where vector ω = ტ⁻¹μ, and thresholds c = ½ μᵀტ⁻¹μ - log n. Here μ is the mean for this class, estimated as μ = ∑x/n for all xi in this class; n is the number of elements in this class. Whichever δ (for whichever class) has the highest value for a given x, defines the predicted class: g = argmax_k δ_k (x). As ტ is assumed to be the same for all gaussians, the formula for it is similar to pooled variance: ტ = ∑_i (xi - μi)(xi-μi)ᵀ/(N-K), where μi is a matching mean for point xi; N is the number of points, and K is the number of classes. The thresholds can also be optimized explicitly, as hyperparameters, to achieve optimal class separation.

The results of LDA for 2 classes only match that of logistic regression if the number of points in all classes is the same.  For more than 2 classes, it doesn't match logistic regression.

One can also smoothly move betwen QDA and LDA, using **Regularized Discriminant Analysis**, in which case you calculate covariance matrices ტk for individual gaussians, and a pooled ტ, and then work with something in-between: αტk + (1-α)ტ, treating α as a hyperparameter. Alternatively, one can shrink ტk towards a scalar σ²I.

LDA can be used for direct **dimensionality reduction**, as **Reduced-Rank LDA**. The idea is that K centroids always lie in a hyperplane of dim = K-1, which is typically ≪ p. It means that once centroids are found, we can project X into the subspace Hk of centroids, and thus get a good lower-dim representation of the data. For K=3, we can also make a 2D plot. If K is ≫ 3, but we need a plot, we can also run PCA on the Hk space (instead of running it in the X space), obtaining so-callled **canonical coordinates**, aka **discriminant coordinates**. Running a 2D PCA is equivalent to finding a pair of coordinates that maximize the variance of μ, which in practice means spreading centroids as much as possible, which resonates with the idea of LDA. Unlike for PCA, the projection doesn't just try to spread all points in X, but separate them. 

> Computational math in ESL p113, and then p114-116. Parked for now.

Refs: ESL p109-113; [wiki](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)

What's the practical **difference between LDA and Log Regression?** In both cases, the math boils down to log P(x ∈ gi)/P(x ∈ gk) = xᵀθ, but θ are different. In both cases joint P(x,g) = P(x)P(g|x), where P(x) is called marginal density of inputs x. P(g|x) has the same form for both LDA and LR, but P(X) is different. With LR, P(x) is free (assumed to be arbitrary), and we maximize P(g|x). With LDA, we explicitly maximize P(x,g), and insist that P(x,g=k) = Φ(μk,ტ) is a p-dimensional Gaussian. Apparently, if the assumption of LDA is true, it gives you a boost of ~30% efficiency on error rate (you get same performance with ~30% less data). But if you aren't sure, then LR is safer.

# SVM: Support Vector Machines
General idea: widest street approach: find a line, so that if you have a band around the line, it separates the positive from the negative examples as best as possible. If the line is described with a normal vector ω ⊥ line, whether a point x is far from the line can be described as ⟨x , ω⟩. If b is a good threshold value for this product (if we get b when x lies right smack on the separating line), we have xᵀω + b ≥ 0. To find the best line, let's try to find ω and b, so that for positive samples we would get xᵀω+b>1 (not just >0), and for negative samples, xᵀω+b<-1. 

We have already seen linear separators f(x) = xᵀθ = 0 before. The hyperplane (line in 2D) it defines is ⊥ to θ, and f(x) is proportinal to distance from x to the plane.

**Rosenblatt's Perceptron** from 1958:  tries to minimize distances from misclassified points to the boundary: D = -∑y(xᵀθ) ≥ 0. This assumes that most y=1 lie beyond the boundary, so if y=1 is left closer to 0 its positive y gets multiplied by a negative xθ, resulting in a negative value; thus - before the sum. The original perceptron used stochastic gradient descent, visiting points one by one, and setting θ := θ + αyx (as usual assuming that x0=1 and θ is intercept). Obvious problems: non-deterministic, slow, doesn't converge for non-separable data (instead, cycles with very slow cycles).

To make it more robust, let's maximize M = max(min(yxθ)): maximize the smallest distance from a (correctly classified) point to the boundary. Aka "maximal separation", or **Optimally Seprating Hyperplanes**. Here θ is constrained to $\sum_{i=1}^p θ^2_i = 1$. Note summing from 1, not from 0: the actual "vector part" of θ is normal, but the intercept $θ_0$ isn't (as we need to be able to move the hyperplane arbitrarily in space). 

> ESL always writes y_i(x_i ᵀ β + β0), but that's annoying, isn't it? Other textbooks  just introduce special notation with a wave $\tilde θ$ for (θ1 … θp) sub-vector, always remember that it's a subvector ([ref](https://www.dbs.ifi.lmu.de/Lehre/MaschLernen/SS2014/Skript/SupportVectorMachine2014.pdf)).

Instead of writing the constraint on θ explicitly, we can just write the equation for an arbitrary θ that got normalized by its upper part: y(xᵀθ)/norm(θ) ≥ M for ∀(x,y); then move norm(θ) to the right: yxᵀθ ≥ M∙norm(θ); then set M = 1/norm(θ), and instead of maximizing M, minimize the norm of upper part of θ: 

* Optimize θ, to achieve min norm(θ), provided that y_i ⟨x_i , θ⟩ ≥ 1, ∀i. 

Here, as in the prev paragraph norm(θ) is actually understood in terms of θ_1 to θ_p, but not θ_0. 1/norm(θ) can be called **thickness** of the decision boundary. Yields a convex problem with linear contraints, solvable via Lagrange optimization: L = |θ|² - ∑ λi ( yi ⟨x_i , θ⟩  - 1)…

> At this point (ESL p133) I give up for now. It seems that math around p134 is not how SVMs are actually implemented anyways, so it can probably wait. I'll rivisit this once I read the actual SVM chapter.

SVMs are very popular with **Kernel tricks**, such as radial basis kernel , as it produces simple yet expressive models ([ref](https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589)).



**Refs**: [a very reasonable post by Ajay Yadav](https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589)

Features, Dim reduction, Clustering
===


# Linear Basis Expansion
The idea is to go beyond "linear" in linear regression, but retain the spirit of replacing complex and noisy X with some simple, probably smooth functions of fewer parameters. $f(X) = ∑β_j h_j(X)$, where X is our data, β are coefficients, and {h_j(X)} is a set of functions (transformations), each projecting from X-like space to y-like values (ℝ^p → ℝ). So a non-linear transformation of X, followed by a linear model in these new coordinates.

> So, this shows that fitting and smoothing can also be considered a case of basis transformations, where you have from a very high dimensionality (lots of consecutive points in a signal, treated as one long vector) into a low dim of parameter space (say, spline coefficients), from which the original high-D signal may be approximately reconstructed.

**Some examples:**
* h_j(X) = x_j. Linear regression
* h_j(X) = x_j∘x_i (**Cross-products**; potentially, for all p(p-1) pairs of coordinates, including self-products, aka squares). Aka **Polynomial kernel** or **polynimal expansion**. Cheap and fast way to perform non-linear modeling with linear methods. Higher polynomials are also possible, but computationally expensive. 
* h_j(X) = indicator(x ∈ area): **binning** in p-dimensions. Emulates cluster analysis without actually running cluster analysis, as clusters are more likely to be covered by the same bin. Can also be used to transform a continuous variable into a pseudo-nominal one.
* **Kernel tricks**: use various functions (kernels) to calculate new synthetic features from old features. For example, distance from a well chosen point in a high-D space would make a useful kernel (see ).

The biggest problems with polynomials is that they are global (non-local), cannot extrapolate, and are unstable. Some alternative that combine mathematical simplicity with good behavior: **piecewise-polynomials**, **splines**, and **wavelets**. All these methods produce a very large **dictionary** of basis functions, and so need further constraints to keep the complexity of the model in check, such as **restriction** (e.g. always the same number of terms in a sum, for addittive functions), **selection** (only retain functions with large enough coefficients), or **regularization**.

# Smoothing
See: 

# Variable selection
Key references:
*  - review on feature selection (as well as some info on construction). 
 
Basic checklist:
1. Use domain knowledge
2. Normalize variables (where appropriate)
3. If variables aren't independent, construct **conjunctive features**
4. If need to prune for external reasons (compute?) create **disjunctive**, or weighted
5. Get baseline by assessing features independently
6. Detect and handle outliers and dirty data
7. Start with the simplest predictor (usually linear)
8. If you have better ideas, implement, then compare (benchmark)
9. Check stability by bootstrapping (cross-validation)

# Dimensionality reduction
Local methods (like KNN) break down in high-dimensions to to the **curse of dimensionality**: it becomes impossible to find a good neighborhood to do local averaging. In high-dim, volume grows so fast with linear dimension (r^K for K dim) that most of volume lies in the shell, not around the center. Say, to find a subcube with 1% of total volume inside a unit-cube in 10-dim, we'd have to take a subcube with a side of 0.6 (because 0.6^10 ≈ 0.01). Meaning that this neighborhood won't be local in any meaningful set (it will span 0.6 of total range for each of the dimensions!). And if we keep the side length (r) small, we won't capture any meaningful examples. Similarly, if points are distributed uniformly within a unit volume, the closest point to any given point is expected to be half-way to the boundary. Especially for highly non-linear functions (say, exponents) it presents a huge problem, as mean(neighbors) no longer resembles the "true value". (ESL p23-25)

## Subspace Methods
**Overview**: define subspaces and assign vectors to classes to minimize the distance between these vectors and these subspaces. Usually distance = acos cosine similarity = acos (v∙proj(v))/v² = acos vPv/v², where P is a projection matrix. PCA is one way to achieve it, but not the only one. But in all cases, the idea is that (Signals) = (Basis)∙(Scores), where Scores can be interpreted as representations.

Several possible possible approaches:
* **PCA** (principal component anlaysis) - optimal reconstruction (minimal reconstruction error, given a limit on basis size), maximal variance of projections. Eigenspace of XᵀX
* **LDA** (Linear Discriminant Analysis) - optimal separation of classes. At each step, maximize separation (distance) between classes, aka **Fisher Linear Discriminance**. Obviously, label-driven, and these labels are nominal. Refs: [wiki](https://en.wikipedia.org/wiki/Linear_discriminant_analysis); detailed descriptoin in .
* **CCA** (Canonical Correlation Analysis) - in 1D case, finds a mix of X, optimally correlated with a given output value y (similar to LDA for labels, but for continuous y). More generally, for two high-D datasets X and Y, finds directions (projections to low-D) in both X and Y such that the correlation between these directions is maximal. In even more general case, develop a sequence of these correspondences, defining an "optimal rotation" from one space to another. Refs: [wiki](https://en.wikipedia.org/wiki/Canonical_correlation)
* **ICA** (Independent Component Analysis) - makes the basis statistically independent. Can be considered an extension of PCA that goes beyound 2nd order statistics.
* **NMF** (Non-negative Matrix Factorization) - keeps all elements in the basis non-negative, which often aids interpretability.

Refs:
* [A lecture with a list of approaches](https://www.cc.gatech.edu/~hic/8803-Fall-09/slides/SubSpace-Learning.pdf)	, by Horst Bischof and Ales Leonardis

## PCA
Simplest type of eigenvalue-based (spectral) factor analysis. X=TΛQᵀ, where X is the original data, T is a set of orthogonal signals (**scores** - like in how much each point scored, therefore dim scores = dim points), and ΛQ is a p×p matrix of weigts that transform basis T into the basis of original data, called **loadings** (like, how hidden factors load into the observed variables, thus dim loadings = p×p, as for basis transformation).

**Intuitions for the matrix form**: covariance matrix C = Cov(X) = XᵀX/N is symmetric, so it can be spectrally decomposed not just in PDP⁻¹, but in QDQᵀ, where Q is orthonormal. Rotation, followed by independent scaling of each coordinate, followed by backwards rotation. It means that if you get p independent normal variables into a matrix Z (this matrix would have Cov(Z) = E (ZᵀZ)/N=I), but scale them by Λ by multiplying by Λ on the right (where Λ_i=sqrt(D_i)), and then rotate with Qᵀ, you'll get data Y=ZΛQᵀ with the same expected Cov as the original data: C' = YᵀY/N = (ZΛQᵀ)ᵀ(ZΛQᵀ)∙1/N = QΛᵀZZΛQᵀ∙1/N = QΛIΛQᵀ = QDQᵀ. In other words, if our data Y is happens to be made from Z by scaling followed by rotation, then eigenvaluedecomposition of Cov matrix into QDQᵀ helps to recover these scaling (Λ=sqrt(D)) and rotation Q.

Now the cool thing about Q is that the following is true: Cov(X)Q = QDQ⁻¹Q = QD = each q_i gets scaled by D_ii. Which means that columns of Q are eigenvectors of Cov(X), and so of XᵀX as well.

Further, as Qᵀ rotates ZΛ towards X (as in X=ZΛQᵀ), the rows of Qᵀ are the directions of axes of ellipsoid ZΛ in new coordinates of X. (If you multiply an index vector by Qᵀ, as in (1,0,...,0)Qᵀ,  you'll just get the first row of Qᵀ, etc.) But rows of Qᵀ are also columns of Q. Therefore, the columns of Q, that we had just shown above to be the eigenvectors of Cov(X), also point along axes of the ellipsoid of X.

Why projecting to these ellipsoid-alinged axes would maximize the variance along this projection? (Which is the usual definition of PCA that I haven't mentioned yet). Let's look for the first axis, maximizing the variance of projection on this axis. Consider all unitary vectors w (unitary ⇒ wᵀw =1), and find the one that captures highest variance. A vector of projections of X (N×p) on w (p×1) is Xw (N×1). The variance of it is (Xw)ᵀ(Xw) / N = wᵀXᵀXw ∙ 1/N = wᵀCw. We want to maximize wᵀCw with a constraint of wᵀw=1, which is achieved via Largange multiplier λ and unconstrained optimization for wᵀCw - λ(wᵀw-1). Differentiate by w (by each coordinate separately, but then written in a vector form), set it to be equal to zero-vector (one vector equation = a system of slacar equations): 2Cw - 2λw = 0. From that we see that Cw = λw, and so w that maximizes variance of projectsions of X on it, happens to be an einvector of C. ([ref](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)) From this example with one vector, using a greedy procedure, we can build the entire basis, kinda by induction.

**Varimax rotation**: orthogonal rotation of PCA results (so components stay orthogonal) that maximizes the share of near-zero coordinates (scores) for points. It tries to rotate points (scores) to the axes, and away from "innards" of each quadrant. Makes points commit to "mostly one axis", instead of being torn between many axes.

**Promax rotation**: similar to varimax, but oblique (with shears, making components non-orthogonal), which allows higher contrasting, and is also cheaper computationally.

## tSNE
"The art of using tSNE" from Nature:
[https://www.nature.com/articles/s41467-019-13056-x](<https://www.nature.com/articles/s41467-019-13056-x>)

## UMAP
ToRead:
* [https://pair-code.github.io/understanding-umap/](<https://pair-code.github.io/understanding-umap/>)
* [https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668](<https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668>)
* [https://towardsdatascience.com/how-to-program-umap-from-scratch-e6eff67f55fe](<https://towardsdatascience.com/how-to-program-umap-from-scratch-e6eff67f55fe>)

# Clustering

# Very High Dimensions
For high dimensions (say, word counts in a document) simple Eucledian distance doesn't work that well, as it becomes impossible to compare long documents to short documents. One way would be to normalize vectors before calculating Eucledian distance, but there's an easier way: use **cosine similarity** (described above). It sorta auto-normalizes lengths, is easier to calculate, and ordering of proximity is the same for normalized Eucledian and cosine similarity. (_unproved here, but apparently it's the case, hmm_)

**Specifics of working with very high dimensions:**
(Genomic data in this case)
* https://towardsdatascience.com/no-true-effects-in-high-dimensions-1f56360182cd
* https://towardsdatascience.com/pitfalls-of-data-normalization-bf05d65f1f4c
* https://towardsdatascience.com/how-to-cluster-in-high-dimensions-4ef693bacc6

# Ensemble methods

Ensembles are about combining (or gradually refining) lots of poor predictions (aka **weak learners**) into a very good prediction (aka "Wisdom of Crowds": popularized by a 2004 [book](https://en.wikipedia.org/wiki/The_Wisdom_of_Crowds) by Surowiecki).

# Bagging

Bootstrapping can be used to improve estimations: bootstrap the data repeatedly; each time build an estimator, then average predictions of these estimators. Why does it work though? For linear estimators (like linear regression) it doesn't, as E(h(x)) = h(E(x)) = h(full data). But for non-linear h(), like **regression trees**, it may smoothen idiosyncrasies of each individual h(x).

How to best combine opinions of several classifiers into one output? One option is to one-hot encode output labels, average them up across models, produce a vector, then pick max coordinate (the most popular label). Another is to make classifiers output vectors of probabilities for different levels, without the last step (actual argmax-ing), and then average these outputs. It's smoother (lower variance), and also if each vector is ∑=1, then the final output would naturally ∑=1 as well.

> Bias is apparently unchanged (ESL p285, stated as a fact, followed by a page of confusing "illustration"). Is it because no additional constraints are introduced by the procedure? How to intuit it? In which cases the bias WOULD BE changed? What sorts of modifications of h(x) procedure change the bias, and which sorts do not?

> So from this part, it seems that the key power of bagging is that it takes a very discrete, run-down method that is by design jumpy, and also overfits like hell, and turns it into a smoothed landscape. Is this shifting from discrete to pseudo-continuous the main reason for why bagging works? Or is some hidden indirect serendipitous regularizaton also involved?

Note that bagging doesn't work if loss is heavily quantized (like in **0-1 loss**, which is essentially accuracy). In this case, if you get a saturated classifier (all points→one class), there's no way to improve on it. Moreover, for random classifier with constant probabilities (returns 0 with probability p0, and 1 with p1) bagging would only worsen things (as bagging will lead to const classifier for whatever p is greater, as it would win on average). _So for this to work, we need to make the classifier as uneven and moody as possible, right?_

A slightly more sophisticated **counter-example**: points uniformly fill a (0-1,0-1) R² square; 2 classes with a diagonal split (ESL p288). Any subsampling from this will look like a blobby squary cloud with a diagonal; a split will fall kinda in the middle, and the edges of the diagonal will never be resolved.

Trees are also notoriouly bad in splitting 2D **XOR** in a unit square, as averaging across both x and y gives left≈right, and a greedy tree would just split at some random point, defined by data density variations, and not really by class borders.

> Great  : model that in a playground, just to get a feel of what it can and cannot do.

ESL p288 claims that bagged estimate is an approximate posterior Bayesina mean, which is something I don't get yet.

**Bayesian interpretaton:** Suppose you're trying to estimate ζ, given a training set Z, and we have a family of models {M}. Then we can marginalize posterior P(ζ|Z) over the family of models {M_i}: P(ζ|Z) = ∑P(ζ| M,Z)∙P(M|Z), summing across M in {M}. It means that posterior mean E(ζ) will also be a weighted average across models, weighted by probability of each of these models given the data: E(ζ|Z) = ∑E(ζ | M)P(M|Z), summing across M in {M}. In practice, **Committee methods** give equal weight to each observed model. (_I'm guessing it means they assume that with enough models samples, P(observing a model) will be taken care of just by design of this model-sampling procedure?_)

Alternatively, try to estimate P(M) somehow; say, if a model has a set of parameters θ, one could Bayes-flip P(M|Z) into P(Z|M), further marginalize P(Z|M) by these parameters as ∫P(Z|θ)P(θ|M)dθ, and compute it numerically. _Not sure what it means and how it helps..._

**Frequentist take**: let's do linear regression on results of all models, and find a mix of them that would minimize the L2 loss. If outputs of every model are f_i, together they form a matrix F, and we know how to do population regression to a true target Y: w = (FFᵀ)⁻¹FY. (**Population regression** just means that here instead of inner products, we imagine taking actual expectations E() over the true distribution that generages the data). As linear regression is a minimization of L2= var(Y-wF), then minimzed var(Y-wF) < var(Y-f_i) ∀i. In practice, population regression is of course impossible, so we can do the next best thing: linear regression on training data.

> But then they say something that I don't understand; how with different models having different complexity somehow everything would break. Why again? ESL p290

# Stacking

A practical way to create a reasonable linear combination of several non-linear models using cross-validation.

**Stacked generalization** or **Stacking**: Say you have a way of building various models f_m, and training each of them on a dataset X. Models f_m may belong to the same class, or to different classes, but it is important that they are sufficiently different: that is, they need to be defined both by the training set X, and by some hyperparameters that come with the model itself. So while each model is a function of a dataset, f(X), two models f_m1(X) and f_m2(X) should still be different.

Consider training each model f_m() on a dataset  X minus observation xi. It creates a family of predictors $f_m^{-i}(x)$ for each model class f_m(), providing a good way of assessing the accuracy of m-type models f_m() via cross-validation: just predict y_i by each f^{-i}, and sum all errors. But instead of just picking the best model, we will build the best linear combination of all these models, by finding an optimal vector of weights w, so that
$\sum_i \big( y_i - \sum_m w_m f_m^{-i}(x_i)\big)^2$ is minimal. In other words, we find a set of coefficients w, so that the linear combination of all f_m gives the best total cross-validation across all "remove one x_i". It also helps to constrain w to w_i>0 ∀i and ∑w_i = 1, as it turns it into a quadratic problem. That's called "stacking".

Refs:
* ESL p282-290

# Bumping

A way to find better models by randomly moving in model space, avoiding local minima. Train a bunch of models on different subsamples of X, then test each on full X, and pick the best. As in this case optimization is kinda shifted towards the end, it's probably better to use quite undersampled training sets. Say, for a **XOR** case, if you test enough small subsamples, at least one of them will probably split the data decently, even though training on a full dataset may be hopeless for many types of models.

It's important to keep all models in  the comparison group similar in their complexity (for trees, same number of terminal nodes).

# Boosting

Boosting uses simplest decision trees possible:  single split into 2 categories, aka **decision stumps**. But with every next group of trees, we increase the weights for those elements that weren't classified correctly by the previous generation of classifiers (starting with equal weights at the beginning of the process, for the first tree). These weights may either be explicitly included in the error calculation, or be used as probabilities of each data point  appearing in next training subset (the result is the same). At the end, predictions are made by **weighted majority rule**: weighted average by the **accuracy of individual trees**, followed by argmax.

The most popular, archetypical approach: **AdaBoost**, aka **Adaptive Boosting**. For a binary discrete case:
1. Start with all points x_i having identical weights {W}. ∑w_i = 1, so w_i = 1/n.
2. For each available coordinate, find the best split (aka stump), with smallest total error E = ∑w_i ϵ_i, where ϵ_i = 1∙(h(x_i)==y_i).
3. Across all coordinates, find a split that minimizes  index for this split. For all loops after the 1st (for all loops when weights are different), either use **weighted gini index**, or randomly resample points with draw probability P ∝ w (draw with repetition, keeping the total number of points considered at each split ~constant).
4. Calculate the weight of this stump as α = ½ log((1−E)/E) . This formula →+∞ for E→0 (perfect split), →-∞ for E→1 (perfectly erroneous split), and ~0 for splits that perform near chance level, when E=1-E, and so we get log(1). In practice, to avoid ∞, a tiny ε is added to both numerator and denominator of the fraction under the log().
5. Increase the weights of misclassified samples: w_i ← w_i ∙ exp(α); decrease the weights of correctly classified samples by multiplying them by exp(−α); then normalize all weights to ∑w = 1.
6. Go to step 2.

> Is it true that in practice randomly resampling points is better than using a weighted formula? Nobody says it openly, but if it weren't the case, why would people repeat this whole resampling story in each tutorial?

7. Once everything is classified, rejects trees with accuracy less than 50%. _Not all descriptions mention that._
8. Produce an average of tree outputs, weighted by α .

Because each split is a single plane (line), || to all other variables, the decision border looks like a combination of these planes (lines). But all are ⊥; there are no curves or non-right angles there. 

For multi-class classification, either create lots of binary classifiers (each class against all others), or encode each class as a superposition of several binary "features" that may be present or absent (say, bunnies are cute and jumpy, cats are cute but not jumpy; crickets are jumpy but not cute etc.), then use AdaBoost to identify the presence of features ([ref](https://engineering.purdue.edu/kak/Tutorials/AdaBoost.pdf)).

In many ways, AdaBoost goes against the conventional wisdom for classification: it doesn't try to build a good classifier (but instead uses a lot of crappy ones); it does not try to identify important variables, or isolate important features using dimentionality reduction (instead, it thrives in this high-D multi-variable space).

Refs: [Akash Desarda](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe); [wiki](https://en.wikipedia.org/wiki/AdaBoost); [Tommi Jaakkola](http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture13.pdf) lecture note; [Explaining AdaBoost](http://rob.schapire.net/papers/explaining-adaboost.pdf) by Robert Schapire; [YouTube video by StatQuest](https://www.youtube.com/watch?v=LsK-xG1cLYA) (good); [Slides by Avinash Kak, Purdue](https://engineering.purdue.edu/kak/Tutorials/AdaBoost.pdf) (very good).

# Gradient Boosting

**Gradient Boosting Machines**, or **GBM**, work in problems where the output is numerical rather than categorical. GB recursively approximates Y as a series of models {F_k], with each next model F_k  improved over the previous one: F_k = F_k-1 + f_k(x), where f_k() is some sort of weak learner. The basic algorithm, therefore, is at each step to iteratively approximate the difference between Y and the previous best model F_k (aka the residual) with a new function f_k(x).

In practice, the most popular type of weak learners f() for GB is a **regression tree**, or more precisely a **regression tree stump**: the simplest decision tree that consists of one basic split over one variable (one coordinate of X), and procudses two different values (levels) on each side of this split. Once the fitting procedure is over, all regression tree stumps are summed together, to produce the final output of the ensemble. 

To find the best split (best model improvement) at each step, we need to first introduce a smooth differentiable loss function (usually L2, provided that there are no outliers, or L1 if outliers are common).  GB then performs a stepwise gradient descent to minimize this loss function. Depending on the function, descent can be performed in several different ways:
* One, is to use f(x) to fit Y-F_k: a vector of differences between each y_i and its beast current estimation F_k(x_i), as described above. This leads to minimzation of L2.
* Another approach is to fit sign(Y-F_k): some sort of Manhattan-style normalized direction towards the gradient. This leads to minimization of L1.

Often, each impovement f_k() also isn't applied in full, but is multiplied by a coefficient η < 1 (typically 0.5 to 1.0), called the **learning rate**. This smoothens the descent.

The first (or rather 0th) optimization step is to calculate the mean of all data (for L2 loss), or its median (for L1).

As GB is a greedy algorithm, it is prone to overfitting, and so requires **hyperparameter tuning** of the maximal number of stages M, and learning rate η. GB can also be **regularized**: for example, stochastic GB only considers a part of the data for each decision tree (kinda like in bagging). It is also possible to use GB with custom loss functions: for example, one can put more weight in false-positives compared to false-negatives, or ther other way around.

**References:**
* [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html), Parr, Howard. Very good description!
* [Custom loss functions for Gradient Boosting](https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d), Prince Grover, 2018
* https://statweb.stanford.edu/~jhf/ftp/trebst.pdf - a technical paper about GB
* A bunch of confusing explanations that I either didn't like, or that explain boosting in general rather than gradient boosting specifically:
    * [GB from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
    * [Gentle Introduction to the Gradient Boosting Algorithm](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
    * [Understanding Gradient Boosting Machines](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab)

## Bagging/Boosting comparison

* Bagging tends to decrease variance, but not necessarily bias. Boosting tends to reduce both variance and bias.
* Conversely, Bagging does not overfit, while Boosting can overfit easily, by slicing space very thin around every point.
* Except for the very last step, bagging can happen in parallel, while boosting is by definition sequential, and so not easily parallelizable.
* Compared to many other methods, both bagging and boosting are very fast.

Refs: [1](https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/), [2](https://towardsdatascience.com/understanding-adaboost-2f94f22d5bfe)

# Random forest

The idea: construct many full trees, by bagging (partial data), but also by providing to every tree a random subset of features (aka **feature bagging**; typically √p features out of p total, or something like max(5, p/3), ref: [wiki](https://en.wikipedia.org/wiki/Random_forest#From_bagging_to_random_forests)). This is an improvement upon bagging, as it makes trees less correlated, more diverse. 

> Not sure if the sequence in which different values are used for splits is randomized for each tree, or allowed to be optimal. I'd expect that both approaches could be possible, depending on dimensionality; is it true?

Variant: **Extra Trees** of **Extremely Randomized Trees**, where for each tree the first split is made at random (random feature, random point), then the rest of a tree is allowed to be optimized.

All trees usually have an equal vote in the final classification.

# Deep learning

**Topics that live in separate documents:**
* 

## Backpropagation
Essentially a chain diff rule, for cost function J by weights w.
Consider a network of several layers (full = dense = all to all), eventually all convering on one output element. MSE loss: J=(a-y)², where a = out = h(z) = h(∑ w10_i a1_i) = dot product of of prev layer activations with 1→0 weights. (I'll be numbering layers backwards, starting from 0 for the output layer)

To change one of the weights w10_i, we need to know ∂J/∂w10_i . Dropping i (but actually it's still for one weight for now): ∂J/∂w10 = ∂J/∂a ∂a/∂z ∂z/∂w10 = 2(a-y) h'(z) a1,
because ∂J/∂a = 2(a-y),
∂a/∂z = ∂h(z)/∂z = h'(z), and
∂z/∂w10 = a1. That is, activation a1_i arriving from the layer 1, that gets multiplied by w10_i.

So like Hebbian rule: if (a-y) is large, and a1_i is large, then w10_i matters, and gets changed.
(And we get a similar formula for bias, as technically it's always h(w∙a+b) and not just h(w∙a): so we just get 1 instead of a_1 in the formula, because ∂z/∂b = 1).

Now we can go deeper, to the yet-previous layer 2→1, with weights w21_j.
No need to sum yet, as w21_j only affects one element in the 1st layer: a1_i:
Again, we have:
∂J/∂w21_j = ∂J/∂a1_i ∂a1_i/∂w21_j.
∂J/∂a1_i can be calculated as above = 2(a-y) h'(z) w10_i,
while ∂a1_i/∂w2_j = ∂a1_i/∂z1_i ∂z1_i/∂w2_j = h'(z1_i) a2_j (activation a2_j from yet prev layer).
So the full expression: ∂J/∂w21_j = 2(a-y) h'(z) w10_i h'(z1_i) a2_j.

Now consider layer 3→2. Again chain rule, but now while w32_k coming from a3_k affects only one a2_j, this activity in turn affects all of the a1_i. And then it gathers back on our single output. So index i is no longer fixed, but we have to run a sum for it:
∂J/∂w32_k = ∑_i ∂J/∂a1_i ∂a1_i/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i ∂a1_i/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i h'(z1_i) w21_j ∂a2_j/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i h'(z1_i) w21_j h'(z2_j) a3_k.

And so on; we can now do it for layers. What's important is that we essentially take the end error derivative 2(a-y), scale it by h'(z); then multiply by the same vector of  w10 that were used on the forward pass, to get "error-effects" at the previous layer 1. Then same way we took the vector of "error-derivatives" (gradient) from layer 1, scale each element by h'(z1), multiply it by transposed matrix w21, and get "error-effects" at layer 2. Backprop!

And the problems are immediately felt: if a certain w_ji=0, it kills the effect of all weights converging on element j. Moreover, if the value of z_j is such that it drives h'(z_j) to zero, it also kills the gradient (aka **vanishing gradients**). Say, for sigmoids it happens for very high or very small z; for ReLUs it happens for any z<0, and they can't recover (aka "Dead ReLUs"). And the other way around, in a deep network, gradients can grow arbitrarily large (**exploding gradients**).

References: 
* [Backprop calc by 3blue1brown](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)

**Some relevant practical concepts:**
* **Hyperparameters**: those somewhat arbitrary values that define the type of solution the model is looking for, and the process of descent. Examples: learning rate, batch size.
* **Learning rate**: Goldilocks principle - the best learning rate should "magically" put you in the minimum in a very few steps. Large learning rate leads to noisy oscillations after what looked like a convergence. It may even break everything (unstable).
* **Mini-batch**: process >1 (usually 10-1000) points at a time. Somewhere in between fully stochastic descent (1 point at a time) and math-optimal (all points every time).

## Loss functions
Obvious choice for continuous output: **Eucleadian distance** (aka **Mean Squared Error**, or **MSE**)

For classification: **Cross-Entropy Loss**. The ground-truth is one-hot encoded vector; the prediction is a vector of probabilities. Definition: H(p,q) = -∑ p_i ∙ lg(q_i) where p_i are probabilities of different classes in the training set, and q_i are probabilities predicted by the model.

Motivation: essentially, average log-likelihood. Assume that the predictions of the model follow a true correct distribution, with q_i encoding P(x_i). Then what it the probability of observing n_i cases of x_i? Assuming that test cases are independent, P = ∏P(each observation) = ∏q_i ^ n_i . If we lg it (to replace products with sums), we get the total log-likelihood: L = ∑ n_i ∙ lg(q_i) . If you average it by dividing by the total number of test cases, and denote p_i = n_i / N, we get loss = ∑ p_i ∙ lg(q_i) =-H(p,q) . And then you are trying to minimize this value by making the model make predictions q_i such as the observed values (from P) are "minimally strange". Feels Bayesian, huh?

SVMs use a special thing called **Hinge loss** (see chapter "Classification")

**Huber loss**: a compromise between MSE loss that is tolerant to small noise (behaves nicely around 0, as x² ≪ x for small x), but isn't super-sensitive to outliers (behaves as mean absolute error there). Essentially, just a parabola with arms that smoothly transition to linear at some point, and continue like that. Formula: L = 1/2∙x² for x<d, but (abs(x)-d/2)∙d for x>d. ([wiki ref](https://en.wikipedia.org/wiki/Huber_loss))

References:
* [by Daniel Godoy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) - a visual explanation for a binary case. Good intuition for why -log(1-prediction) makes sense: if you were very certain, and got it wrong, that's a more important learning point compared to a case when you were pretty lukewarm about it to begin with.
* [Wiki for cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)
* [A list of losses supported by Keras](https://keras.io/losses/)

## Regularization
(see also: Ridge regression in )

**L2 regularization**: use a modified loss = Loss(Data|Model) + λ∑ω² where ω are model weights, and λ is a hyperparameter known as **Regularization rate**. High values of λ push {ω} towards normal distribution, while low λ make the distributin of {ω} closer to uniform. Discourages sparseness of {ω}, as having many small weights becomes better than having a few solid ones. So, in a way, L2 regularization is pro-democracy, and anti-parsimony. As λ is increased, weights are pressed down asymptotically.

Alternative: **L1 regularization**, aka **Lasso** (least absolute shrinking). Loss = Loss(Data|Model) +  λ∑abs(ω) . Aggressive shrinkage of small ω; encourages parsimony; discourages "leakage of features". As λ is increased, weights, one by one, suddenly drop dead from something to zero.

**Elastic net**: some sort of combination of both, with two different λ. Flexible, nice.

## Dropout
Huge breakthrough, discovered in 2012: resolves overfitting in deep networks. Randomly inactivate a large share (~50%?) of all units in every layer at every training step; then only consider the output of remaining units; and only train them. When all weights are learned together, many of them slack out by just running to the ground (zero), and not contributing to anything (would have to be pruned). But with drop-out, we have something of a built-in ensemble method when many sub-networks have to train in parallel.

> Is this intuition even true? Is dropout in any way similar to ensemble? Are ensambles and regularization secretly the same thing?

A gradient of a dropped-out network is the same as for a full network, but with an additional "regularization term" for activation: ∂J_dropped/∂w = ∂J/dw + p(1-p)w∙I² , where p is the probability of elements staying active at each step (they use fixed probability, and not a fixed number of active elements). _Ref 1 pretends to have math, but immediately plunges from obvious to confusing._

Because of that, dropout is most effective for p=0.5 (it maximizes p(1-p)). In practice, it's OK to set p at 0.5 for intermediate layers, but it should be higher (0.8?) for input layers, in order not to imitate undersampling. Keras takes 1-p as an argument (p of dropping), so adjust accordingly.

**Gaussian dropout**: instead of completely eliminating a node at each step, just multiply each activation by 𝒩(1,σ). A cool thing is that it doesn't change the gradient on average, so nothing needs to be scaled. (_What does it mean?_)

> How much exactly does drop-out help? How to quantify that? How do people quantify effectiveness of things like regularization? If dropout is similar to regularization, is it strictly better than regularization, or is it the same? If we explicitly add this wI^2-proportional (but fixed) regularization, will it give the same result? I'm guessing the answer is "no", but why?

There's a claim that dropout should not be used on convolutional networks, because it does not actually work (does not inactivate elements), as convolution introduces coordination between changes in different weights. Refs [1](https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2), [2](https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)

Refs:
* [Simplified math](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275), by Chitta Ranjan - *soso*

## Activation functions
* https://en.wikipedia.org/wiki/Gated_recurrent_unit

# Other layer types

## Pooling layers
**Maxpool**: Only retains the maximal value, and drops all the rest. Mostly used in convolutional networks, in which case it has dimensions, depth, and a stride. Has no learnable parameters. At backpropagation step, it only propagates to those neurons that contributed to the selected value (max value); all other neurons aren't updated. Good for downsampling feature maps, where the presence of a feature (quantified by different components of the depth-vector) is more important than the precise position of this feature.

Extreme version of this: **Global pooling** - entire tensor in one vector (equivalent to doing maxpooling with W and H of full image). A faster alternative to having a fully connected layer from a convolutional layer to a global feature vector.

**Average pooling**: as clear from the name, just averages all the inputs. Good for downsampling.

Refs: [one](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)

## Softmax
Takes in an aribtrary vector, and transforms it so that all values are positive, between 0 and 1, and the sum is = 1 (and so outputs can be interpreted as probabilities). Formula: first exp(each input), then divide by the sum of all. Preferred last layer in classification networks (in which case it's used with cross-entropy loss function).

If placed in the middle it is roughly equivalent to an exponential activation layer with batch normalization. But apparently it is possible to use it in the middle as well ([link example](https://github.com/gorobei9/jtest/blob/master/machine-learning/MNIST%20for%20Crazy%20People.ipynb)).

How to train Softmax layers? Better to balance labels, to use all possible training points for the label that is trained, but only a random subsample for negative labels. _Is it to avoid overtraining on negative examples, as for any given label the majority of examples in a natural dataset will be negative?_

## Batch normalization

Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.
[https://arxiv.org/abs/1502.03167](<https://arxiv.org/abs/1502.03167>)
Main paper on batch norm (with like 30k references)

# CNNs
**Convolutional Neural Networks**

* Wavenet: https://deepmind.com/blog/article/wavenet-generative-model-raw-audio

# Recurrent Neural Networks

* Attention and Augmented Recurrent Neural Networks: https://distill.pub/2016/augmented-rnns/

## LSTM
* http://colah.github.io/posts/2015-08-Understanding-LSTMs/



# Autoencoders and GANs

# Embedding layers
Solves the problem of data sparsity (to avoid direct training on long one-hot encoded vectors). A bottleneck layer from an N-hot layer to learn an embedding. Number of units in the bottleneck = number dimensions in the enbedding. Empirical rule of thumb: n dim ~ (n possible values)^(1/4). _Where does it come from?_ With too few dimensions you won't capture the complexity; with too many - risk of overfitting, and longer to train. N dimensions should be treated as a hyperparameter, and optimized.

Classical training: get hold of some similarity or co-occurance measure; use to produce groups, then "withhold some", predicting missing members of a group based on those that are visible. But look also: , ,  .

## Word2vec
Developed by Google; works with "bags of words". See: [tensofrlow tutorial](https://www.tensorflow.org/tutorials/word2vec/index.html)

# GANs

Odena, "Open Questions about Generative Adversarial Networks", Distill, 2019.
A nice 	short review.
https://distill.pub/2019/gan-open-problems/
* Good in modeling within one type of object (CelebA), worse if data is diverse and sparse (Imagenet)
* Problems with discrete output (text), but doable through gradient estimation
* As of April 2019, they know only one application to graphs
* Training may diverge; convergence guarantees aren't clear (?)

# Graphical Methods

## Markov Random Fields

.

## Convolutional Graph Networks

.

# Texts and Language


**Toolbox of methods and concepts:**
*  - N-grams: low-level feature for text analysis, beyond single words. "Bags" of several (2-3) words.
*  - Text Frequency - Inverse Document Frequency. A basic ranking approach for text relevance.
*  - main measure of language model quality, as well as a great objective function
*  - a way to go beyound a level of "one word at a time" by tree exploration
*  - a simplistic Markovian approach to text generation

# To read:
https://ruder.io/unsupervised-cross-lingual-learning/index.html

https://ruder.io/state-of-transfer-learning-in-nlp/index.html

https://www.aclweb.org/anthology/P19-1334/ 

Two posts about BERT by Jesse Vig:
* https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8
* https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1


McCoy, R. T., Pavlick, E., & Linzen, T. (2019). Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. arXiv preprint arXiv:1902.01007.
https://www.aclweb.org/anthology/P19-1334/
Criticism of DL text prediction models.
https://thegradient.pub/nlps-clever-hans-moment-has-arrived/

# Reinforcement learning
 

# To Read

Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks
Arthur Juliani
https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0

The present in terms of the future: Successor representations in Reinforcement learning
Arthur Juliani
https://medium.com/@awjuliani/the-present-in-terms-of-the-future-successor-representations-in-reinforcement-learning-316b78c5fa3

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Petersen, S. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.
https://daiwk.github.io/assets/dqn.pdf
A popular RL paper about Atari games. Other papers (PCGRL: Khalifa 2020) reference it as "fractal networks", even though the term isn't used in this paper itself.



# AI and biology
 

# Topical Bibliographies
*  - papers about auto-tracking many animals at once, then doing ML on this set
*  - papers about reconstructing connectivity from spiking
*  - papers on collision avoidance, tecta, hindbrains, OMR etc.



# Monte-Carlo Stochastic Approaches

 :
Visual guide to Evolution strategies:
http://blog.otoro.net/2017/10/29/visual-evolution-strategies/
(Must, because visual and nice!!)

https://en.wikipedia.org/wiki/Mean_field_particle_methods

## Variational Inference
Links for now:
* https://en.wikipedia.org/wiki/Variational_Bayesian_methods
* https://arxiv.org/abs/1601.00670
Variational Inference: A Review for Statisticians
David M. Blei, Alp Kucukelbir, Jon D. McAuliffe

## Gaussian processes
See cards:
* 

## Markov Chain Monte Carlo (MCMC)
General references:
* https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo
* https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm
* https://en.wikipedia.org/wiki/Slice_sampling
* https://en.wikipedia.org/wiki/Gibbs_sampling

Say, you have a multidimensional distribution with probability P(x). You don't know P(x) exactly, but you have a guess f(x) that is proportional to P(x). Apparently this is an important nuance, as understanding which x are more probable and which are less probable is relatively easy, but to estimate P(x) exactly would require integraing it over X, which is often an unachievable luxury. You want to sample points from P(x). Usually, because you want to integrate something over X with P(x) inside the integral.

To generate these points, we use an interative approach ("walkers"). Pick a random point x_t. Find a random step (from a good symmetrical distribution, usually Gaussian). Calculate how much probable it is compared to current x: α = f(x_next)/f(x_current). Accept the jump with probability α.

Obviously, it makes consecutive samples highly correlated. One option is to throw most points away. Another, use wide jumps (but then we'll reject most jumps of course). Convergence may be very slow. Also, first few samples (before it finds the dense part) are also bad (known as *burn-in period*).

> Not sure if these methods are considered **Bayesian**, or are just similar to them, but it seems related, right?

Some ways to improve:
* Calculate gradient and try to use it
* Jump back if dead-ended
* Use momentum

Alternatively, instead of a random multivariate step, we can iteratively construct a step, dimension by dimension, using various univariate probability functions (say, conditional probabilities). *Gibbs sampling* seems to work like that, if I got it right.

# Evolutionary algorithms

## Quality-Diversity
An approach to evo algorithms in which you don't just pick individuals based on top fitness, but to to optimize the diversity of solutions. Apparently also known as **illumination**.
https://quality-diversity.github.io/

List of papers: https://quality-diversity.github.io/papers

One of the productive algorithms in this family: Multi-dimensional Archive of Phenotypic Elites (MAP-Elites). The idea is that on top of a objective function (target to maximize or minimize) we introduce dimensions (like, different aspects, properties of the solution). Then each dimension is divided into bins, and thus the entire space is sections into cubes. The algorithm looks for a good solution within each cube, for each intersection of bins, and retains top performances in each cube. If the cube is empty, so be it.

Mouret, J. B., & Clune, J. (2015). Illuminating search spaces by mapping elites. arXiv preprint arXiv:1504.04909. https://arxiv.org/pdf/1504.04909.pdf

Dictionary of Terms
===
Not exhaustive: if a term is explained in one of the thematic chapters, it is not duplicated here. This list is only for "orphan" terms that are yet too short or off-topic to get a personal entry elsewhere.

## Yet Unprocessed:
Snowflake
Redshift

# AB

**Bagging** (ml): the simplest way to create an ensemble of classifiers using an algorithm: split data into bags (with replacement), train a different model on each of them. Final prediction = mean of all predictions. [ref](https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/). See .

**BIC** (stats): Bayes Information Criterion, an approach to model selection. To be described (for now, [wiki](https://en.wikipedia.org/wiki/Bayesian_information_criterion)).

**Boosting** (ml): an alternative to _bagging_ for ensemble generation: Iteratively select samples that previous learners failed to learn or disagreed upon, and use them to train new learners; majority vote at the end. [ref](https://analyticsindiamag.com/primer-ensemble-learning-bagging-boosting/). See .

# C

**CART** (ml): Classification And Regression Tree; often used for gradient boosting. An umbrella term to cover decision trees (classifiers), but also trees that have real numbers assigned to their leaves as outputs. See .

**Clique** (math): an all-to-all subgraph within a graph. Two confusing names: **Maximal clique** is just a clique that is not a part of a larger clique (you cannot include one more vertex or ege from a graph, and still have a clique). **Maximum clique** is the largest clique in a graph. Apparently finding cliques, as well as enumerating cliques, is an NP problem with no good solution. [wiki](https://en.wikipedia.org/wiki/Clique_problem), [current best algorithm (hard)](https://www.sciencedirect.com/science/article/abs/pii/S0305054810001504)

**Collaborative filtering** (business): a name for what Netflix users do when they watch movies and generate data of "co-occurrence". Probably a pre-DL synonim for Federated Learning?

# D

**DAG** (math): Directed Acyclic Graph

**Databricks Workspace** - workspace (environment) to interact with programs (jobs), notebooks, libraries, data.

**Dataiku** - some sort of visual programming language platform for data science?

**Design Patterns** (programming): A famous programming paradigm, and a name of a book by Peter Norwig, about typical ways to engineer interactions between classes and objects in object-oriented programming. About 23 or so archetypical solutions and interfaces that Java/C++ programmers learn by heart. [wiki](https://en.wikipedia.org/wiki/Software_design_pattern)

**Docker** - Cloud computing service that provides a pratform with OS-level virtualization: an isolated user space (called **container**) that seems like a dedicated computer to programs running on it. Containers can talk to each other via defined channels. But unlike virtual machines, there's only one OS, so is less resource-hungry.

**Document store** (database) - instead of rows, returns XML documents or something similar. [wiki](https://en.wikipedia.org/wiki/Document-oriented_database)

**Drop-in replacement** (development): Replacing part of the code without rewriting anything else. Aka "bug for bug compatibility" (drop-in will only work if all idiosynctratic bugs match exactly).

**Dynamic programming** (programming): Recursion (or some other form of divide-and-conquer) + memoization (to never calculate the same sub-problem twice).

# EF

**ECS** (dev): Amazon Elastic Container Server. To manage (run, stop) **Docker** containers on a cluster.

**EDA** (dev): Exploratory Data Analysis.

**Feature store** (data sci) A practical concept for data project implementation: a collection of curated features that are automatically produced (updated) from new data, and can be tapped into by various projects. Paying it forward with feature engineering.

# G

**GAM** (stats): Generalized Additive Models: f = sum of smooth functions. Often GLM + some smoothing splines.

**GBM** (ml): Gradient Boosting Machine. See 

**GCP** - Google Cloud Platform.

**GOFAI** (ai): Good Old Fashioned Artificial Intelligence (aka Symbolic Intelligence). Dominant until 1980. Plugging inputs into a logic scheme (at most - fuzzy logic).

**Golang**, aka **Go** - statically typed compiled language by Google, similar to C, but with memory safety, garbage collection, concurrency support, and structured type system.	

# HIJ

**Hadoop** - collection of open-source software utils for cluster support, maintained by Apache.

**Heap** (programming): a tree-like structure where children < parents. An optimal realizatin of a priority queue (similar to a stack, but with a build-in sorting according to priority).

**Indicator matrix** (data): A matrix Y, in which each row one-hot encodes the class g of a data point x. See .

**JVM** (dev): Java Virtual Machine.

# KLM

**Kafka** - open-source stream-processing plantform developed by Linkedin, written in Scala, and donated to Apache. High-throughput low-latency platform to handle realtime data feeds.

**Kubernetes** - Open-source system for deploying applications on a cloud. Originally developed by Google. Operates **pods** that each contains several **containers**, to be placed together, within the same localhost. Pods can communicate with each other via pod IP.

**LDA** 
1. (text): Latent Dirichlet Allocation (see )
2. (ml): Linear Discriminant Analysis (see )

**loess** (stats): Locally Estimated Scatterplot Smoothing, aka Savitzky-Golay filter. See 

**MAE** (ml): Mean Average Error, aka L1 loss.

**MapReduce** (dev): programming model for parallel computing; a type of split-apply-combine strategy. Essentially, seems to define two functions, one function on splittable tasks (something like `for each a in List`) that analyzes its `a` and sends answers out; and another one that catches answers and combines them all into one answer.

**MCMC** (ml) Markov Chain Monte-Carlo. See .

# N

**NMF** (text): Non-negative Matrix Factorization, V=WH + U, where W and H are non-negative, and also typically V is alot×alot, but W is alot×little, and H is little×alot (similar to other dimensionality reduction techniques). Often used with text analysis.

**noSQL** - a general word for non-relational s: not tabular-based, good for realtime big data. May work with documents, key-values, wide-columns, graphs. Have lower guarantees on consistency (stale reads: updates don't update immediately), which can lead to data loss. [wiki](https://en.wikipedia.org/wiki/NoSQL)

# OPQR

**Petri net** (math): a way to model discrete dynamic systems via a directed bipartite graph with 2 types of nodes: places and transitions. Tokens (agents?) can accumulate at places (states, circles), until a transition (bar) fires (consumes input tokens, and creates output tokens). Because of that, tokens inherently interact with each other. May be deterministic or not. [wiki](https://en.wikipedia.org/wiki/Petri_net)

**Pickling** (development): dumping binary data into a database, to be loaded later, instead of processing it in some meaningful (human readable) way. In Python, may be used for serializing Python object structure. A better alternative: JSON, which is human-readable, while pickles aren't.


**Plotly Dash** - A dashboard (interactive graphs and such) for Python and R. A bunchof JS, rendered in browser, that interacts with Python script that generated it via certain callbacks. These callbacks are put within the script in a certain way, and you can set (or know?) the functions the JS dashboard will call in Python when events happen in the dashboard, so you can define them, and thus process them.

**POC** (development): Proof of Concept.

**Propensity score matching** (stats, epidemiology) comparing output variable in a case of unavoidable confounding factors; go for something like range constriction for confounders by carefully matching them (many methods here), then analyzing this filtered set. The trick of this method in particular is that instead of matching in high-D, it uses an all-data linear model to conflate all confounding values into one "risk factor" (aka propensity score), and match based on it (both a strong, and a weak point obviously). [ref](https://en.wikipedia.org/wiki/Propensity_score_matching)

**PubSub** or **Pub/Sub** - asynchronous messaging (event ingestion and delivery) service used by Google cloud. [wiki](https://cloud.google.com/pubsub/docs/tasks-vs-pubsub)

**PySpark** - a Pythonic language (wrapper) to run on a JVM and interact with **Spark**. Coz it's a wrapper, apparently doesn't like dynamic typing that Python tolerates, but other than that is mostly similar.

**QR Factorizatoin** (math): A = QR where Q is orthonormal, and R is upper triangular. Achieved via Gram-Schmidt algorithm (aka elimination). See .

# S

**Scala** - functional programming language, an improved on Java. Compiles to Java bytecode. Looks kinda vaguely like a mix of Java with Pascal, inspired by Python, in the sense of having more features, allowing a shorter code, but without compromising the integrity. No semicolons; different ways to define variables, functions; declare types; can define control structures, and lots of other interesting stuff. Less verbose, runs faster, and I think supports parallelism better?

**Singular Value Decomposition** (math): A = UDV, where U and V are unitary (orthonormal, may be complex), and D is diagonal with d_ii ∈ R, d_ii>0. Dimensions: mn = mm∙mn∙nn. More general than spectral decomposition, as applies to all matrices on C, unlike QDQᵀ that only works for symmetic, or a slightly more general PDP⁻¹ for diagonalizable (sum dim eigenspaces = dim A).

**SMOTE** (stats): Synthetic Minority Oversampling Technique. A simple approach to oversampling (supplementing) under-represented categories by creating fake points (in pD of variables describing each case), that are linearlly shifted from under-represented points to one of its closest neighbors. [wiki](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) 

**Spark**, aka **Apache Spark** - a cluster computing framework (engine to work with big data). Written in **Scala**; donated to Apache. Runs on the JVM, nterfaces with programming languages, has its own SQL, graph processing, and what not.

# T

**Tehnical debt** (development) picking an easy (quick to implement) but inherently limiting solution, even though statistically every limiting decision now may have to be reingeneered (scaled up) later.

**Topic modeling** (text): unsupervised classification of pieces of text into different topics, similar to k-means clustering for numbers. Typically uses LDA or NMF. ([ref](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df))

# UVW

**Wide column store** (databases): Imagine a table, except that different rows may have different columns (number, names, types). 

# XYZ

**Zero-inflated models** (stats): models that are biased towards observations of zeros; for example a bimodal switch between active and inactive states, with inactive ==0, and active generating a Poisson process. [ref](https://en.wikipedia.org/wiki/Zero-inflated_model)

# Zettelkasten

This is my personal Zettelkasten system (knowledge base), built and updated with [Zettrl](https://www.zettlr.com/).

As it is a _personal_ knowledge base, some opinions may be wrong, the phrasing may be awkward, and many words may be spelled incorrectly. Yet the beauty of Zettlr as a Zettelkasten system is that every time I re-read these notes, can I try to improve them a bit, so the quality should be slowly improving.

Note that to link cards within the Zettelkasten system, Zettelr uses id-based links, such as this one: . GitHub, obviously, does not support this format, so in the public version of this base, all local links and references will be broken.


# Datavis inspirations
 

# Links

* [Marie Neurath](https://medium.com/nightingale/the-missing-legacy-of-marie-neurath-f9800733d1fc) - really nice mid-century datavis illustrator and pioneer. Extremely aesthetically pleasing and inspirational (blocky, modernist, wood-cut-inspired style)

# Temporary storage for all broad fun topics
 

# Free will

 - a quick summary on free will in human neuroscience
 - zombie roaches don't have free will ⇒ normal roaches do

Vohs, K. D., & Schooler, J. W. (2008). The value of believing in free will: Encouraging a belief in determinism increases cheating. Psychological science, 19(1), 49-54. - allowed people to cheat on a test, but asked not to cheat. Some received some sort of priming about how everything is deterministic. They cheated more. Was it replicated, I wonder?

Burns, K., & Bechara, A. (2007). Decision making and free will: A neuroscience perspective. Behavioral sciences & the law, 25(2), 263-280. - A decent long-form review with emphasis on humans and legal implications; 80 citations. Available online.

# Neuro-math

What explains the relationship between spatial and mathematical skills? A review of evidence from brain and behavior. (2020). Zachary Hawes. Daniel Ansari.
(Available on ResearchGate)



# Illusions
  

Adelson, E. H. (2000). 24 Lightness Perception and Lightness Illusions.
http://persci.mit.edu/pub_pdfs/gazzan.pdf
A very good richly illustrated review on illusions of tone perception created by shapes that are perceived as "photos" of 3D shapes with shadows. Summarizes  his (Edward Adelson) work on how the brain can essentially reconstruct 2 maps from one image: reflectance map (material) and illuminance map (shadows).

Geisler, W. S., & Kersten, D. (2002). Illusions, perception and Bayes. Nature neuroscience, 5(6), 508.
[academia edu questionable link](https://s3.amazonaws.com/academia.edu.documents/30895024/GeislerKerstennn0602-508.pdf?response-content-disposition=inline%3B%20filename%3Dpdf.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20200128%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200128T051333Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=edbdbab0eacdb82367f02bb1f501c62a3fc1b4f2036001009f1df08864ea6669)
Bayesian interpretation of visual illusions? Very short (like, a report).

Carbon, C. C. (2014). Understanding human perception by human-made illusions. Frontiers in human neuroscience, 8, 566.
https://www.frontiersin.org/articles/10.3389/fnhum.2014.00566/full
Hypothesis and theory article. While seems to claim (rightly) that the brain is a guesser, at least in the introduction, seems to also spin it philosophically in the direction of "everything is an illusion and objective perception is impossible", which I find rather questionable. If true, may even be used as a reference for "some people are mistaken about what illusions really are". But check first.

# Bibliography for Connectivity Reconstruction
 

Gerhard, F., Kispersky, T., Gutierrez, G. J., Marder, E., Kramer, M., & Eden, U. (2013). Successful reconstruction of a physiological circuit with known connectivity from spiking activity alone. PLoS computational biology, 9(7), e1003138.
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003138
Show that fancy point-based statistics works, while Granger Causality doesn't work. Use STG as a model (3 neurons with various connectivity patterns?)

# Tracking and analyzing animal behav
A collection of references to papers that track animals, and then analyze their behavior. (Not sure they deserve individual cards at this points, so going for a summary)

  

Shemesh, Y., Sztainberg, Y., Forkosh, O., Shlapobersky, T., Chen, A., & Schneidman, E. (2013). High-order social interactions in groups of mice. Elife, 2, e00759.
https://elifesciences.org/articles/00759
Simplistic analysis, and not that popular based on citations, but fun idea. They tracked 4 mice over time, hot-coded where within the arena each of them was at each time (vectors like x=(1 3 2 5), where each number is a nest, or a feeder, or something like that). Then analyzed these 4d vectors. Showed that if you guess full 4d P(x) from pairwise probabilities, you overestimate rare combinations by a lot. Which supposedly means that mice don't just interact pairwise, but behave as a group, and so "cluster" more tightly in the state-space. Nice, and probably very simple, way of analyzing this kind of stuff.



# On agruing and persuation


 - Cheap moves, unfrair interdisciplinary questions, as a way to shut down and derail the conversation

# DL on videos


As I dont plan to work on videos in the nearest future, but who knows what would happen in a more distant future, this would be a good way to collect good papers without comitting to reading them, for now.

Sideways: Depth-Parallel Training of Video Models
Mateusz Malinowski, Grzegorz Świrszcz, João Carreira1 and Viorica Pătrăucean (DeepMind)
17 Jan 2020
Claim that came up with a better way to integrate time into forward- and backpropogation- calcualtions. Some fancy-looking math, nice conceptual diagrams, result curves.


# New papers on Xenopus-related topics


# Collision avoidance

Stanard, T., Flach, J. M., Smith, M. R., & Warren, R. (2012). Learning to avoid collisions: A functional state space approach. Ecological Psychology, 24(4), 328-360.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.918.1282&rep=rep1&type=pdf
Seem to compare different types of logic (angle, derivative of it), comparing it to human behavior in a test paradigm. Come up with a linear model that reasonably explains the data?

# OMR
Xie, J., Jusuf, P. R., Bui, B. V., & Goodbourn, P. T. (2019). Experience-dependent development of visual sensitivity in larval zebrafish. Scientific Reports, 9(1), 1-11.
https://www.nature.com/articles/s41598-019-54958-6
Compare spatial frequency tuning between naive and visually experienced larvae. See an improvement with experience.

Kist, A. M., & Portugues, R. (2019). Optomotor Swimming in Larval Zebrafish Is Driven by Global Whole-Field Visual Motion and Local Light-Dark Transitions. Cell Reports, 29(3), 659-670.
https://www.sciencedirect.com/science/article/pii/S221112471931201X
Used reverse-correlation with bran recordings (from ZF larvae) to exactly pinpoint the stimulus detected by OMR circuits. As it turns out, it's a dark edge moving forward. Show funny image-like visualizations of "Behavioral-Triggered Averages" that feel a bit too complicated / counteritunitive to easily interpret. I originally thought these were the reverse-engineered stimuli, but they aren't, actually.

# M-cell
Koyama, M., Minale, F., Shum, J., Nishimura, N., Schaffer, C. B., & Fetcho, J. R. (2016). A circuit motif in the zebrafish hindbrain for a two alternative behavioral choice to turn left or right. Elife, 5, e16808.
https://elifesciences.org/articles/16808
May be relevant to decision making, even tho it's not collision detection per se.

# Xenopus specific

McKeown, C. R., & Cline, H. T. (2019). Nutrient restriction causes reversible G2 arrest in Xenopus neural progenitors. Development, 146(20), dev178871.
Without food, tadpoles enter developmental stasis: reduced neural proliferation. Everything resumes once food becomes available. Describe the molecular biology of it, what receptors are involved etc.

# ZF specific
Pursuit and Evasion Strategies in Zebrafish: Mathematical Modeling and Behavioral Experiments
AP Soto - 2019. PhD Dissertation.
https://escholarship.org/content/qt6f06r1f3/qt6f06r1f3.pdf
Reverse-engineering behaviors through kinematic optimization. Interested, as a genre, in case I end up optimizing the computational part of it.

The Zebrafish Visual System: From Circuits to Behavior
JH Bollmann - Annual review of vision science, 2019

Parallel channels for motion feature extraction in the pretectum and tectum of larval zebrafish
K Wang, J Hinz, Y Zhang, TR Thiele, A Arrenberg - CELL-REPORTS-D-19-02981, 2019

Antinucci, P., Folgueira, M., & Bianco, I. H. (2019). Pretectal neurons control hunting behaviour. Elife, 8.
https://elifesciences.org/articles/48114.pdf

# CS education
 

 - a bad take on not teaching kids programming, because cooking and fixing furniture is somehow closer to programming than programming classes (no, they aren't)
 - weak claim for negative effects of tech (computers, tablets) on student achivement. Interesting stats on computer access, across countries, and genders. Some data is really overinterpreted.

# Beam Search


**Beam Search**: a somewhat alpha-go-like formally coded tree of solutions for picking the best sequence, not just the best word, during translation. At each step you consider several winners (not just winner take all), explore a tree of possibilites, prune bad ones, and end up with an overall winner for a few words ahead. Introduced in (Wu 2016 Google Search paper), and later used in the  paper.

References:

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint. arXiv:1609.08144, 2016. 

# Deoldify: auto-colorizing old photos
https://github.com/jantic/DeOldify

Free online version: 
https://deepai.org/machine-learning-model/colorizer

  

Colorization of old photos is not just a great example of self-supervized learning (see ), but also a great test for **generalized intelligence**, separate from text. Which is nice.

For example, the current model (as of Jan 2019) always colors US flags right (as they have a recognizable pattern that it just learned). Some less common flags it fails to colorize (say, the Chinese Revolutionary Army, which was red with a little blue sun). But this is still a simple case of learning by correlation. Say, it seems (not sure, but it seems) to colorize flags with a Hammer-and-Sickle red (or at least reddish), suggesting that maybe it had seen them.

But show it a picture from some regional event in Soviet Russia, with Cyrillics, a portrait of Stalin and some flags, and it totally fails to color the flags red. Because this requires contextualization and generalization. A true understanding would still start with correlations, like clothes and photographic artifacts → era, letters → language → region. Even these are out of grasp of modern networks, as modern vision-procesing networks cannot really read, and also don't have access to everything learned by GPT2, for example. And even GPT2 only learned in one language, I think, didn't it? But even then, the final step would be a leap of faith, by guessing (Bayesian-style?) that this strage blurry photo from the 50s from somewhere in Tatarstan is likely too be Soviet, and thus the flags have to be colored red.

A sketch of a list that a model should be able to do to solve this puzzle correctly:
* Integrate across domains, such as images and text
* Be able to develop a hidden representation of image styles (retouche, objective type), integrate it with clues of time (clothes, postures, makeup, hair styles, car models etc) and region (on top of everything just listed, also vegetation, weather, inscriptions, cultural signs etc.)
* Maybe: somehow cross-reference it with historical information (learned by a history model), although this part is both ridiculously hard (how can you learn that a samovar is called a samovar?), and may not be necessary, except for most esoteric tasks
* Make cross-domain guesses: e.g. from knowing (in a statistical fashion) that Soviet photos from the 80s in Moscow had red everything, and from knowing (from texts?) that there was a cultural continuity for quite a few decades, infer that a much older photo from a different Soviet region should also be colored red.

Curiously, current DeOldify actually does a decent job with large-scale parades:
https://img-fotki.yandex.ru/get/15552/174326890.8b/0_f7b39_fabc4648_-1-XL.jpg
Which suggests that SOME Soviet photos made it into the training dataset.

Another intersting bias: how all Old Believers in kaftans in my photos were colored blue.

A consequence of that: if this sort of technology becomes widespread, it may shift our perception of the past. Instead of imagining that WW1 was BW, everybody will imagine uniforms and other clothes blue or violet, even if actually they were green, brown, or black. Kinda like what happened with Greek statues that we imagine white, even though they allegedly were all painted in bright colors (were they?)

# Gini impurity



Has nothing to do with Gini coefficient from economics, except that it was introduced by the same mathematician Corrado Gini ([ref](https://jamesmccaffrey.wordpress.com/2018/09/06/calculating-gini-impurity-example/)). Good for quantifying the quality of classification by a decision tree (not necessarily with 2 categories). Lies betweein 0 (limit case for one category overpowering all) and 1 (limit case for inifinitely many equally represented categories).

Quantifies the **probability of mislabeleing an element if we pick it at random**, and then **label it at random**, but with probabilities of different labels matching observed probabilities in the distribution.

Say we have N labels, and the frequencies of these labels are p_i. Then, assuming that these probabilities are true probabilities, if we pick an element at random, we will pick an element of true label i with probability p_i. If then we label it at random, we will label it incorrectly with a probability of (1-p_i). 

It means that the total probability of picking an element at random and mislabeling it is equal to I = ∑pi(1-pi) = ∑pi - ∑pi² = 1-∑(pi)² . In a binary case, 1-p²-(1-p)² = 2(p-p²), where p=Ncases/Npoints (for either class 0 or 1, as the formula is obviously symmetric).

When picking the **best split** for decision trees, for every point (as every point can potentially become a split point), calculate gini impurity for the left part (g1), and for the right part (g2). The total impurity for the split is the weighted average of these two impurities, weighted by the number of elements in the left and right parts: g = (n1∙g1 + n2∙g2)/(n1+n2). Decision split find the split point that achieves lowest total impurity. ([ref](https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe))

It seems that for **weighted Gini index** the formula is the same, just pi for each class is no longer just ni/N where N=∑ni, but rather the sum of weights within class, divided byt the total sum of weights: $p_i = ∑_j w_j δ_{ij} / ∑_j w_j$.

# References
* [wiki](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity)
* [Example calculation](https://jamesmccaffrey.wordpress.com/2018/09/06/calculating-gini-impurity-example/)

# Hotteling test


**Hotteling test**, aka **Hotelling's t-squared distribution** is a generalization of t-test to multivariate hypothesis testing. In essence, if you have 2 clouds of points in dim>1, and you want to check if the clouds are located differently, instead of doing a coordinate-by-coordinate comparison (lots of separate t-tests), it may be better to run this one.

Looks especially meaningful in 2D case, perhaps just because 2D visualizations are more common than many-D visualizations (irony haha).

Wiki claims that there are better alternatives, with some references.

**Refs:**
* [Wikipedia](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution)

# N-grams


https://en.wikipedia.org/wiki/N-gram
One of the simpler ways of text analysis: just take n consecutive words (or characters, DNA bases, aminoacids etc), and count the frequency of all n-pairs. It's obviously better than just word or letter statistics (that can be considered an 1-gram), but worse than actually understanding the meaning, or projecting larger bags-of words into semantic space DL-style.

Example: how to tell whether a text belongs to a corpus of texts? Cut the corpus into chunks of similar size, calculate cosine distances (inner products) of each bootstrapped chunk with the full corpus, find the sd. Then calculate cosine distance for the text in advance, go to z-score, and get a p-value that this text also belongs to the corpus. Or, alternatively, one can also calculate mean and sd for eacn n-gram independently, then find z-scores for text in question, and thus identify unusual n-grams.

Variants: **Skip-grams** - take a sequence of n words, but eliminate (skip) k words from it. Thus, (3,1) skip-grams from a sequence of 3 words would return 3 sequences: two 2-grams + one 1-skip-2-gram.

An obvious problem with noisiness (some n-grams would never be observed), so there exist various methods of smoothing. I haven't read about them yet, but for example:
https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation

# Perplexity


Mostly used with text. Intuitively, in its simplest form, just the inverse of average probability of words in a sentence. Except averaging is geometric:

per = P(combination of N words)^(-1/N)

If all words are independent and have the same probability p=1/k, for example, then perplexity = 1/(p^n)^1/n = 1/p = k. So in this case perplexity is ≈ vocabulary size.

As multiplying probabilities is boring, usually people do log, and then exp. Except that instead of e^x,  it's more fun to use 2^x.

$\displaystyle \text{per} = 2^{-\frac{1}{N}\sum \log_2 P(w_i)}$

This formula above is usually derived as a partial case of a more general definition, according to which perplexity is just 2 power entropy: P = 2^H, where H = −∑p∙log(p). The higher the entropy, the higher the perplexity. And in this case we use log with base 2. ([ref](https://stats.stackexchange.com/questions/10302/what-is-perplexity)). The lowest theoretical possible perplexity is that for complete certainty, 2^log(1) = 1.

The name originates from a different situation though: when a model is trained on probabilities {q}, but meets a sample with probabilities {q}. The measure of mismatch (the amount of information needed to encode a message form {p} using symbols from {q}) is equal to cross-entropy: H(p,q) = -∑p∙log(q). The higher H, the more confused, perplexed the model is. The better the match between p and q, the smaller the perplexity (as cross-entropy H(p,q) is always larget than entropy H(q) = H(q,q)).

From this pov, "vanilla" perplexity defined above is like calculating cross-entropy of model distribution with an equiprobable distribution, as if the model trained on q = P(w_i) encoutered and tried to encode an equiprobable sequence of words (or sentences), p = 1/N. Which maybe corresponds to a situation when a model engages with a text, and in this text each sentence is represented only once, so the probability of getting every sentence is p = 1/N_sentences? _Not sure about that._

When training models on text, **perplexity goes down for larger ngrams**. For a model trained on English journal articles with ~20000 vocab), for one-words (unigrams) perplexity~1000, for two-words (bigrams) ~200, for three-words (trigrams) ~100 (ref: Jurafsky). The higher  we consider, the more structure there is, on average, as most P is concentrated in fewer combos, and so −∑log(P(combo_i)) becomes smaller compared to −∑log(P(all equiprobable combos)). In a random text, perplexity would have stayed constant with ngram length incease, but for a structured text, it goes down.

**Perplexity minimization** (for p from test data, and q from model predictions) is a great objective for training text generators. The lower perplexity, that is, the more certain the model is about each next word in the test dataset, the better. Low perplexity strongly correlates with human ratings of bot quality, such as the average of Sensibleness and Specificity measures (SSA), which in turn positively correlates with subjectively assessed "human-likeness" (ref: Adiranwana 2020). For a good model, vocab is ~10k, while perplexity is ~10 (ibid).

Perplexity of two models can only be compared if they use the same vacabulary, and tested on the same testing dataset (Jurafsky).

# Refs
* [Post by Lei Mao](https://leimao.github.io/blog/Entropy-Perplexity/) with some general formulas
* Adiranwana 2020: Towards a Human-like Open-Domain Chatbot. Google Research, Brain Team. Jan 2020.
    * https://arxiv.org/pdf/2001.09977.pdf
    * 		https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html - summary
* 		Yale: http://www.cs.yale.edu/homes/radev/nlpclass/slides2017/213.pdf
* 		Standord: https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf
* 		Both are based on this textbook by Jurafsky and Martin: https://web.stanford.edu/~jurafsky/slp3/3.pdf

# Point processes


Review of Probability Distributions for Modeling Count Data. F. William Townes (Submitted on 10 Jan 2020)
https://arxiv.org/pdf/2001.04343.pdf
When to pick Poisson, or Negative Binomial, or Dirichlet-multinomial, or yet something slightly different yet.

# RBF = Radial Basis Function
 

The idea is to go from Cartesian space X to a space defined by each point's proximity to a set of archetypical origin points {μ}. So you define a new set of values parameters K, quantifying each point location, such as K_ij = exp(-γ ∙ dist(x_i, μ_j)). Then you optimize for {μ}, γ becomes a hyperparameter, and you also still have to optimize decision boundaries using some classification method (such as SVM, for example).

> So it feels like 3 nested optimization problems: find optimal decision boundary (SVM?), assujming a set of points (μ), that themselves are optimized given a certain γ.

Refs:
* [wiki](https://en.wikipedia.org/wiki/Radial_basis_function_kernel)
* On their use with SVMs: [a blog post by Ajay Yadav](https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589)

# Smoothing
 

# Splines
Consider **Piecewise polynomials**. The simplest example ever: downsampling, which is the same as piecewise constant, or just replacing values with their means. Better: piecewise linear fit. Even better: piecewise linear continuous at knots. Continuity implies limitations (as in a+bx = c+dx), and so a lower number of parameters to fit.  For **Splines**, not only f(x), but also first derivative f'(x) are continuous on the edges of segments. Usually cubic, but not necessarily. There's apparently an efficient way to calculate them, called a **B-spline basis**, but park for now. (ESL p144, referencing Esl chapter 5 appendix) 

**Natural cubic spline**: is forced to become linear on all knots (with f''=0), with no curvature. Three benefits: 1) same polynomial can be safely extrapolated outside the range, as it fits edge data with a reasonable straight line (instead of a crazy overshoot that cubic splines always want to do), 2) two fewer degrees of freedom per spline to deal with, 3) arguably, look quite nice. Formulas may be solved explicitly, it is just annoying. [ref](https://towardsdatascience.com/numerical-interpolation-natural-cubic-spline-52c1157b98ac)

# Smoothing splines
Instead of picking knot points (for splines), let's use regularization. Loss = ∑(y-f(x))² + λ∫(f''(x))²dt, where λ is a **smoothing parameter**. Then apply it to a natural spline sequence with knots in unique values of x_i (that apparently happens to be an optimal function for this type of task). Apparently, there also exists an optimal basis of natural splines F (B-splines? _Or are B-splines different?_), so f = ∑Fi(x)θi, and after combining it with the equation fo loss, we get a generalized ridge regression:

L(θ, λ) = (y-Fθ)ᵀ(y-Fθ) + λθᵀΩθ, where Ω is a matrix with $Ω_{jk} = \int F'' _ j(t)F''_ k(t)dt$. It gives the solution: θ = (FᵀF + λΩ)⁻¹Fᵀy. There exist computationally effective way to calculate that all. 

An approximation for y can now be produced from here, by doing h = Fθ = F(FᵀF + λΩ)⁻¹Fᵀy = Sy, where S is a **smoother matrix**, or smoother operator. Linear in respect to y. This matrix S = F(FᵀF + λΩ)⁻¹Fᵀ is similar to a standard projection operator B(BᵀB)⁻¹Bᵀ; it's also symmetric positive semidefinite, but while H² = H, S² ≤ S (because of the shrinkage). As for a (reasonable?) projection operator tr(H) = the number of coordinates remaining in place = dimention of the projection space, we can use df$_ λ$= tr(S) an estimate for the **effective degrees of freedom**.

Another way to write S is to split it in a so-called **Reinsch form**: S = (I+λK)⁻¹, where K is not depended on λ, and is called a **penalty matrix**. (_I'm not immediately sure why it is possible._) The igenvectors can be found (ESL p155), and they don't depend on λ. The **Bias-Variance Tradeoff** is quite prominent in this case (ESL p159)

>  : ESL p161 to p181 skipped for now: Nonparametric Logistic Regression, Multididimensional splines, Reproducing Kernel Hilbert Spaces (RKHS), 

# Loess

**Locally Estimated Scatterplot Smoothing**, aka **Savitzky-Golay filter**. Say you have y~f(x), but very noisy. Set α as **smoothing paramter**: α = K/N, where K is the number of local points to fit a polynomial, out of total number of points N. If using cubic polynomials, α obviously has to be larger than (power+1)/N = 4/N. 

And also, for the fitting purposes, to calculate the value of smoothed function in x, we weigh different points x_i ; typically using $w_i = (1-d^3)^3$, where d is something like distance from x to x_i, but scaled to $[0,1]$: $d = |x-x_i|/\max_{ij}(|x_i-x_j|)$ for xi, xj ∈ {K neighbors}. This weird formula is called **tricube**, and it's a popular pseudo-gaussian kernel apparently.

Refs: [wiki](https://en.wikipedia.org/wiki/Local_regression), [nice lecture pdf](http://pj.freefaculty.org/guides/stat/Regression-Nonlinear/Nonparametric-Loess-Splines/Nonparametric-1-lecture.pdf)

# Wavelets

> Also parked for now: Wavelet smoothing (around ESL p170)

# Kernel Smoothing Method

> ESL 192

# Stupid back-off


**Stupid Back-off**: an official name for a simplistic Markovian text-generation model, where you learn a bunch of n-grams. Then when faced with a generation task, you match your seed n-gram with a dictionary of n-grams. If no match, or low match, look for (n-1)gram, and so on, until there's a match with decent count (and thus, probability). Some "Brants 2007" paper. The description is from the [Jurafsky Martin 2019 textbook](https://web.stanford.edu/~jurafsky/slp3/3.pdf) on language processing.


# TFIDF


TF-IDF = Text frequency - Inverse document frequency. A measure of how often a word is used in a document, compared to all other documents. tfidf() = tf()∙idf(). Here:

* tf() = **text frequency**, which is usually this word frequency in the document (% of all words), or log() of it, or normalized frequency, or just raw count of words, or maybe even binary (yes or no)
* idf() = **inverse document frequency** = usually, the share of documents having this word, but flipped and log-ed: idf() = log(N/(1+n)) + 1, where N is the total number of documents, and n is the number of documents containing this word ([wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) has several more alternatives).

Apparentlyh, can be justified via conditional entropy. 


# Transformers
  

# Background and history

An alternative to RNNs (summary ref: [1](https://towardsdatascience.com/transformers-141e32e69591)). Text is inherently a stream of signals (words), organized in time, so it makes sense that it should be analyzed by **RNNs**. The problem is that text has long-ish connections (say, objects are elliptically passed from one sentence to another), and RNNs have problems with that (forget, or rather, don't even learn). Then **LSTM** was invented, which improves things, but not enough, as the context still tends to decay exponentially. Also, hard to parallelize, as inputs need to be processed word-by-word. Then they came up with **attention**, by assigning every word a certain "hidden state", and passing it to the entire line (_??? I really not sure how it works_), but still it didn't help enough. The lack of parallelization was the main bottleneck.

But convolutional networks that process info in chunks are highly parallelizable! And the information depth is lower (instead of going through all N elements, activation from the furthest element climbs up the hirarchy through just log N layers). Transformers (developed by Google Research and Google Brain) combine the idea CNNs with attention.

# Structure

Transformers consist of several (original paper has N = 6) **encoders** and the same number of **decoders**. Each encoder has the same architecture, but different parameters, and the same is true for d coders. 

## Encoders

Each **encoder** consists of **two layers**: **Self-Attention** block, consisting of several self-attention operations performed in parallel (aka **Multi-Headed attention**), followed by a shallow **FF network**. 

### Self-Attention

First encoder receives words, and embeds each word in an **embedding** vector (say, length 512). Self-attention transforms this vector with 3 trainable matrices into 3 smaller vectors (length 64) **Query, Key, and Value**. 

* For each pair of words ij, we calculate a **score**, which is a **Query-Key interaction**: s_ij = ⟨q_i , k_j⟩, 
* scale it down roughly (s_ij/8), and 
* softmax (now ∑s_ij = 1). These **final scores** S quantify interactions between the words. 
* Then for each target word i we calculate the weighted sum of its scores s_ij with other words (j), using a set of **Values** v_j (each one – itself a vector) as weights: z_i = ∑ s_ij ∙ v_j. This set {z_i} becomes an output of the self-attention layer.

In practice of course, vector multiplications for each word happen not in a loop, but in matrices Q, K, and V, with columns corresponding to words. A full formula for attention, parallelized, for all words : $Z = Ⰿ(Q^T K / \sqrt{d}) V$, where Ⰿ stands for columnwise softmax operation. Remember that $Q = XW^Q$, $K = XW^K$, and $V = XW^V$, where X is the input to this layer.

Several (h=8) attentions are calculated in parallel, each with a different set of {W} for QKV. The outputs of different attention heads are then concatecated together, to form a longer vector of length 512, which is passed further. This is called **Multihead Attention**.

> So, this 512-long output for each word contains 8 attention outputs  concatenated, each representing a diffeerent "idea" (QKV values) about this word. And we have a matrix to represent all words in a bag.

### FF network

Each output Z from the attention layher is passed to a FF network. The **FF network** consists of a dense ReLu layer, followed by a dense linear layer, with coefficients W_1 and W_2.  that performs A (dim=512) → transform with W_1 (to dim=2048) → ReLu → transform with W_2 ( to dim=512 again).

### Residuals

Endoders were not just stacked on top of each other in a simple FF fashion, but used **residual connections**, which means that out = in + f(in). So for each encoder, they added its input to its output; then **normalized** the results (subtract mean, divide by sd).

## Position encoding

At the embedding stage for the original text input, they also **encode word positions**, using some sort of a concatenation of two chirp-like signals, one sin-, another cos-based, with exponentially decreasing frequency (see the formula below). This appears to be something like a kernel trick (?). To make a DL ANN relate to position, it's better not to encode it as a one-hot variable (as it would waste 2 layers just to quantify it), but instead encode different positions as different combinations of nicely distributed basis functions. (A visualization of these functions is shown [here](http://jalammar.github.io/illustrated-transformer/))

$f(k,p)=sin(p/τ^{(k/d)})$, where k is depth (from 0 to d) running down the embedding vector; p is the position of the word within a sentence, and τ is some arbitrary period (they used d=256, and τ=10000). And then they do the same with cos, and *interleave* these two embeddings elementwise, so that they go 1a 1b 2a 2b etc. 

Claim that this particular embedding is good, as "f(p+offset) is a linear function of f(p)". What they seem to mean here is that f(p+offset) = Af(p), where A is some matrix.

> Is it true? I'm still confused about why these particular functions were chosen. Why chirps? And where does this claim about linear combinations come from? Perhaps it's simple and obvious, but I'm somehow missing it.

And then it looks like they literally **added**, not in terms of concatenation, but **elementwise**, one 512-long vector to another, semantic embedding to position embedding. 

> How on Earth this is possible, and why it is a good choice, I cannot understand. Maybe that's why they used this exponential chirp, as it has a very slow, long, flat tail? So essentially, once they train the semantic embedding part, it mostly utilizes the tail part, and leaves the front intact? Still I don't understand why they wouldn't encode the position into few first bits, and then just concatenate it with semantic embedding.

> Also, wouldn't it mean that semantic embedding cannot be trained separately from the position embedding? Isn't it a debt liability, as it means that they cannot use pre-trained semantic embeddings, when, say, moving to another language?

## Decoders

Each **decoder** had **3 layers**: a similar **Self-Attention** (analyzing inputs from the previous decoder), followed by an **Encoder-Decoder Attention** layer, in which Q comes from the previous layer, whille K and V  come from an output of a matching encoder, followed by a **FF network**. Apparently, it mimics what RNNs were known to do well before transformers (aka "RNN encoder-decoder mechanism").

The **first decoder** receives **output at the previous time step** as the input to its Self-Attention layer. All other decoders receive inputs from previous encoders. 

**Self-Attention layers** in decoders are similar to that in encoders, with one difference: all transformations for a given word don't get access to word after it (they are **masked** with -inf).

>Why on Earth is a good idea?? Don't we prepare grammatically for words that are about to come? And the final output of the model is one word anyways, isn't it? I'm really confused.

**ED Attention** layers in all decoders always get inputs from the top (final) output of encoders. But not the final final output (not the Z vector), but rather Keys and Values inputs form the last encoder, while Quries are internally generated from the input to this layer (output of the self-attention layer).

At the very end of all decoders, a linear layer with ~10000 outputs, and a softmax to find the best word.

# Training

In the original paper, for the final model, they averaged last 20 checkpoints. _Why??_

# Application

For machine translation, they actually don't just apply the model, but use a thing called **beam search**: at each point they don't just take the max probability word for each word, but select first 4 options, and for each assume that it was taken, and re-run the model, generating a tree of solutions (depth 50, I think?). Finally, pick the most probable total solution out of all these solutions. And there's a punishment on length. 

>  So I don't think they explore the whole tree to full depth; I think they truncate branches if they exceed combined level of total inprobability. But not sure how it was done in practice. There's a link ("ref 31") below that explains beam search, apparently.

# Open questions I still have
* How many words do they process at once? What's the size of the bag? And why cannot I find this information in the original paper?
* Why position encoding is so weird
* Why residual connections are a good idea
* Why decoders are so weird, with this KV from one sourse, and Q from another?
* Why this masking of self-attention layers in decoders is ever a good idea?
* How come we ended up with this really weird architecture, and how have we convinced ourselves that it is beautiful and optimal in any sense? Is it really optimal in some way, as our concepts of simplicity and symmetry are wrong, or is it an example of ad-hoc "evolution of human thought", as wild and random as Darwinian evolution?

# Criticism
There are claims that transformers are overrated, and same results may be achieved with simple architectures, and much lower computer:
* Merity, S. (2019). Single Headed Attention RNN: Stop Thinking With Your Head. arXiv preprint arXiv:1911.11423. [link](https://arxiv.org/abs/1911.11423)

# References
*   - the original Transformers paper, titled "Attention is all you need". Doesn't actually describe the motivation, nor how they came up with this, nor how it works, nor the philosophy of it. ([Direct pdf](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf))
* For **word tokens → 512-vector embeddings**, "ref 24": Using the Output Embedding to Improve Language Models, 2017, Ofir Press, Lior Wolf. ([direct pdf](https://arxiv.org/abs/1608.05859))
* For how their **positions encoding** iks as good as learned position encoding, "ref 8":  Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.
* For **beam search**, "ref 31": Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016. 

**Comments, reviews, explanations:**
* Re-implementation of the original paper, in Python: http://nlp.seas.harvard.edu/2018/04/03/attention.html
* http://jalammar.github.io/illustrated-transformer/ - a very nice diagrammatic explanation from Jay Alammar
* https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8
* https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1
* [http://jalammar.github.io/illustrated-gpt2/](<http://jalammar.github.io/illustrated-gpt2/>)





# Triplet Loss

Described in a paper I haven't yet read:  Schroff2015facenet

Say, you want the model to learn that all objects in group A belong to class A, etc. If you just reward low Δ score for objects in A, model will map everything to const and achieve Δ = 0. So instead take 2 objects from A and 1 object from B, and make the model minimize $D_+$ = D(a1,a2) relative to $D_-$ = D(a1,b). 

Say, you can drive $max(0,D_+ + (M-D_-))$ down, where M is some characterist distance that we want to promote between objects from different classes. 

> In the paper, they apparently use some other tricks as well. 

Paper: FaceNet: A Unified Embedding for Face Recognition and Clustering Florian Schroff, Dmitry Kalenichenko, James Philbin


# Zettelkasten

 

## How One German Scholar Was So Freakishly Productive
Clear 2019
https://writingcooperative.com/zettelkasten-how-one-german-scholar-was-so-freakishly-productive-997e4e0ca125

A good description of how zettelkasten works. The main idea is linking cards. So, to paraphrase, **the rules** seem to be:
1. Always add new stuff to the system. Don't take other notes, in other notebooks and places. Feed the Zettelkasten!
2. Don't be afraid to create new notes. Don't try to impose a pre-conceived rigid structure. Add new notes (nodes), and let the graph live.
3. Always link new nodes to existing nodes. Don't leave them alone, integrate them into the system, by linking to stuff.
4. Never just cite; always either comment liberally, or rephrase. Treat it as a conversatoin, ot shape it into an external memory.

Some **asides and consequences**:
* As an aside: tags are good, but limiting. Still it's probably a good idea to at least tag any new card.
* Aside to aside: at this point I don't know how to tag this one ;)
* In Zettlr (as in paper card version) direct (outcoming links) are clearly visible, while incoming links are hidden. Still, unlike for paper notes, in Zettlr you can fetchc the name of this card, and put it in the search box. You'll get all cards linking this card.
* My current system of naming cards after papers (author-year-topic) is actually still not complete Zettelkasten, as it retains (and follows) the desire to keep control of card readability. The original system by Luhmann allowed him to let go of this fear of control, and just inject information into the system, relying on the power of links.

## References about Luhmann

* **Archive of Luhmann's cards:** https://niklas-luhmann-archiv.de/bestand/zettelkasten/zettel/ZK_1_NB_1_1_V

It seems that he also mostly filed cards within a tree. So every card was "guaranteed" to belong to a certain thematical box (sometimes strongly, sometimes loosely). Any other links would come on top of that. Perhaps With Zettlr, it's easier to use tags as this "guarantee of minimal inclusion", just because they are so much easier to implement.

* **Why Luhmann had to start a 2nd system ZK2** (abandoning the ZK1): https://zettelkasten.de/posts/luhmanns-second-zettelkasten/
* Some **analysis of how Luhmann branched ideas**, with a conclusion that his process was NOT specifically about branching ideas, even if in many cases it happened like that: https://zettelkasten.de/posts/luhmann-folgezettel-truth/

It seems that while his system was ingenious, Luhmann was also constrained by the physicality of cards that could only get that much info on them. So if he kept having thoughts, he would have to create a linked list of cards. We however can just add stuff to the same file.

This note, for example, would have been represented by several linearly linked notes in his system. But in Zettlr, it's easier to just keep writing, until you need to branch (and actually, maybe, even for some time after that, until the branch becomes too long).

* From another [blog polst by Phil Houtz](https://writingcooperative.com/zettelkasten-its-like-gtd-for-writing-and-here-s-why-you-should-consider-it-7dddf02be394): Actually, as it turns out, **Luhmann had tags** as well. For keywrords and key themes, he used special cards that listed cards related to this keyword.

> Which implies that maybe my current system has everything he had. The general outline and structure of the repo is analogous to his filing system; longer files that grow out of a seed of an individual card are analogous to his chains of linked cards, and tags are like keywords. Only better. And I can look for things. And I can search incoming links. And I can improve my cards much more easily! Yay!

# Some writing by Luhmann himself

## Communicating with Slip Boxes
http://luhmann.surge.sh/communicating-with-slip-boxes

For this system to work, it should feel like a communication. For it to feel like a communiation, it should be different (have a different schema) than whatever you have in your mind as you use the Zettelkasten, or try to use it to write. There has to be an element of surprise. 

Advices against "filing in order", as in a book catalog, precisely because it would bind you to a certain system of thinking for decades. Instead, just file them one after another, with incremental address-liek numbers. _Which implies that either I have misunderstood the structure of the archive above, or that the archive was restructured when it was put online, to make reading easier, or that Luhmann actually was more systematic than he wanted to be, and than he recommended._

He also had bibliographic notes (that only contan info about the books), and then cards dedicated to these books, once he read them. At which point, obviously, the former link to the latter. But at the same time, different thoughts and ideas from the same book should be split across different cards, so that, ideally, they would aquire a life of their own. Because some will be involved into clusters of cards, while some will be forgotten, or maybe even lost, and it is OK.

> This may imply that linear sequences of thoughts, like this file, should be mercilessly broken down once there arises a slightest temptation to break them down (to reference a part of it). Because if you want to reference a part of the file, then you shoudl really have two files (with the first linking to the 2nd).

A funny quotation: "If you wish to educate a partner in communication, it will be good to provide him with independence from the beginning. A slip box, which has been made according to the suggestions just given can exhibit great independence."

> Still unclear, from this short article, how he wrote after all. Did he start with an outline, and then followed it, going on tangents through the links?

## More info on how Luhmann wrote
https://mindyourwriting.wordpress.com/2015/09/10/how-famous-researchers-work-niklas-luhmann/

It seems that, when it was time for him to write the book, he would just start creating an outline, linking it to cards. He spent most of his time on that: linking cards, and maybe improving cards themselves, and not working with any kind of "final text". And then he would just produce a text; and he woudn't revise it.

Which is where we should stop following his stead, it seems, as his writing is apparently is well known to be confusing, extremely dry, convoluted, and hard to read. Poorly structured, repetitive, long, and aesthetically unpleasing (it's a quotation from someone).

> But that's OK, as once we have a draft text, we have other ways of working with it, don't we? If we solve the problem of making an interesting first draft appear, that would already be a huge win!

# Algorithms


# Big O notation
Limiting behavior at ∞. Different meaning in textbooks and in practical questions. In textbooks: upper bound, so if something is O(N), it is also O(N²) by definition. In practice, just say the truth. 

* O(log N) appears when you have a tree, and always process one branch of this tree. Then the total number of steps = the number of branches = $\log_2 n$ . 
* If you have to visit every branch of a tree, the total number of operations is n (for leaves) + n/2 + n/4 + ... = 2n, so we have O(N).
* If we have to through all n elements at each recursion before going further, and if the recursion is good (we split in two equal sizes), then we have a recursion: T(n) = n + 2T(n/2) . Unraveling: T(n) = n + 2(n/2 + 2(n/4 + ... 2(1))) = n + n +  ... + n . This sum contains k ns where k = ceil(log_2 n), so the total O = nlogn. 
    * Quick sort has O(n log n) on average, but still goes to O(n^2) for the worst case. 
    * Mergesort is O(n log n) even for worst case, but takes more space.
    * Heapsort is mathematically the best, but 1) is real hart to explain, because of the heap data structure (sort of a tree), in practice is slower than quicksort.
* Selection sort (repeatedly find min element) is O(N^2) as it does the triangle thing (loop in a loop) through the array.

# Classic algorithms

**Median of medians**: Recursive way to find a median in linear time (median through sorting is O(n lg n) ). Imagine we have a linear agorithm for finding i-th element (i=N/2 for a median). Split the array into groups of 5. Find median of each; then find the median of medians. It is guaranteed to be between 30th and 70th percentiles (just draw it for a proof). Set this value as a pivot. Binary split all elements into those lower than the pivot, and those higher than it. If upper=lower, pivot is the median. If not, look either for the k-th elements of the upper part (if upper is larger), or i-k-1 -th elements of the lower part (if lower is larger.)  Because the size of the array is guaranteed to shrink fast (not slower than 0.7 for main recursion step, and 0.2 for the median of median), by induction, the entire thing is O(n), just multiplied by some weird factor. Refs: [wiki](https://en.wikipedia.org/wiki/Median_of_medians), [better explanation](https://www.austinrochford.com/posts/2013-10-28-median-of-medians.html)

**Stable matching algorithm**: A elements rank B elements by preference; B elements rank A elements by preference; find a matching such that no cross-wiring (swapping) of matches can improve the match (for all those 4 that would be cross-wired). The logic: each A proposes to their B. Each B picks the best A of those that proposed. All A that aren't matched propose to their second choice. If B can upgrade, they upgrade. Repeat until everyone are matched. Proof that all matched at the end: the least lucky A will propose to all B, so there cannot be a free B. Prove that the match is stable: if KL MN can be improved to KN ML, then K prefers N, but then K proposed ot N earlier; as N is with M, N must have traded K for a better match after that, so MN→KN isn't an improvement for N, contradiction. Reference: [wiki for the problem](https://en.wikipedia.org/wiki/Stable_marriage_problem), [wiki for the algorithm](https://en.wikipedia.org/wiki/Gale%E2%80%93Shapley_algorithm)

# Math about graphs


See also:  - linear algebra about graphs

# Sorting and searching
**Topological order** (aka topological sort): any sequence of vertex ids, such that for no directed edge A→B, vertex B is given in the sequence before vertex A. Is only possible in a DAG (in a graph without directed cycles), as obviously a directed cycle makes a sequence like that impossible to construct.

In general, 2 main ways to explore graphs:
* **DFS**, aka **Depth-First Search**
    * Recursive: often easier to code.
    * Takes less memory than BFS (no need to store a queue)
    * Naturally creates **topological sorting**: the order in which it _leaves_ nodes (after processing them and all their dependencies) is a valid topological order (that's assuming that you add elements at the beginning of your list, as in stack, rather than at the end. It's called **reveresed postorder**).
    * _According to [this quora response](https://www.quora.com/What-are-the-advantages-of-using-BFS-over-DFS-or-using-DFS-over-BFS-What-are-the-applications-and-downsides-of-each), also has some other benefits that I don't quite understand yet: finding bridges, connected components, articulation points (aka cutvertices), planarity test._
* **BFS**, aka **Breadth-First Search**
    * Naturally finds a **shortest path** from the root node (while DFS can first arrive at a vertex in a very roundabout way)
    * Creates a more balanced tree (with more similar distances from the root to the furthest points in each branch)



# Information and entropy


Stone, J. V. (2018). Information Theory: A Tutorial Introduction. arXiv preprint arXiv:1802.05968.
https://arxiv.org/abs/1802.05968

## Entropy

For an event x with probability p(x), the amount of information obtained if we learned that the event happened is -lg(p(x)). Nice property, because if two independent events happen together, probabilities multiply, and so informations add. lg() is a base-2 logarithm.

Entropy of a random variable = expectation of entropy of a message (expectations of entropy if you observe a value of this variable), so = -∑p(x)∙lg(p(x)). Note how it makes some variables more informative (surprising, unexpected) than others: if it's highly skewed (in an extreme case: almost always one value), entropy is small (for a constant, it's 0). That's intuitive, and mathematically that's because x log(x) → 0 when x→0. (The way it is proven, BTW, is by replacing x with "divide by 1/x", then taking a derivative from both numerator and denominator that is called "L'Hopital's Rule")

References:
* Jason Brownlee about [entropy](https://machinelearningmastery.com/what-is-information-entropy/), [cross-entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) (confusing)

## Cross-entropy

Definition: H(p,q) = -∑P(x)∙lg(Q(x)), or integral for continuous.

Is used a loss for classification tasks (see respective chapters: classification and DL).

Not to be confused with joint entropy, even tho people use same notation H(p,q). Joint entropy is just an entropy of a vector variable composed of two variables that are "joined"; nothing fancy.

## KL divergence

Kullback–Leibler Divergence between 2 probability distributions P and Q: 
D(P||Q) = -∑p∙lg(q/p)
(they use this funny symbol || everywhere)

Note that:
* division by p, so p cannot be 0 anywhere
* Asymmetric. P is the "target distribution", and Q is the one you compare to it.

Sometimes is called _relative entropy_ of P in respect to Q. Like, how much more you learned if you updated from Q to P (Bayesian), or how many bits you need to transmit a message from P using a code optimized for Q.

From this point of view, entropy for a variable X following distribution P:
H(X) = log(N_events) - D_KL( P || uniform_across_events )
Because for uniform P(x) for every x we have max entropy, and we updated from it to P, entropy decreased. Like, we learned something ([ref: wikipedia](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)).

> Interesting that in terms of update, it is P that is a posterior, or an update, while if you use it as a loss, P is the "ground truth", and Q is compared to it. So which one is known first, and which one second, is kind of opposite in these two scenarios. What is common, is that P is a better known, "more true" distribution.

KL divergence is related to mutual information: I(X;Y) = D_KL( P(X,Y) || P(X) P(Y) ) . That is, how much extra information you have, if you know how events from X and Y match each other, compared to if you only know them separately).

Cross-entropy of two random variables: H(p,q) = H(p) + D_KL(P || Q)

## JS divergence

Most commonly used symmetric alternative to LK ([wiki](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)). 
JSD(P||Q) = ( LK(P||M) + LK(Q||M) )/2
where M = (P+Q)/2
Always between 0 and 1.

# Linear algebra

**Linear** means f(ax) = a f(x), and f(a+b) = f(a)+f(b). Examples of linear: negation, reversal, running sum, demeaning (subtract mean(x) from all x_i elements of x).

*Th*: If f(x) is linear, it can be represented as a matrix multiplication Ax
Proof: x = sum x_i e_i , and as f is linear, f(x) = sum x_i f(e_i) where f(e_i) is in out-space (codomain) of f. If we write coordinates of each f(e_i) in this space, we get a matrix with columns of f(e_i), and f(x) = Ax. QED 

What most people think of as "linear" is actually **affine**: Ax+b (linear + const). This representation is unique, and A = (f(x)-f(0)) - because -f(0) kills the "b" part, and then we have f(x)-b linear, so see above.

Examples:
* Cumulative sum, as applied to a vector length n, and producing vector lengths n. A lower triangular matrix of ones.
* Difference matrix: the opposite of that, and not square, as takes n-long x, and outputs n-1-long with elements x2-x1 etc. Elements of this matrix are like -1 1 shaped into a diagonal, which fits fine as the matrix has a shape of (n-1)n.
* First-order Taylor approximation: f(x) ~ f(x0) + Df(x0)(x-x0) where D = Jacobian, or matrix of all possible derivatives of coordinates of f over coordinates of x: Df_ij = df_i/dx_j . Obviously affine.

## Examples of linear equations
Ax = b means that b is a linear combination of columns in A. If x like that exists, then b can be represented via columns of A. An example: **Polynomial interpolation**: $y_i = \sum c_k x_i^k$ . If we consider  A with elements a_ij = x_i^j , this turns into a matrix equation. And an A like that is called **Vandermonde matrix**.

**Markov model**, aka linear dynamics model : states in next moment of time x(t+1) = Ax(t). A is a matrix of transitions. (There may also be an input with matching dimensions). Example of a model: pandemics (not as a network model, but a basic model of transitions: health -> infected -> either recovered or dead).

Same approach can be used to numerically solve DE, as x = x + x'dt, so A = (1 dt ; 0 1) where dt is an integration step for Euler integration; would take (x, dx/dt) and produce a new pair.

_Th:_ A composition f(g(x)) of two linear functions is also linear. That's obvious.
_Th:_ A composition of affine, is also affine. Also obvious.

Example: **Second difference matrix**: a multiplication of two different matrices $D_{n-1}D_n$ . The shape of this matrix is obvious from matrix multiplication. Consider chain rule for h=f(g(x)) , where everything is multivariable. Then $dh_i/dx_j =  \sum_k dh/dy_k (g(x)) \, dg_k/dx_j$ , which totally looks like a vector equation. Or rather a matrix, as this is true for every coordinate i . A matrix of all possible derivatives of elements of f, taken along each of the dimensions (coordinates) of g, is multiplied by a matrix of all possible coordinates of g, taken over dimensions of x. Dh() = Df(g()) Dg().

If h() yields a scalar, then a vector of dh/dy is just the gradient ∇h, in which case we get:
∇h = (∇f^T ∇g)^T = ∇g^T ∇f.

If g() is affine Ax+b, then ∇g = Aᵀ, and so ∇h = Aᵀ ∇f(). (vlms p185)

> How come ∇(Ax) = Aᵀ? Because to use derivatives in a vector space, we  have to assume a certain **Layout convention** ([wiki](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions)): namely what happens if y and x are vectors, and we're considering ∂y/∂x. For consistency, a derivative of scalar by a vector (gradient), and a derivative of vector by a scalar (like, speed?) should have opposite orientations (if one is a row, the other one should be a column). Or you won't be able to work with vector fields (as one way or another, you need to get a matrix once you differentiate vector by a vector). If you want gradient of a scalar be a normal vector (column), it's called _denominator layout_, or _Hessian formulation_. But then derivative of a vector by scalar is transposed compared to the vector, which feels super-wrong (only think of speed!). If on the other hand you want the devative of a vector have the same shape as the vector itself, then it's called _numerator layout_, or _Jacobian formulation_. Then gradient is a flipped vector (row), making ∇ᵀ a normal vector (column-vector). As a consequence, it gives ∇(Ax) = Aᵀ. And if your matrix of derivatives y over x looks like $d_{ij} = \frac{\partial y_i}{\partial x_j}$, then ∂/∂x(Ax) = Aᵀ, meaning that it's a denominator layout. VLMS book uses this layout (ref: wiki)

# QR factorization

**Gram matrix** or **Gramian**: for a set of vectors {v_i}, a matrix of all possible inner products: G_ij = v_i ∙ v_j. Sometimes people use the word "Gramian" for the determinant of this matrix. (ref: wiki)

If v_i are arranged into a matrix A (as columns), Gram matrix can be defined as G = A'A. For orthonormal {v}, A'A = I.

Properties: a transformation defined by an orthonormal matrix Ax→y preserves inner product. Indeed: (Ax)'(Ab) = x'A'Ab = xb. As a consequnce, |Ax| = |x|. And so angles between vectors are also preserved.

> Here VLMS splits the topic in 2 pars: first Gram-Schmidt (p 97), then much later QR (p 190). I combine both together.

**Gram-Schmidt algorithm** for finding a set of orthonormal vectors {q} that span a set of vectors {a}. And also, identifies whether {a} are independent, but finding the first linear combination. Repeatedly, 1) subtract projections on all previously found vectors, 2) check if you didn't get a zero (in which case - quit!), 3) if not zero, normalize.

```
for i in 1:k
  q_i = a_i - sum_{j in 1:i} proj(a_i on q_j) 
      = a_i - sum_j (q_j'∙a_i)q_j
  if q_i is 0, stop
  q_i = q_i / |q_i|
```

Why orthogonal? Prove by induction. Say, q3'∙q2 = (a3 - q1'∙a3 q1 - q2'∙a3 q2)'∙q2 = a3'∙q2 - (q1'∙a3)(q1'∙q2) - (q2'∙a3)(q2'∙q2) , which shows all key patterns. Here we consider new i and old j<i, and get crosses of j with all possible k<i. By induction q_j∙q_k is 0 for all j≠k, and the only one j=k gets cancelled with a_i∙q_j. So we get 0 for all j≠i, proving that "original q_i" are orthogonal. And then separately we notice that the actual, updated q_i, are also normal.

Now, in matrix form vectors {q} form Q. As they are orthonormal, Q'Q = I. By Gram-Schmidt column of A are in column space of Q, as they can be represented as a linear combination of them, so **A = QR**. Here Q is orthonormal, and R is upper triangular (as a_i is only expressed through q_j for j≤i).

Fun claims (from VLMS exercises):
* Orthogonal matrix with all elements a_ij≥0 has to be a permutation matrix (a_ij = 0 or 1, with only one 1 in each col)
* A block-matrix (U U; V -V)/sqrt(2) made of orthogonal U and V is also orthogoal
* K-means in matrix form: X ~ 	ZC where Z is a matrix of means, and C is a selector matrix (c_ij = 1 if vector i is assigned to cluster j). Now with this limitation on C, k-means becomes a matrix optimization problem.

# Inverses
Left inverse: X for A such as XA = I. Not necessarily unique.

_Th:_ If left inverse exists, then columns of A are linearly independent. Proof: Say they aren't. Then ∃ a lin comb of cols of A with non-zero coeffs x that gives 0, or Ax = 0. But as left inverse exists, ∃C: CA = I. Then we can write x = Ix = CAx = C0 = 0. Contradiction with prior assumption that x≠0. □

As a consequence, only square or tall matrices are left-invertible (wide necessaritly don't have linearly independent cols).

Similarly, right inverse. Full inverse = both left and right. Therefore, invertible matrices has to be square. Five equivalent statements: that A is invertible, that it has left inverse, right inverse, all cols are independent, and all rows are independent. (Easy to prove: from side-inverse you prove independence (above), from there ability to serve as a basis, represent unit vectors in this basis, thus proving other-side inverse existence).

If full inverse for A exists, then Ax = b has a solution for every b.

**Dual bases**: columns of A, and rows of B form 'dual bases' if AB = I (for invertible A). Inner-product with vector of {B} give coefficients in the basis of {A}. _I don't like it, as it feels obvious, with a loud name._

A **Triangular matrix** with all non-zeroes on the diagonal, is inversible. That's because all cols are linearly independent, which is because Lx = 0 implies x=0 (by induction across all coordinates of 0, as it settles all elements of x to 0. For a lower triangular matrix, do induction from 1 to n; for an upper trianguler: from n to 1).

# Linear algebra and networks


## Graph representations
**Adjacency matrix** (obv.). For a directed graph powers of A (A^k) give the number of paths length k that lead from node i to j.

**Laplacian matrix**: L = D-A where A = adjacency, D = degree matrix (diagonal, with 0 at i≠j and d_ii = degree of i_th node). For oriented graphs, apparently, there exist two different Laplacians, depending on whether you choose to work with in-degree or out-degree? _What does it mean in practice?_

**Incidence matrix**: M(e,i) = 1 if edge e goes i->j, -1 if edge e goes  j->i exists, 0 for all other cases. Not square: E rows (number of edges), N columns (number of vertices).

If f is E-long vector of total flow in each edge, then Mf = total flow into each vertex. If the flow is balanced (no accumulation of stuff in nodes), Mf = 0. If it's not balanced (there are sources: a V-long vector s), then Mf = -s, or Mf + s = 0.

This obviously turns any flow (diffusion) task into a matrix equation.

And the other way around, if nodes have potential (v), then M'v is a vector of differences in potential. The measure of how high these differences are is called **Dirichlet energy**: D(v) = sum M'v ^2  = sum_e (v_i - v_j)^2 . That is, sum of squared differences in v across every exiting edge. (At least it's claled that way according to VLMS book;  Wikipedia seems to disagree and calls it 'Dirichlet sum', which is less fancy)

**Th:** L = M'M . **Proof:** L_ij = ∑_e M'(ie)M(ej) = ∑_e M(ei)M(ej) . (We used e as a running variable here, as a foreshadowing, as M(ei) indicates whether an edge e really goes out of node i (-1), or towards node i (1)). Therefore, if i≠j, M(ei)M(ej) will give 0 for all edges that don't connect i and j, and -1 for the only one connecting them. If i=j, it will give +1 for all edges that either start or end at j, so it will give a total degree of D. That's assuming no loops. But M cannot represent loops anyway, and L cannot represent loops either (if diagA≠0, it gets subtracted from D). QED.

Dirichlet sum seems to be related to Laplacian quadratic form, but for now I'm not sure it's the same Laplacian; may be one of its normalized cousins: https://en.wikipedia.org/wiki/Laplacian_matrix

## Other weird Laplacians

> According to Wikipedia, there are like 6 alterenative formulations for a Laplacian, but let's park it for now.

## Graph Spectra



# LA for signal processing

## Convolution

$c_i = \sum_{k=1}^n a_{i-k+1} b_k$ . For example, for n = len(b) = 3, c3 = a3b1 + a2b2 + a1b3 , so b goes backwards in time. On edges (where b doesn't fit), it's logical to just crop it (so use min(n,legal) for upper target)

As it's linear, it may be represented as c = Ta where T = almost diagonal matrix with b written in columns, and sliding down. T_ij = b_{i-j+1} if legal subscript, and 0 otherwise. Apparently it's called a _Toeplitz matrix_.

In practice conv isn't calculated via direct matrix multiplication though, but via a fast convolution algorithm, somehow based on FFT. Wow.

2D convolution is similar, VMLS denotes it as $A\star B$ ; as it's 1D sister it's commutative, associative, and linear for one matrix if the other one is fixed.

# Lagrangian


A way to solve equations with constraints.

**Statement:** to minimize f(x) given a consraint c(x)=0, just consider f(x)-λ∙c(x), and solve for ∂L/∂x=0 and ∂L/∂λ = 0 (like, unconstrained in dimensionality p+1). Here, λ is called a **Lagrange multiplier**.

**An intuitive illustration:**

Say, c(x) is nice enough that we managed to solve it, and explicitly express some subset of X coordinates (let's call it y) through the rest of X (let's keep calling it x). 

Let's say the solution for c(x,y)=0 is y=g(x).

So now instead of f(x,y) given c(x,y)=0, we get f(x,g(x)), without constraints, that we can solve directly:

$\displaystyle \frac{df}{dx} = \frac{∂f}{∂x} + \frac{∂f}{∂y} \frac{dg}{dx} = 0$

On the other hand, let's look at what Lagrange proposed. That we take f(X)-λc(X), and set partial derivatives to 0. As in this case we (supposedly) found an explicit form for y = g(x), so we can replace c(X) with (y-g(x)), and f(X) by f(x,y):

L = f(x,y) - λ(y-g(x))

Differentiating, we have a system:

$\displaystyle \frac{∂L}{∂x} = \frac{∂f}{∂x} + λ \frac{dg}{dx} = 0$

$\displaystyle \frac{∂L}{∂y} = \frac{∂f}{∂y} - λ = 0$

From the second one here, we see that λ = ∂f/∂y, which can be put in the 1st equation, giving us the same exact equation that we got above! So at least with the assumptions we made, and with this particular type of c(X) constraint, Langrangian leads to the same formulas, and thus the same optimum, as the direct calculation. Except it doesn't really need an explicit form of y=g(x), and so can handle more arbitrary constaint.

> It's not a proof tho, is it? More of an illustration, as g(x)-y is a very specific form of c(x,y)=0.

Best search site:
[https://www.compart.com/en/unicode/](<https://www.compart.com/en/unicode/>)

∙ −
≠ ≤	≥ ≪	≫ ≈ ≡ ≔ ∥ ∦
∑ ∏ ∫ ∬
⟨ ⟩
⇒ ⇄ ↻ → 
∆ ∇ ∂
ᵀ 
⁻¹ √ ∝
∀ ∃ ∄ ∅ ∈ ∉
𝒩 ∞ ℝ
□∎
⨀⨂⨁ ∘ •

Α α, Β β, Γ γ, Δ δ, Ε ε, Ζ ζ, Η η, Θ θ, Ι ι, Κ κ, Λ λ, Μ μ, Ν ν, Ξ ξ, Ο ο, Π π, Ρ ρ, Σ σ/ς, Τ τ, Υ υ, Φ φ, Χ χ, Ψ ψ, Ω ω ϵ ϖ ϰ ϑ ϱ ϒ ϕ
ϐ ϗ Ϙϙ Ϛϛ Ϝϝ Ϟϟ Ϡϡ Ϣϣ Ϥϥ Ϧϧ Ϩϩ Ϫϫ Ϭϭ Ϯϯ ϴ ϵ ϶ Ϸϸ ϻ ϼ Ͻ Ͼ Ͽ
℥ℨ

ᕵᕴᒋᒉᕕᕓᕗᕙᕈᕋᕂᕆ

ა თ ი ო ბ ზ მ ნ პ რ ს შ ჩ ძ ხ ჰ გ დ ე ვ კ ლ ჟ ტ უ ფ ღ ყ ც ქ ჭ ჯ წ ჱ ჲ ჳ ჴ ჵ ჷ ჸ ჶ ჵ ჹ

ա	բ	գ	դ	ե	զ	է	ը	թ	ժ	ի	լ	խ	ծ	կ	հ	ձ	ղ	ճ	մ
յ	ն	շ	ո	չ	պ	ջ	ռ	ս	վ	տ	ր	ց	ւ	փ	ք	օ	ֆ	ու	և

 ⰀⰁⰂⰃⰄⰅⰆⰇⰈⰉⰊⰋⰌⰍⰎⰏⰐⰑⰒⰓⰔⰕⰖⰗⰘⰙⰚⰛⰜⰝⰞⰟⰠⰡⰢⰣⰤⰥⰦⰧⰨⰩⰪⰫⰬⰭⰮ
 
 æÆ
 
 よのまて
 
 https://en.wikipedia.org/wiki/Mathematical_operators_and_symbols_in_Unicode

Testing fancy formulas:

$\displaystyleლ = \frac{1}{ზ}\sum^{ზ} _ {უ=1}{ტ _ ბ\big(ბ_უ-წ(ქ_უ\cdot ჱ)\big)^2}$


$\displaystyle \large{Ⱉ=\frac{1}{4Ⰲ} \sum_{Ⱁ,Ⱂ \in Ⰸ} \big(Ⱋ_{ⰑⰒ} - Ⱑ_{ⰑⰒ} \sum_{Ⰻ\in Ⰸ} Ⱋ_{ⰑⰋ} \big)Ⰵ_{Ⱁ}Ⰵ_{Ⱂ}}$

$\displaystyle \large{Ⰱ(Ⱓ) = \frac{Ⰿ(\frac{Ⰳ+1}{2})}{\sqrt{ⰃⰗ}Ⰿ(Ⰳ/2)}\left(1+\frac{Ⱓ^2}{Ⰳ}\right)^{-\frac{Ⰳ+1}{2}}}$



Testing weird interactions between italics and mathjax in markdown:

This is fine before $(a^{ij}_k)$ but becomes italics after
This is italics before $(a^{ij}_ k)$ but is fine after
This is fine both before $(a_{k}^{ij})$ and after
This is also fine both before $(a _ k^{ij})$ and after

# Emergent Tool Use from Multi-Agent Interaction
Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., & Mordatch, I. (2019). Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528.
https://arxiv.org/abs/1909.07528

Also this blog summary with illustrations and videos.
[https://openai.com/blog/emergent-tool-use/](<https://openai.com/blog/emergent-tool-use/>)

   

That OpenAI hide-and-seek study. Agents progressively built 6 distinct strategies and counter-strategies (some of them glitch-based).

Hiders are rewarded for not being seen, seeekers - for seeing them (and opposite punishments). "Seeing" objects is directional, but there's also "sensing objects" and distance to them that is lidar-like (all directions, but with occlusion). Also a prep phase, and a penalty for going far from the "main arena". No direct reward for interaction with objects etc.

Emereging **strategies** create an **autocurriculum**: a game-like dynamics. Each next behavior negates the previous one (random > chasing > door blocking > ramp use > ramp defense). Coordination and coopearation emerges, including who will grab which object, and passing objects. Even more strategies in a more randomized environment, including "shelter construction", and finally glitch-based "box surfing" and a counter-locking it. This also counts as **tool use** (and they have some more refs for it).

Training infrastructure an algorithms were described earlier:
* [OpenAI Five](https://openai.com/blog/openai-five/)
* [Dactyl](https://openai.com/blog/learning-dexterity)

The compute is huge of course (described on p7)

"Each object is embedded and then passed through a masked residual self  block, similar to those used in transformers, where the attention is over objects instead of over time. Objects that are not in line-of-sight and in front of the agent are masked out such that the agent has no information of them."

> What does it mean? It feels that I don't understand attention well enough to even understand this sentence.

Policies are trained using [Proximal Policy Optimization](https://openai.com/blog/openai-baselines-ppo/)

They also compared a direct game with a game in which agents had "intrinsic " to visit as many states as possible (esp. infrequently visited states). It appears that states were expliticly coded (like, the position of each box etc.) It didn't work (less efficient, and looks weird: less purposeful). I'd say however that their implementation of motivation seems botched (overparameterized, lack of generalization and pruning).

**Evaluation** of different policies: mention "Metrics like ELO (Elo, 1978) or Trueskill (Herbrich et al., 2007)" (?), but then also say that they don't really work.

Then tested  on novel tasks: count objects, lock and return, sequiential lock, blueprint construction, sheltere construction. For most, agents pre-trained in "Hide-and-Seek" had  a leg (but not for shelter construction, surprisingly!!) They also compared it with pre-training on a much simpler (I think?) policy (explicitly motivated counting of objects), and it performed similarly, or even better. So this wasn't that much of a success, which is actually cool.

# Network neuroscience

Bassett, D. S., & Sporns, O. (2017). Network neuroscience. Nature neuroscience, 20(3), 353.

 
The most famous  on this topic.

Great introduction. First talk about network science in general, and how it applies to brains, including the limitations of straight connectomics, and the necessity of statistical and hierarchical approaches. Then cover , cavities, and persistent topology. Introduce activity on networks, and of networks (dynamic networks), prophesizing that the combination of both is the future. 

Multilayer networks: combine several networks (across time or subject) into one, using "node-identity" edges. A whole section on "network control" - that is, using network info to nudge the activity of the system.

# Cyclic transitions between higher order motifs underlie sustained activity in asynchronous sparse recurrent networks
Bojanek, K., Zhu, Y., & MacLean, J. (2019). Cyclic transitions between higher order motifs underlie sustained activity in asynchronous sparse recurrent networks. bioRxiv, 777219.
https://www.biorxiv.org/content/10.1101/777219v1.article-info

Updated version tbc (plos?)

    

Is it possible to predict whether a spiking network with a given topology would spontaneously halt (truncate), or become persistently active, after being activated with a kick to a few newurons? Can we guess it, either from simple network properties, or early on, from the dynamics of network activation?

> Something like a halting problem for directed graphs :)

While simple properties, such as network density, define the region at which persistent activity is possible, there is lots of variability in activation stability between different instantiations of a random network. Moreover, even for networks that can support persistent activity, a reasonable change in the starting conditions may lead to a truncation. 

Even more curiously, truncation is not always easy to predict, as the network may seem to be quite active, before suddenly coming to an unexpected halt. 

To some extent, truncation can be predicted from the statistics of network activity, but perhaps in somewhat unexpected ways: say, **synchronous networks are more prone to truncation**, compared to asynchronous networks. That's not what many people would have probably guessed.

# Motif analysis

A critical part of this paper is that they generated some connectivity networks, but then ran activity on them, and looked at **activation graphs**, or something like that. So not what motifs were present in the underlying skeleton, but what motifs were actually activated by the network. That's kinda unusual, isn't it?

To quantify that, they looked at the comparison of three **motif-bound clustering coefficients**, aka **motif propensity** or **triplet clustering coefficients**. For example, when they write that in-fan motifs were more pravalent than out-fan motifs, it is (_I think?_) equivalent to a statement that nodes with high in-degree have higher clustering coefficient. 

Or, more precisely, but much less neatly, in this example, if two edges were often activated in such a way that they converged on the same node (aka in-fan), their backs were more likely to be connected and activated consecutively (triangle motif, or clustering coefficient), compared to a different pair of edges that started at one node, and diverged activation to two different nodes (fan-out).

It seems that stable networks tend to have a balance of 3 types of non-cyclical triangle motifs (fan-in, fan-out, and middle-man), and that these **types of motifs are often activated in succession** (perhaps as an epiphenomenon of network modularity, with activity fanning from smaller clusters to larger clusters, before returning back to smaller ones). If this quasi-cyclicity is absent, the networks are unstable. Moreover, if you go into a 3D space of activations for these 3 types of motifs, and look at trajectories, they mostly fall in a 2D plane lying at an angle in this 3D space. If you loook at the trajectores in this 2D plane, **persistent networks were nicely quasi-cyclical** (chaotic, but constrained), while networks that eventually halted, first did all sorts of extreme stuff, with wild variations of activity, and then just got ejected away from the attractor, towards nothingness.

In activation graphs, fan-in seem to be particularly overrepresented. 3-cyclical graphs are an order of magnitude more rare.

The statement about motif cycling was quantitatively replicated on random Erdos-Renyi graphs.

> An interesting statement then: can you fix a halting (truncating) network? Like, what is the minimal number of edges to add (or remove?) from a halting network to make it persistent? And how to find them? Is there a good heuristic?

> It seems that in this particular paper, they rewired networks randomly (not sure if from scratch every time, or continuously with continuous monitoring), but there was no genetic alrogirhm, and ther was no target rewiring.

# Model

Leaky integrate and fire (AdEx), 4000 excitatory + 1000 inhibitory neurons, conductance-based synapses (mean 1.13 nS for excitatory, ~ 10 times stronger for inhibitory), synaptic strength comes from a heavy-tailed log-normal distribution. Resting ≈ -70 mV, spike threshold ≈-50 mV, Cm ≈ 280 pF.

> So excitatory reversal was 0, so excitatory drive of 70e-3 V, giving the current of ~1e-9 * 7e-2 ≈ 1e-10 A. Prob acting for a few ms, (say, 10 ms, as their excitatory time const is 10 ms). It gives 1e-10 * 10e-6 = 1e-15 Q. With Cm of 3e-12 and ∆V=20mV=2e-5, we need CV=6e-17 to spike. Right? So does it mean that 1 incoming spike was enough to make another neuron spike? Looks like that.

Connectivity probabilities that they lifted from previous literature: Pee = 0.2, Pei = 0.35, Pie = 0.25, Pii = 0.3 (here "ee" stands for excitation-excitation, etc.). But then they set Pee = 0.2, Pii = 0.3, and used computational experiments to find a region where network was **running the longest, but with lowest spiking rate** (kinda "threshold area", in a way), which happened at Pei = 0.22 and Pie = 0.31. So close to previous studies, but slightly different. Probably if one were to try another model, this would have to be recalibrated, as it obviously depends on time constants for excitation and inhibition, but at the very least, it's a good starting point.

**Model creation:** first set 50 random clusters, then assigned each excitatory neuron to 2 different pre-defined clusters simultaneously. Neurons were 2 times more likely to connect within their clusters, then outside of them. Inhibitory connections were fully random.	

Then selected "good" networks: they used a proxy for synchrony: var_t( mean_n(V) )/mean_n( var_t(V) ), where V are voltages for each neuron, variance is taken over time, and averaging happens across all neurons. In other words, do neurons variate together (which would make variation of toal signal ~ variation of individual neurons), or do they sub for each other (that would bring var(mean(V)) to 0). A value of 0.5 as an arbitrary threshold for "asynchrony".

And then, it seems they rewired networks randomly, keeping the density of connections as constant as possible (but it seems, still a narrow range, so not exactly constant), until the networks would become low-spiking and low-synchrony (aka near-critical).

# References

* When talking about quantification of network branching (a measure of critiquality): Beggs JM, Plenz D. Neuronal avalanches in neocortical circuits. J Neurosci. 2003;23(35): 11167-11177. doi: 10.1523/JNEUROSCI.23-35-11167.2003.

# A rubric for ml production readiness

Breck, E., Cai, S., Nielsen, E., Salib, M., & Sculley, D. (2017, December). The ml test score: A rubric for ml production readiness and technical debt reduction. In 2017 IEEE International Conference on Big Data (Big Data) (pp. 1123-1132). IEEE.

https://storage.googleapis.com/pub-tools-public-publication-data/pdf/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf



Tests for features and data:
1. Schema: encode tests for some obvious facts or asymptotics, end test them explicitly (and automatically). Visuals are helpful to identify and produce the schema.
2. Unhelpful features are weeded out
3. Features that are hard to compute but that add little are eliminated
4. If external requirements (e.g. legal) make certain features inacceptable, enforce that programmatically
5. Data pipeline has automated privacy control. Tight access right. Propagate data deletion.
6. Build in potential to add new features
7. Test 	new features before including them in data generation process

Tests for model development:
1. Version control + code review for all changes
2. Understand the relationship between offline (e.g. loss) and online (actual target) metrics
3. Tune all hyperparameters
4. Know the impact of model staleness. How often do you need to update?
5. Regularly test against extremely simplified models, to make sure you see the difference
6. Model quality is sufficient on all important data slices (not just on average)
7. Model was tested for social considerations, like inclusion

Test for ML infrastructure:
1. Training is reproducible (deterministic training is best. Seed random generators?)
2. Unit test model: e.g. configuration files, single step of gradient descent, restore from checkpoint. Training to overfit may be included here.
3. Test pipeline integration
4. Automated quality test that terminates a model if it's obviously off (compared to the prev one?)
5. There's a debug mode (that allows to run this model on a single example and reports all stages)
6. "Use a canary". What they mean is: make a gradual roll-out, don't remove old model immediately, compare as you go
7. Rolling back is easy and safe

Monitoring tests:
1. Monitor dependencies, updates
2. All data is tested via schema (above), alert if weird. Tune false-positives to make it meaningful
3. Monitor whether live behavior matches testing behavior (apparently in complex situations they may diverge, and it's like problematic)
4. Staleness
5. Alerts for NaNs, zero weights and the share of ReLUs returning zeros
6. Monitor compute times, and alert if they increase (technical reasons)
7. Make sure performance doesn't drop.
    * No bias, and no change in bias: 0.5 probability should be true in 50% of cases. (Definition of an unbiased model: average of predictions = average of observations. [ref](https://developers.google.com/machine-learning/crash-course/classification/prediction-bias))
    * If possible, measure or estimate actual performance
    * If not, manually generate new testing data every now and then

Other pieces advice:
Checklistgs are good. Use them.

# RandAugment

Cubuk, E. D., Zoph, B., Shlens, J., & Le, Q. V. (2019). RandAugment: Practical data augmentation with no separate search. arXiv preprint arXiv:1909.13719.
https://arxiv.org/abs/1909.13719



[Video summary from HenryLabs](https://www.youtube.com/watch?v=Zzt9i3gDueE).

The key part seems to be that it automatically finds a sequence of augmentations to boost your accuracy when learning on an image dataset. And it does it computationally cheaper than some other competing solutions (some thing called AutoAugment?), and has fewer hyperparameters (two). Originally they thought of it as a sort of  learning, as you want to start with small augmentation amplitudes, and then gradually increase them. But then they realized that actually this curriculum doesn't matter at all, and you can as well always use const amplitude, or random amplitude; you get exactly same final results.

I didn't understand the story about some "small subnetwork" that they need to find something something? Not sure.

What I did get, and liked, is a part where they look at the effect of adding more, and different transformations, on learning quality. Shear (y) and translate are OK, but rotation is the key! Rotation  is the most informative type of augmentation. On the contrary, in turns out that simple changes of color (contrast, brightness) don't matter, while things like solarize and posterize actually _harm_ performance. In terms of the number of different transformations, rotations also saturate really fast (relatively few rotations bring you most of the way to the full effect), while with cuts and translations you need to get dozens of those for every image before you reach similar performance.

# Woodlice move at random
Fraenkel, G. S., & Gunn, D. L. (1961). The orientation of animals: kineses, taxes and compass reactions.



Woodlice apparently just move randomly with speed ∝ dryness and light. So they stop moving once it's damp and dark. Spectacularly simple, and spectacularly effective!

# Free Energy Principle
Dec 2016, Karl Friston, something like a blog post
http://serious-science.org/free-energy-principle-7602



Not a good summary. Bad summary. It pretends to be written informally, but is actually pretty hard to understand.

Brain as a machine that collects of information: not just uses it to infer stuff, but seeks it, and even chooses which info to seek. But to have information, you need objects, or states, that are separate from each other. **Markov blanket**: according to [wiki](https://en.wikipedia.org/wiki/Markov_blanket), some sort of a collection of variables that describe the node, which in a markov net includes the node itself, everything that immediately leads to it (parents), and all immediate children.

For the brain, he claims, it's all the sensory info (sensory state) + active states, or the output of behavior, sorta.

And then he claims that a stable system doesn't dissipate (with some metaphors about ink in water and what not). Which means apparently that for this active system with a Markov blanket, if it were to exist for a prolonged period of time, it needs to maximize Bayesian model evidence (what??). Or, which is the same, minimize free energy (what ??).

Somehow brings up, as an example, that functional areas in the brain stem from the fact that this world has properties, which means that it may be beneficial, from the practical point of view, considering them separately. Like "what" and "where". That's a direct consequence of the fact that objects move, but remain the same as they move, so concepts of position and identity may be disentangled. 

From there, somehow arrives at gradient descent. Extremely unclear!

# Weight Agnostic Neural Networks

Gaier, A., & Ha, D. (2019). Weight agnostic neural networks. In Advances in Neural Information Processing Systems (pp. 5365-5379).
http://papers.nips.cc/paper/8777-weight-agnostic-neural-networks.pdf
Google Brain

Also a blog post:
https://weightagnostic.github.io/

  

Used evolution algorithms to find networks that perform well with **random weights**, using precocious animals (those that can do useful things right after being born) as a metaphor.

> Compare to  who used a simpler search + some weight-learning. If you invest in search, you can divest from training.

Tasks used: bipedal walker, car racing (lil 2D game).

In related work, interesting links to **network pruning** and **neuroscience**

# On predatory wasps and zombie cockroaches

Gal, R., & Libersat, F. (2010). On predatory wasps and zombie cockroaches: Investigations of free will and spontaneous behavior in insects. Communicative & integrative biology, 3(5), 458-461.

 

This is a **comment on a research paper** by same authors: Gal, R., & Libersat, F. (2010). A wasp manipulates neuronal activity in the sub-esophageal ganglion to decrease the drive for walking in its cockroach prey. PLoS One, 5(4), e10019.


Parasitoid jewel wasp *Ampulex compressa* stings roaches in the head, injecting venom into its cerebral ganglia. That turns roaches into "**zombies**". And this poses an interesting conesequence for interpreting spontaneous behavior in insects as "free will". If when they are messed up by a wasp, it feels that roaches were turned into "zombies", and stripped of something, doesn't it mean that they had this "something" originally? **If hijacked roaches don't have free will** (or at least it feels like it to us), then it kinda follows that **normal roaches do** have free will.

Start with nice poetic references to Jewish theological literature. But do small animals have free will, or are they just automatons with "mistakes"? Example of a fly in a featureless white space that still flies randomly.

What happens to roaches: after being stung, they don't get paralyzed, but also don't initiate spontaneous behaviors, and don't escape from aversive stimuly (the wasp eats its antennae and drinks hemolymph). Then the wasp **grabs the antenna and pulls the roach**, and it "**follows submissively**", with a normal walking pattern. Then it leaves the roach in the nest, puts an egg on its leg (note: not inside the body as with many parasitoids), seals the nest, the roach eventually gets eaten from inside.

From the  neuroscience pov, the venom seems to selectively elevate the threshold for initiation and continuation of walking, while walking itself, and thresholds for other behaviors, are apparently spared. _But then why doesn't it fly away?_

Some particular ganglion (sub-esophageal ganglion, or SEG). Same effect as in zombie roachces if anasthetic is injected there. Inject in another ganglion; can have a super-running roach.

# References
* Fruit flies flying randomly: Maye A, Hsieh CH, Sugihara G, Brembs B. Order in spontaneous behavior. PLoS ONE. 2007;2:443.

# “What not” detectors help the brain see in depth

Goncalves, N. R., & Welchman, A. E. (2017). “What not” detectors help the brain see in depth. Current Biology, 27(10), 1403-1412.
https://www.sciencedirect.com/science/article/pii/S0960982217304049

 

How do we see in 3D? Intuitively one could think: find same details (corresponding to the same object), then triangulate. But if you think deeper, doesn't work, as the space for searching is too sparce, and generally, if 3D relies on _differences_ between L and R eyes, why would it start with finding similarities? 

Also we know from studies in V1 that RFs for L and R eyes are actually very different, making any sort of "matching" problematic. Here they use ANNs and a principle of optimal coding to show that actually RFs should be complementary, as then neurons can look for differences right away.

Their ANN: 2 layers:
* 2 images from 2 eyes as an input
* bank of trainable linear filters ("simple units"): essentially, a convolutional layer, but looking at matching segments of images from both eyes at the same time
* ReLu
* 2 units in the output layer: "Far" and "Near". 
Supervised learning of all weights with true Far/Near labels. 

Achieved 99%+ accuracy. The first layer developed something like Gabor filters, but not matching between L and R images. Instead, phase-shifted (in extreme case - reversed, corresponding to a pi/2 phase shift). 

Tested with noise stimuli, confirmed that the network as a whole responds to translational shift of the pattern.

This result actually explained "a long-standing puzzle from the psychophysical literature ref 22, 23 that demonstrated better judgments for stimuli comprising dark and bright dots (mixed polarity) compared to only dark or only bright dots (single polarity) /""


# A Visual Exploration of Gaussian Processes
Jochen Görtler, Rebecca Kehlbeck, Oliver Deussen
https://distill.pub/2019/visual-exploration-gaussian-processes/
Interactive introduction.

 

A random process x(t) (note: not one trace, but the whole "process", understood as all possible traces), such that if you pick a vector of random times t_i, and then consider a random vector {x(t_i)}, you find that it is has a multivariate normal distribution. 

> Wait, this part on wiki sounds as if x(t) was univariate, but it kind of doesn't make sense for it to be univariate. Something is off with this definition.

By definition, a multivariate Gaussian is fully defined by a vector of means + covariance matrix. Which makes life easier. The correlation matrix is symmetric and positive semi-definite (_Is it easy to prove?_). The coolest thing about Gaussians is that conditional distributions (lower-dim transects of a multivariate Gaussian) are also Gaussians in their respective dimensions. Moreover, an integral over a few dimensions (aka **marginal distribution**) is also a Gaussian! (_That's probably a direct consequence of the fact that a sum of normals is a normal, as it survives through integration, right?_)

In practice, they are used to estimate the joint distribution of training and testing data P_X,Y so that we could guess test data X from training data Y, under assumption that P_X,Y is a multivariate normal distribution with dim(X)+dim(Y) total dimensions. And so "Guessing X" becomes a task in estimating Bayesian conditional probability P(X|Y). 

> This is confusing. Why would anybody call something you're trying to learn X, and something you know - Y? At first I thought these "distill.pub" people have a typo there, but no, they are consistent, so it's clearly by choice.

"In Gaussian processes we treat each test point as a random variable."

> What? So even for a univariate in->out, and, say, 100 points in the training set, and 50 in the testing, then we have one 150-dimensional vector? And this vector is the _only point_ from the supposedly Gaussian distribution that we have? Why is it even helpful?
> I actually think they have a mistake here, and what they actually mean is that they treat each point as a snapshot of some random variable in time x(t_i). So the entire vector X is assumed to be made of {x(t_i)} where x is a random variable, but if we imagine going from one instantiation of x to another, all points at all t_i would move together. Kinda. What they write later makes more sense if you assume that.

Then what they call a "clever step": we assume a certain behavior of x time, which means that x(t_i) will all be linked ot each other in some fashion. As a Gaussian process is fully defined by its covariate matrix, assuming (or estimating) this covariate matrix locks all points in our dataset to some pattern. This covariate matrix is called a **kernel**. Say, we can assume that each point is similar to the previouis point, and this similarity decreases with time ( kernel = Gaussian(-(Δt)²). Or maybe it's periodic, and the system oscillates. Or maybe it's non-stationary, which gives a linear kernal (???). In all these cases, we usually center and normalize the variables to assume that all means are equal to 0. The kernel apparently comes from "domain knowledge". Essentially, the kernel function sets the smoothness, and other properties of the process.

> So does it mean that our training data really actually has to come from past observations of the system, at equal intevals? Looks like that.

Wikipedia describes certain properties a Gaussian process may have, but does a lousy job defining them:
* Stationarity - whether it's bound in the long-term? Brownian motion for example is apparently non-stationary.
* Homogeneity - whether changes in both directions have same properties (something like temporal symmetry?)
* Smoothness
* Periodicity

Now we introduce training points. They constrain the subset of functions that satisfy the kernel, as they have to pass near  the training points (assuming some additive unexplained noise).

**See also:**
* https://en.wikipedia.org/wiki/Gaussian_process
* http://katbailey.github.io/post/gaussian-processes-for-dummies/
* https://blog.dominodatalab.com/fitting-gaussian-process-models-python/

# An introduction to variable and feature selection
Guyon, I., & Elisseeff, A. (2003). An introduction to variable and feature selection. Journal of machine learning research, 3(Mar), 1157-1182.

 

14000 citations!

Typical for modern (as of 2003) solutions to work with >10^6 variables (genes, words), as opposed to old-school papers that were in the dozens. The problem: build useful features, and also avoid redundancy. 

Nice checklist (summarized in my main chapter)

## Variable ranking

Cites Fisher’s criterion: `var_between / var_within` , but it's unclear what their judgement is, in terms of using or not using it.

Simple: correlate each input var x_i with y; then sort (aka rank) by $r^2$. It may be OK to be generous here, and transform the variables, or use non-linear fits.

Claim that `cor(Y,x_i)` can be easily extended to nominal Y, citing this, but don't give the formula, citing (Furey 2000) instead.

Alternative: use a classifier (set a threshold for both x_i and y, and just see how well you can predict). For threshold for y, use median; for x_i, use the one that brings false-positive and false-negative rates to the same value: FPR=FNR. If many variables allow full separation, either rank by margin, or switch to correlation. _Not clear to me why anyone would use a classifier-based ranking of variables when they can do correlation and look at r squared. It's even computationally harder._

Fancy option: Information criteria. Bin both x_i and y, calculate a standard double integral of $p(x_i,y) \log( p(x_i,y) / (p(x_i)p(y)) )$. How to bin best? They cite "non-parametric Parzen windows" (Torkola 2003)

But the problem (visible even for simple examples of 2 variable) is that sometimes both variables are useless by their own, but allow perfect separation of classes together. So a more interesting topic is selecting a subset of variables!

## Subset selection

Three approaches: 
* **Wrappers**: define goal, try all possible combos, pick the best
* **Filters**: pre-process variables before training, standard and domain-specific tools
* **Embedders**: learn a subset during training

For **Wrappers**, need to 1. Define model performance (usually, validation set, or cross-validation), and 2. Define a strategy for samplinging variable subsets (unless you really do all possible subsets). Alternatively: **greedy algorithms**. Either **forward selection** (add features to the list), or **backward elimination** (exclude noisy features). Downside: not all data is used for training (as validation set is necessary). _Note: apparently, the use of greedy algorithms is hugely controversial, as they are always inferior to other approaches (see )._

**Embedders**: Nested subset methods: define the objective function as J(s) where s is the number of chosen variables. _Then they list some topics I don't understand without a context (and in the paper, they are given without a context):_
* Quadratic approximation of the cost function
* Kernel methods
* Optimum Brain Damage - an old paper by LeCun from 1990 (ref below) where they did something like pruning, but using an approximation for the objective function (is it because they couldn't calculate it directly)

**Direct objective optimization**: Take the goodness of fit, and add punishment for model complexity, then optimize it directly. Similar to shrinking (regularization).

Another weird method: use all variables for an SVM, then rescale all variables by multiplying them by the absolute values of the weight vector. Repeat until convergence. It will shrink weak variables to nothing (Weston 2003). 

But in most cases direct optimization introduces a hyperparameter (the cost of complexity).

**Filters**: faster, as happen entirely before training. May be linear or non-linear (?). Markov blankets (?) and other information-based criteria. _Overall, they seem to cite lots of contemporary papers from the same issue, but probably by now we have better methods, don't we?_

## Feature construction and dim reduction

**Clustering**: may be both unsupervised and supervised (based on examples). Claims that it's linked to **information theory**, as it's about finding a proxy ξ for existing x→y, such that mutual information I(ξ , x) is minimized, while I(ξ , y) is preserved. Can be optimized with a Lagrangian multiplier J = I(ξ,x) - β∙I(ξ,y). _Not sure I can intuit this at this point._

But this definition above apparently opens a window to use methods like **KL divergence** for information-based clustering.

Another method (_Isn't it way more popular now?_) is **matrix factorization**.

**Supervized clustering**. Reference some methods I never heard about (e.g. "Parzen windows"). _Skip for now; it feels both scary and oldschool, which makes it gangerous._

## Validation

Basic info on cross-validation.

Finishes with "Advanced Topics and Open Problems" that I mostly skipped.

## References

Furey, N. Cristianini, Duffy, Bednarski N., Schummer D., M., and D. Haussler. Support vector
machine classification and validation of cancer tissue samples using microarray expression data.
Bioinformatics, 16:906–914, 2000.

LeCun, Y., Denker, J. S., & Solla, S. A. (1990). Optimal brain damage. In Advances in neural information processing systems (pp. 598-605).
http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf
3k citations

Torkkola. Feature extraction by non-parametric mutual information maximization. JMLR, 3:
1415–1438, 2003.

J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for
SVMs. In NIPS 13, 2000.

# Breaking down hierarchies of decision-making in primates

Hyafil, A., & Moreno-Bote, R. (2017). Breaking down hierarchies of decision-making in primates. eLife, 6, e16650.

 

A follow-up on the earlier paper, with a new analysis of old data:
Lorteije, J. A., Zylberberg, A., Ouellette, B. G., De Zeeuw, C. I., Sigman, M., & Roelfsema, P. R. (2015). The formation of hierarchical decisions in the visual cortex. Neuron, 87(6), 1344-1356.
https://www.sciencedirect.com/science/article/pii/S0896627315007096

The gist of it is that they have lots of behavioral data from a 2-state decision process (monkeys took an action consisting of 2 choices one after another, with all information available upfront). They try to figure out whether the decision process was flat (all options considered together, and then a path implemented once), or hierarchical (first large-scalle choice, then smaller-scale choice). In that older paper they said "based on model fit, it's definitely hierarchical". In this new paper they are saying "nah, flat model actually works great as well, so we cannot claim hierarchical"

The most interesting thing about it all is the method they used. It's a nice case of using decision theory, and a curious introduction to models they use.

Experiments: described in Lorteije 2015. They had a binary tree 1>2>4 drawn on the screen, and colors of branches, at branches points, slowly randomly drifted. A monkey needed to move its gaze along the tree, on command, always picking a lighter branch. Which means that it had to take 2 decisions in a row: first bifurcation, and then the 2nd (one in each branch). They looked at psychophysigological curve (probability of choosing a branch as a function of the difference in lightness), and some other stats that I don't understand.

Model: better described in Lorteije 2015, but then here they claim that the old model is actually wrong. Originally they had each target's score = ∫ over time of k(∆luminance) for relevant branches, and then it drifts until one of the scores wins (crosses the threshold). And that's how in 2015 they compared flat (all targets are running against each other) and hierarchical (first one decision, then another one) models. Now they introduce leak (each score leaks, by additing -av to the formula for dv step), replace ∆luminance with abs(∆luminance), and add concurrent inhibition (each v_i inhibits other v_j by being present as bv_i in their dv_j).

And somehow they claim now their flat model fits the data just as well as the hierarchical model used to fit it.

That's all really strange, and I don't understand how one can make these sorts of claims using these sorts of techniques, but it's a curious window into the world of decision-making models :)

# Harnessing nonlinearity

Jaeger, H., & Haas, H. (2004). Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication. Science, 304(5667), 78-80.
https://www.jstor.org/stable/pdf/3836613.pdf

Original communication about reservoir computing (aka  state network). 2000+ citations. A randomly connected network that propagates signals. Some nodes are assigned inputs at random, some nodes are assigned outputs. Then you train one readout layer to produce something meaningful, using the original network randomness as a library of behavioral repertoirs (filters). 

Simple math: essentially regression. 

Good for predicting chaotic processes (for example, see: https://www.quantamagazine.org/machine-learnings-amazing-ability-to-predict-chaos-20180418/ )

# One-shot learning and behavioral eligibility traces in sequential decision making

Lehmann, M. P., Xu, H. A., Liakoni, V., Herzog, M. H., Gerstner, W., & Preuschoff, K. (2019). One-shot learning and behavioral eligibility traces in sequential decision making. eLife, 8.
https://elifesciences.org/articles/47463

  

Claim that they can differentiate between eligibility traces (RL rewards entire sequence) and "classic learning" (aka "Temporal-Difference algorithms") that slowly propagate the reward backwards through the sequence) behaviorally, and that humans do the one-shot thing.

Start with a summary of RL models in humans (refs). Popular modern models contain a hidden memory about past states, aka "eligibility trace" (lots of refs, for both humans and computational models). Multi-step learning with delayed feedback allows a comparison based on qualitative data. Claim that without eligibility traces after 1 shot only the latest step woudl be reinforced, and it would take at least 2 shots to "propagate" (not the word they used) back to earlier steps.

Then test it on something like Markov sequences, except the first one is fake (the person is told that they guessed the sequence, but actually the choices they make define a 2-step sequence). And they make humans learn several (6?) schemas like that at once. They then check how they work in the future, and notice that they remember the 1st step, not only the 2nd one. Both behaviorally, and in terms of pupil dilation. Which is probably a more interesting result. About 20-50 episodes per participant; three types of stimuli (clipart, sounds, spatial), 12-22 participants in RL experiments, 9-12 in control (observe, but no RL). Interestingly, control was only added after reviewers required it :)

Then they claim that they can estimate eligibility trace decay from their data. Then they try different RL algorithms (8 in total): 4 with eligibility trace: Q-λ, Reinforce, 3-step-Q, SARSA-λ, and then 4 without it: Hybrid, Forward Learner, Q-0, SARSA-0, Biased Random. Claim that eligibility trace worked better (log-likelihood fit? I haven't read the methods too closely), and then the λ is around 0.7, 0.8, or 0.96 for diff stimuli. But these are some unpleasant values, related to how the signal decays from one presentation to another. So they transcribed it into time (mapping it to their specific experiment I guess), and got τ (decay in time) or around 10 s, corresponding to 2-3 inter-stimulus intervals. 

Apparently, no changes in control. The sample size was smaller, but the effect size seems much larger in RL session, so it's probably real (they don't report effect sizes tho, only p-values).

In the discussion say that 10 s is a cool value, as it's not that off than dopamine modulated plasticity in the striatum, or HT5/NE plasticity in the cortex (refs). Notably shorter than minutes reported in the hippocampus.

# What does it mean to understand a neural network?

Lillicrap, T. P., & Kording, K. P. (2019). What does it mean to understand a neural network?. arXiv preprint arXiv:1907.06374.
https://arxiv.org/abs/1907.06374

  

Imagine an ANN doing something cool. We can code it, describe it (vectors of params), or look at its elementary operations. But we usually cannot grasp "how" it works.

> My thoughts: But how do we understand how another human acts, or maybe even how we function? Say, you learned to hit a target with a ball, or tell a raven from a crow. What does it mean to understand how you do it? Argually, we only feel that we understand it if we can explain it, so essentially create a "curriculum learning" sequence that, when presented to a different but similar human would teach them how to perform same function. And the neater (shorter), simpler (robust to noise), and clear (effective in teaching) this explanation is, the more we feel that we "get it".
> If translated to an ANN case, to "understand an ANN", rather counter-intuitively, may mean "to provide an efficient distillation paradygm".

They mention "compessability of rules", so again "distillation".

"Neuroscience should focus on understanding development and learning" - wholeheartedly agree!

Human understanding is necessarily based on compactness: as we can only engage with a limited number of statements at any given time, we only feel that we "understand" something when the description at any moment is compact (like a few lines of code for an ANN).

Random fact: Go has more than 10^(10^48) possible games :)

A short summary of current attempts at understanding (with 2-3 refs for each) that mostly don't work:
* Sensitivity of outputs to changes in the system
* Adversarial stimuli (illusions)
* Perturbation by removal of units

> Then, I'd say, a good strategy would be to try "super-distillation": that is, finding a curriculum of a _very small number of stimuli_ that, when paired with a very high learning rate (and maybe an unusual evolution of this learning rate), would bring a network to a target state. Possible study plan: train a network. Reverse-engineer a sequence of stimuli that, when given to this network, with a very high learning rate, would bring it to a trained state as fast as possible. These stimuli won't be realistic obviously, but what would they be? We can try to go forward (from the same random state as the original network), or backwards (from trained to equal everything, subtracting info instead of adding it). If the process if starting state-dependent, we'd also have to check whether it teachers other networks similarly well (say, in case of distillation), and if not, we'll have to make sure it's not idiosyncratic to a network (how? not sure).

Example from neuroscience: people feel that they "understand the brain" if they have a mid-level "lossy model", similar to physics: not just the underlying forces, but also not just a list of outputs (behaviors), but something in-between, even if it's imperfect.

"Instead of asking how the brain works we should, arguably, ask how it learns to work."

### Some curious refs from this paper:
History is full of seemingly impossible things that ended up being possible (4):
Michael Dickinson. Solving the mystery of insect flight. Scientific American, 284(6):48–57, 2001.

Humans know more than a megabyte worth of info about their own language (21):
Francis Mollica and Steven T Piantadosi. Humans store about 1.5 megabytes of information during language acquisition. Royal Society Open Science, 6(3):181393, 2019.

# Learning spatiotemporal signals using a recurrent spiking network that discretizes time
Maes, A., Barahona, M., & Clopath, C. (2019). Learning spatiotemporal signals using a recurrent spiking network that discretizes time. arXiv preprint arXiv:1907.08801.
https://arxiv.org/abs/1907.08801
Final version:
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007606

  


A big problem with spiking networks is how to get from ms time scales to behaviors that take  seconds / minutes. (refs for motor tasks). Introduce sequential and stochastic dynamics as two main modes of operation (refs). Many ML-like approaches are not biologically plausible: either backprop in time, or non-local information.

Two part curriculum training: first activate groups of neurons together at high freq to make them form clusters (cliques? they call it a "temporal backbone"), then activate them sequentially (slowly), making them learn a sequence. Tight connectivity ⇒ reverberation ⇒ change in the order of magnitude for time. They use:

* Random init on an Erdos graph + check (I suspect that manually and by eye; they don't tell it) that the activity is chaotic, rather than oscillatory
* leaky integrate-and-fire neurons with strong adaptation (important to slow recurrent activity down); 
* feedback inhibition (to prevent seizures); 
* mildly asymmetric STDP (no punishment for co-activation, so bidirectional connections are possible - I don't think it's an important detail though);
* weight normalization (L1 norm: sum weights = const)
* a read-out layer with simplified neurons and more Hebbian plasticity

**Eigenvalue analysis** of resulting connectivity matrix. Can learn non-Markov sequences (transversing the same set in different directions), presumably by encoding the same state as several "synonymous" clusters (making it Markov). Can also learn 2 sequences in parallel (again, diff clusters, diff outputs).

**Sensitivity analysis:** larger clusters make slower networks (for them, 40 neurons per cluster = 500 ms for sequential activity period; 10 neurons per cluster = 300 ms period). Still it's a weaker dependency than I would have expected! Moreover, while adaptation constants affect sequential activity period, the dependency is very weak! For adaptation times from 50 to 150 ms, the period stays practically flat. Also show that period replay variability is mostly proportional to period value, which is apparently expected for Markov diffusion (it's also very small: about 1% only).

For plasticity, they use several different low-pass filters (one for spiking, 2 for voltage), all tuned to enable reasonably realistic LTP and LTD, but from their sensitivity analyses, it doesn't actually look like these are too critical for the model to work.

Also learn a bird song, as a demo proof of principle.

In the discussion, a few paragraphs on the evidence for hierarchical learning of sequences.

Used Julia for simulations! Nice!

# French and German babies

Mampe, B., Friederici, A. D., Christophe, A., & Wermke, K. (2009). Newborns' cry melody is shaped by their native language. Current biology, 19(23), 1994-1997.

https://www.earlychildhoodireland.ie/wp-content/uploads/2015/06/Baby-LanguageDev2009_mampe.pdf

 

Analyzed cry of 30 French and 30 German babies from 2 hospitals (Berlin and Paris). Found that German and French newborns have different cry profiles, that apparently reflect typical prosodic patterns of each language. Even though they could only listen to this language from the womb (but also it was pretty much the main thing they were listening to). 

German babies rise (both frequency and volume) early on, then slowly relax. French babies rise (again in both frequency and volume) much more slowly, and then drop quickly. Nice figures. They call it "Cry melody".

# GPT-2 and the nature of intelligence
The Gradient. 25 Jan 2020. Gary Marcus
https://thegradient.pub/gpt2-and-the-nature-of-intelligence/

   

An opinion piece on OpenAI's gpt2 model.

Starts with contrasting two schools of thought:
* Nativist (human mind isn't blank): Plato, Kant, Chomsky, Pinker, Elizabeth Spelke
* Empiricist (mostly learning): Locke

Then claims that GPT-2 may be a test. Can it learn to speak without being given an explicit Chomskian structure of language, from statistics only?

> To me, setting a split like that is an obvious recipe for disaster, just because post-Darwin, there is no longer a philosophical / metaphisical split here, but merely a practical one. Starts with the **peripathetic axiom** by Aristotle (promoted by Aquinas): "**There's nothing in the intellect that was not first in the senses**" ([ref](https://en.wikipedia.org/wiki/Peripatetic_axiom)). At the surface it is obviously untrue, as no animal is born without pre-configured architectures, reflexes, instincts, developmental priors, and what not. But at a deeper level, of course one can argue that no pre-packaged ability to see can develop (evolve) in a species that doesn't use vision, as otherwise it would not have mattered. So in a way it was in the experience, just not in the experience of this individual. Which is interesting and rich in its own way. But it totally shifts the problem. And obviously, makes any comparisons of animal (human) mind and AI rather complicated, as one has to always account both for personal history of learning, and the priors. A difference in the split may be important, or it may be irrelevant, depending on the question!

The author then states that GPT "is the antithesis to almost everything Noam Chomsky has argued about language", as in Chomskian system there's a pre-set tree of "universal grammar" that has pre-defined slots for, say, verbs (actions) and nouns (objectgs), and every sentence fits this tree to some degree, and is both generated by it, and intepreted via it. While GPT is just a bag of correlations.

> Which again shows either a perfect misunderstanding of the situation, or, more likely, a semi-intentional misinterpreatation. If Chomskian paradigms are present in the training dataset, in the sense of implicitly giving structure to every sentence in it, then the model is bound to learn it, as it would be an efficient way to compress (represent) this data. If there is structure, it will be discovered. So while DL models can be used a test for Chomskian hypothesis, it would not about whether (or how) they learns to interpret or generate language, but about how they represent it in their inner layers. Which probably may be directly observed (although it's not easy), and probably can also be tested.

Then he claims that transformers make no commitments to parts of speech.

> This is also questionable, for a yet different reason: the  model, used for GPT, is actually quite forced, in the sense that it is pretty far from a simple stack of dense layers that left to their own devices, developing inner representation. While this entire structure of Keys, Queries, and Values is not pre-set with noun-aspects and verb-aspects, it is clearly designed to seek representations like that. I do not have a good intuition for how much it can be considered Chomskian - both philosophically (to what degree you can be called Chomskian if you expect the universal grammar, but do not make statements about its structure?) and practically (how would a performance change if you encode Chomskian semantics directly). But it doesn't feel like Marcus's statement is as obvious as it seems.

Marcus admits later that "Of course, nothing can literally be a blank slate; true empiricism is a strawman", but quickly dismisses that, and claims that gpt is still "awfully close" to a tabula rasa. _Hmm, no._

Then tests gpt with a bunch of sentences that look like "I grew up in Athens. I speak fluent...", except that the name of the city is changed, and the model always follows with a correct language.

Second hypothesis: that sentences can be represented as vectors, and do not need complex trees. Marcus mostly describes it as Geoffrey Hinton's hypothesis.

> Again, doesn't feel fair. Everything may be encoded in a vector, including a value of any node in a tree, or the structure of a tree itself, It's how this vector is used, in the sence of interacting with the structure of the network, is that sets the non-linear aspects of transofrmations. Ultimately, it's all about non-linear aspects of it, isn't it? Vectors can be added and even averaged; with trees maybe you can add, to some degree, but averaging may quickly become problematic.

But then demonstrates with some good examples, such as counting objects from a narrative, the model fails spectacularly ("I put two trophies on a table, and then add another, the total number is..."). It cannot generalize too well. And then some other examples where one sentence introduces topic 1 with aspects 1 (such as place, or language), then another sentence introduces topic 2 with aspects 2, and a question about topic 1(or rather, seed, that is to be continuied) is answered with some mix of aspect 1 and aspect 2. Like super-priming, where you cannot hold 2 thoughts at the same time, but have them bleed into each other, and intertwine semantically.

> Which is not surprising, given the structure of the model. But this part is good: Marcus identified the weak points of this model really nicely!

Also it fails on syllogisms. Behaves as a Wernike aphasia patient.

Great quote: "This is why GPT-2 is so much better at writing surrealist prose than holding a steady line in nonfiction"

# Understanding the generalization of ‘lottery tickets’ in neural networks

November 25, 2019, Ari Morcos, Yuandong Tian
https://ai.facebook.com/blog/understanding-the-generalization-of-lottery-tickets-in-neural-networks

 

A summary of several of their papers.

**Lottery ticket hypothesis**: one can find a 10-100 times smaller network with same loss, if it's trained from a "lucky" initial configuration. 

It's not about pruning, although related. With pruning, you remove unhelpful weights (up to 99% of them, really??) With lottery ticket, you start with a small network and train it, but have a "lucky" (trainable) original configuration. And it outperforms pruning. How to find it? By trying many starting points ("tickets") and training again and again, which obv requires lots of compute.

Of course these "tickets" would be more helpful if they generalized across data. Now they show that they do actually generalize to some extent (in their case, from one images dataset to another: CIFAR-10 to ImageNet). Tickets from larger datasets generalize better. More image classes (with n images kept constant) also generalized better.

Tickets were first described in image processing, but now they show that they exist in RL and NLP domains as well. Also outpefrom pruning. For NLP, their "ticket" is only 60% smaller than the full network (and the full one is 2e8 parameters), so perhaps not as impressive as for images?

To understand how tickets work, they trained a *larger* network as a student, from a *smaller* network used as a teacher (so the opposite of distillation). Found that neurons in a student network are more correlated with neurons in the smaller network than with each other, meaning that the larger network actually emulates the teacher directly, not just via learning from it. And this *specialization* (neurons in network 2 impersonating neurons from network 1) depends on the original weights to these neurons in the student network (**they should by chance happen to be close to that in the teacher network**). Which makes it sorta a "reverse ticket situation". It appears they also did some theoretical analysis of it.


### Refs:

Morcos, A. S., Yu, H., Paganini, M., & Tian, Y. (2019). One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers. arXiv preprint arXiv:1906.02773.

Frankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635.

Yu, H., Edunov, S., Tian, Y., & Morcos, A. S. (2019). Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP. arXiv preprint arXiv:1906.02768.

Tian, Y., Jiang, T., Gong, Q., & Morcos, A. (2019). Luck Matters: Understanding Training Dynamics of Deep ReLU Networks. arXiv preprint arXiv:1905.13405.

Tian. Student Specialization in Deep ReLU Networks With Finite Width and Input Dimension
https://arxiv.org/abs/1909.13458

# Griffiths phases and the stretching of criticality in brain networks
   

Moretti, P., & Muñoz, M. A. (2013). Griffiths phases and the stretching of criticality in brain networks. Nature communications, 4, 2521.


Introduce **criticality**. As precise boundary is hard, there's a band that it is OK. 

How to tell? avalanches, sensitivity to perturbations (divergence of succeptibility; this paper may be a best link to illustrate this point). 

Insist on inherent **modularity** of biological networks. For their model, iteratively, hierarchically link subnetworks bottom-up. Use topological dimension D (power law); their model produces **fractal networks** that span a large range of D. Intermediate quasi-chaotic behavior arises in hierarchical systems as some modules may remain chaotic when others are ordered. Small world networks are dysfunctional precisely because they don't have this behavior. Everything is global, so death-chaos border in them is very sharp!! 

In their model neurons inactivate with p1 and propagate to connected nodes with p2, which they slide to find a tippint point between death and stable activation. In hierarchical modular networks it's not a point, but a band. 

Claim same effects in C elegans connectome. Then somehow link it to spectral analysis of these networks (not sure how exactly).

# Neocortical plasticity: an unsupervised cake but no free lunch
Eilif B. Muller 

https://openreview.net/pdf?id=S1g_N7FIUS 

 

When training a network to do something good, how can we train it to *also* not do something bad? (They call it a "Monkey's Paw effect" after some horrid story from the 1910). One option could be to explicitly tell the model what is bad, and try to exhaust bad cases (they call it: "inject adversarial examples into learning data stream", and "specifying the negative space of the objective"). But that's bad.

Instead, try to "disentangle the underlying sources of variation" in an unsupervised manner (Bengio 2012), which amounts to "learning a good ".

How can it happen in the cortex? NMDA spikes, clusters of co-coding synapses + cliques of neurons. Inhibition puts a limit the length (and thus spatial propagation) of NMDA spikes, and so defines the size of cliques. It also suppresses activation of competing cliques.

Then quote LeCun, from NIPS 2016 talk apparently:

 “If intelligence is a cake, the bulk of the cake is unsupervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning.” 

# On the Bias-Variance Tradeoff: Textbooks Need an Update
 
https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update

A simple-language explanation to accompany 2 papers from 2019/2020 (to be read :)

Classic U-shaped (or rather √-shaped) error(complexity) plot works for many methods, such KNN and kernel regression with varied kernel size. However not all measures of "modell complexity" lead to it: for DL, an increase in network width drives error down monotonously. 

Claim that (Geman 1992) is self-contradictory (words don't match graphs). Another possible effect: for boosting, variance grows exponentially slower than bias decreases, leading to net monotonous decrease in error rate (has refs). 

To explain this all, one needs to split variance into "variance due to sampling" and "variance due to optimization". It is the second term that defines whether a particular method will produce variance growing higher enough to result in a √-shaped plot. 

Some comments on "double descent", with refs that ∥ this work.


```BibTeX
<pre>@misc{Neal2020bias-variance-tradeoff-textbooks-update,
    author = {Brady Neal},
    title = {On the Bias-Variance Tradeoff: Textbooks Need an Update (Blog Post)},
    day = {05},
    month = {January},
    year = {2020},
    url = {https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update},
    note = {Online; accessed 8-January-2020}
}</pre>
```

# On Markov blankets and hierarchical self-organisation

Palacios, E. R., Razi, A., Parr, T., Kirchhoff, M., & Friston, K. (2019). On Markov blankets and hierarchical self-organisation. Journal of Theoretical Biology, 110089.
https://www.sciencedirect.com/science/article/pii/S0022519319304588

Elsevier, behind paywall, no preprint.

 

Biology = spontaneous pattern formation. Markov blanket = separate of internal and external states. All linked to free energy minimization. 

Some background info from Wikipedia:
* "The free energy principle is that systems defined by their enclosure in a Markov blanket try to minimize the difference between their model of the world and their perception. This difference can be described as "surprise" and is minimized by continuous correction of the world model of the system." ([ref](https://en.wikipedia.org/wiki/Free_energy_principle))
* Ibid: In a 2018 interview, Friston acknowledged that the free energy principle is not properly falsifiable.
* The Markov blanket for a node A in a Bayesian network, denoted by MB(A), is the set of nodes composed of A's parents, A's children, and A's children's other parents. ([ref](https://en.wikipedia.org/wiki/Markov_blanket))
* The objective is to maximize model evidence p(sensory | model) or minimize surprise -log p(sensory | model)

> Wait, but would not curiosity maximize that? Isn't the point of curiosity to seek discrepancies with the model?

"A Markov blanket is a set of states that separates the internal or intrinsic states of a structure from extrinsic or external states."

Introduce a super-clunky term "hierarchal compositions of Markov blankets of Markov blankets" to describe hierarchical "configurations of configurations" - like, really?

Distinction between "Variational free energy" (that they use) and "Thermodynamic free energy" - without definitions, and claim that related anyways.

"Shannon entropy of sensory states is necessarily bounded". System bahaves "as if it is gathering evidence for its own existence" (I have no idea what it means tho)

Introduces some clunky formalism, and claims that to counter-act natural dissipation, live systems perform "gradient ascent on the ergodic density over the internal states and their Markov blanket". Define "Variational energy".

OK, at this point I give up. They follow with some more formulas, then with simulations in which blobs, themselves made of blobs of nodes, apparently emerge from these formulas. I'm not sure what it means and how it helps. At the very least, this needs to be restated in a more straightfoward way (Bayesian? Information theory? Representations and loss?) Otherwise it's completely unrelatable.

# Students, computers, and learning
Programme for International Student Assessment (PISA), tudents, computers, and learning
http://www.oecd.org/education/students-computers-and-learning-9789264239555-en.htm



Early access to computers (before age 6) differs a lot by country, from ~50% for Nordic countries and Israel, through ~30% for most of Europe, and down to ~10-20% for Middle East and Japan. Girls always lag by about 10%, with some fun exceptions (Japan, China, New Zealand - except that Japan is actually exceptionally low, while NZ is pretty high)

Students who report feeling lonely use Internet more (well, duh).

Most computer-oriented school systems: Nordic, NL, Czechia. Least: China, Japan, Korea.

**Fig 6.3**: Claim that in terms of math performance, the more computers are used in school, the lower is the _change_ in math PISA scores from 2003 to 2012. So from this POV, NZ, AU, Finland, Czechia - all kinda tanked, and all use computers a lot, while Turkey, Mexico, and Brasil improved a lot. Doesn't seem like an interesting causal statement at all to me. Like, if computers mean "rich for a while now", and rich mean decent PISA scores in the past, you'd expect something like that, no?

**Fig 6.7** At the same time, they also have some analysis that is averaged across countries, but split by student performance (is 1 data point before average = 1 school here? Not sure, but probably not "1 student"). Find that **students who don't use computers in math classes score better in math**. Not by a lot though: about 2% of the total score.

[This article](https://americanaffairsjournal.org/2019/08/rotten-stem-how-technology-corrupts-education/) (soft paywall) shows a bar chart that they claim came from PISA, but that I cannot find on the PISA site, with a title "Give students a laptop to knock off half a letter grade". Show consistent detrimental effect of computer technology on PISA scores, with largest effects on reading, smaller on science, and yet smaller on math. Some effects are tiny (wifi ~ 2% worse, desktop computer ~3% worse), some are huge (tablet ~10%, e-book readers ~15% worse). The only tech that helps are projectors (10-15% boost). The article claims that it's from "540,000 students in 72 countries". 

I cannot find the data though; the article doesn't reference it, and the exact wording from the figure cannot be found on the PISA site. Also, the x-axis says "Change in PISA scores", which I now strongly suspect may actually be "a change between one year and anothe year" (as reported above), which woudl mean that the plot is questionable, and the % reported are actually not the attribuable effects, but literally, changes that happened in schools from different "privilege strata", so to say.

So if my guess about the origin of this data is right, it wold be wrong to claim that giving ebooks to students is predicted to drop their scores by 10%. What's fair to say, is that rich schools that happened invest in ebooks (arguably, a particularly useless piece of tech, not used by most schoos) also tend to worsen their math scores in the long-term (over course of 10 years), - probably (or at least that would be my buess) because they don't invest in more meaningful stuff (teachers, innovative pedagogy etc.). At least my hypothesis would be that it speaks more about school management strategies, than about classroom effects of tech.

# Coverage and other references
* https://americanaffairsjournal.org/2019/08/rotten-stem-how-technology-corrupts-education/
* Replication in West Point students, who apparently score low if allowed computers: Susan Carter, Kyle Greenberg, and Michael Walker, “The Impact of Computer Usage on Academic Performance: Evidence from a Randomized Trial at the United States Military Academy,” SEII Discussion Paper no. 2016.02, May 2016.

# Porter 2019 Nonlinearity plus networks

Mason A Porter
https://arxiv.org/abs/1911.03805 

  

Subjective review (rather short, but 201 references!). By "networks" they really mean stochastic graphs, and not ANNs. One para intro to basics (degrees etc.). Temporal network: changes over time. 

Intro to  representation, but not very clear. (What are those "aspects"? What does this figure show?) For time representation, each time point = one layer; nodes are typically connected between time-layers with undirected edges, but they claim that it may be more productive to use forward-facing directed edges, not necessarily between adjacent layers. This creates a "supra-adjacency matrix" $A_m$.

Introduces  measures that were generalized to temporal networks: Bonacich, Katz, communicability, pagerank, and eigenvector. Eigenvector-based centralities are easy to calculate, and diff matrices produce diff centralities (hubs, authorities etc.) References @Perron–Frobenius theorem. A bunch of nice links about stochastic block models, detection of communities (including their split / merge).

**Activity-driven models**: a name for when the network changes due to processes on it. References the "original model of Perra et al":  N. Perra, B. Gonçalves, R. Pastor-Satorras, and A. Vespignani. Activity driven modeling of time varying networks. Sci. Rep., 2:469, 2012. 

Continuous-time can probably be skipped.

**Dynamical processes on networks**: refs for coupled oscillators, games, spread of diseases and opinions. Notably doesn't even mention spiking networks!

Then moves to stochastic processes.

"reaction–diffusion equations and Turing patterns on networks" and "rich theory of Laplacian dynamics on graphs (and concomitant ideas from spectral graph theory)" - four links here (all harvested)

Finishes with brief overview of how simplicial complexes relate to that.




# A deep learning framework for neuroscience

Richards, B. A., Lillicrap, T. P., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen, A., ... & Gillon, C. J. (2019). A deep learning framework for neuroscience. Nature neuroscience, 22(11), 1761-1770.
[Direct link shared by Blake](https://www.nature.com/articles/s41593-019-0520-2.epdf?shared_access_token=n1zyUZ6-ypeHWkeaEs1FPNRgN0jAjWel9jnR3ZoTv0N5dsTXXcjpcGP7i54eL_L9GTMgy1V6NUDPE4-SxE_8Ip1gIa5G35VU4LeqRZ56IGy5uMJKd6aUZ4JeYonqPfWkstTCNFgazGPl8xJGrQAvuw%3D%3D)

   

That impactful opinion piece with 20 or something coolest comp neuroscientists. Overall, a great review to cite every time you claim that "bottom-up then compare" is a more productive approach for reverse engineering complex systems than going "top-down".

Success stories for "few neurons recordings": CPGs, VOR, motion in the retina. With many neurons, this doesn't work, but maybe ANNs will help.

For ANNs, 3 components that aren't learned:
* Objective function (or loss)
* Learning rules (update rule)
* Architecture

A one-link-for-each review of what deep learning can do. Followed by a list of what brain things were replicated with deep learning: grid cells, shape tuning, temporal receptive fields, visual illusions, apparent model-based reasoning.

In the blue box: a summary of backprop, and ~10 links to various potential solutions for **credit assignment** that estimate gradient, but are more biologically plausible. Fig 2 roughly classifies all these methods on their Bias / Variance performance (most for now have high variance rather than high bias). If interested in credit assignment, this part should be harvested for links.

"No free lunch theorems": different problems require different learning rules. "AI set": a hypothesis that those things that animals do well, should be "easy" for neural networks. However animals have strong inductive biases (see Zador's recent opinion), and similarly humans have to fine-tune ANN architectures for the task. And still this is better than tuning neurons by hand, as everything boils down to "3 components" outlined above, even tho it means no low-level interpretability. Compare brain development to evolution: we know the rules, but not the result.

How to reverse-engineer objective functions for the brain? Sometimes from common sense (e.g. predictive coding = optimize description length). Box with more examples: log-probability of action sequences scaled by the reward they have produced; increase of mutual information with the environment ("empowerment" - apparently a term!) Also a quick plug for "learning how to learn". Ultimately, we need behavior / ethology to guess the objective functions (*they mention brain-machine interfaces, but virtual environments Engert-style would actually be kinda more in spirit*)

How to use this approach? Create models that solve the task; make predictions; compare with the brain (*my paper totes fits the bill*). Link plasticity rules to representations, and the other way around (ref 77: Lim2005).

Interesting point: most past comp neuro work was done on brain dynamics, and this new proposal doesn't quite interact with that past work. 

> They sorta brush this last statement off, but maybe that's just because we (humans) know and like DL right now, so DL becomes a good metaphor that we try to apply to the brain, even though the brain is mostly recurrent. Maybe when (if) we understand recurrent networks better, and depart from the DL paradigm, dynamics would be easily plugged into this framework as well? As yet another readout, similar to connectivity, that can be rebuilt bottom-up, then verified?

# Neural representation of newly instructed rule identities during early implementation trials
Ruge, H., Schäfer, T. A., Zwosta, K., Mohr, H., & Wolfensteller, U. (2019). Neural representation of newly instructed rule identities during early implementation trials. eLife, 8.
https://elifesciences.org/articles/48293

 

Made humans follow arbitrary rules (images→ finger moves, or tones→ finger moves, of different complexity) while in fMRI. Measured voxel activation, applied multivariate pattern analysis technique (MVPA) for representation dynamics (cite their own paper from 2018, and some older, going back to 2001). Claim that it can do some fancy "time-resolved identity-specific pattern correlations within runs".

Basically this MVPA works like that: measure voxels, consider them a vector, calculate corr() between responses to the same stimulus, and between responses to different ones. The measure (Identity-specific multi-voxel pattern similarity) = corr(same) - corr(different). 

Show that PFC is max involved during first time presentation, then declines. For more complex tasks, PFC is involved for longer.

# Video Architecture Search

Michael S. Ryoo, Google, 2019
https://ai.googleblog.com/2019/10/video-architecture-search.html

   

A mini-review of 3 papers from the same group.

Videos are hard to process, as they combine "what" (appearance) with motion. Most architectures either try to expand image-based 2D architectures to 3d, or to create 2 information flows that are then manually fused.

Instead, they try three architecture evolution algorithms (links to arxiv papers)
* [EvaNet](https://arxiv.org/abs/1811.10636)
* [AssembleNet](https://arxiv.org/abs/1905.13209)
* [TinyVideoNet](https://arxiv.org/abs/1910.06961)

One nice side-product of using an evolutionary search is that you end up with several different architectures of similar performance, that can all be trained, run in parallel, and made into an ensemble.

**AssembleNet**: their goal was to discover something like multisensory integration (or at least multi-stream integration), and they developed graphical representation of architecture with this goal in mind. Also some innovation in mutation?

**Tiny Video Networks**: the goal was to find networks that can run realtime, by including runtime (total computer) in the objective function. 

> " Interestingly the learned model architectures have fewer convolutional layers than typical video architectures: Tiny Video Networks prefers lightweight elements, such as 2D pooling, gating layers, and squeeze-and-excitation layers."

# Deep Learning: Our Miraculous Year 1990-1991
Jürgen Schmidhuber
http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html



Sort of a memoir blog post?

Claim that in this one academic year they actually published a first:
* Unsupervised pre-training before supervised training
* Distillation (that they called "collapsing" or "compressing")
* Sequential attention
* Hierarchical reinforcement learning
* "Fast weights": a network that outputs weights for another network, providing a "good guess" for it

The bizarre thing apparently is that all of these approach were later reinvented, without citations to his work. Huh. A beef with Geoff Hinton? Or not?

Related: Alexey Ivakhnenko apparently invented backprop in 1965, but is almost never cited ([another ref by Jürgen](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html)).

Apparently he also proposed GANs in 1992 in some way, which led to a strange "long question" in 2016 [Neurips presentation by Ian Goodfellow](https://www.youtube.com/watch?v=HGYYEUSm-0Q&t=3779s). Except he called it "predictability minimization", and apparently Ian addresses it in his (original?) GAN paper.

Schmidthuber also  wrote a review of that whole GAN history in 2019:
https://arxiv.org/pdf/1906.04493.pdf

#  Taking aim at free will
Smith, K. (2011). Neuroscience vs philosophy: Taking aim at free will. Nature News, 477(7362), 23-25.
(free from Researchgate)

  

Non a research paper; more of a news piece with some interviews. Deascribes neuroscientist **John-Dylan Haynes**, who studied people in a fMRI scanner. Show them random letter, ask them to press either of 2 buttons (spontaneously), and remember the letter that happened to be shown on the screen at this moment. 

They found that they can predict (with only 60% accuracy) the decision (which finger would be used) 7 seconds before the act is actually made. Apparently, before the participants are consciously aware of the decision (_how do they know?_).

> Check the original article for details. It's kinda hard to argue about this experiment without these details.

From this they have a complete personal existential breakdown, as somehow they feel that if you make a decision before you feel that you made a decision, then you didn't make a decision. Which is something I personally cannot relate at all.

History of similar experiments: **Benjamin Libet** (1980s), based on EEG, but with similar results (prediction).

Another related piece of research **Itzhak Fried**, electrodes implanted in humans before epilepsy surgery; activity of individual neurons predict decision to press a button about 1s before the action. His take on it is actually much more sane: that the decision is admitted into conscionsness (awareness) at some point later in the game. Probably some time before the action, but not even necessarily. Makes a good built-in counterpoint if this piece is assigned as a reading.

Then the author of the article shifts  philosophers, including those funded by the John Templeton Foundation. The panel: Walter Glannon, Adina Roskies, Al Mele. Two main definitions of free will philosophically (according to Glannon):
1. Ability to make a rational decision
2. Ability, given a history of the entire Universe, reach different devisions

My take: so, after all, free will is a 2D notion that always has some superposition of computation and spontaneity. Depending on whom you ask, the contribution of these two notions, to make a "perfect free will" is different, from panpsychists, who accept "no computation, full spontaneity" as a case of free will (like in this case a candle or a radioactive atom has free will), to some sort of human exceptionalist, maybe sualists (in which case most animals don't have free will, but somehow humans have, at least when they are enlightned or something. I'm not sure they are actually saying that, but I'm imagining they might be saying that :)

And also on top of that a bunch of philosophers make contradictory claims, saying that various concepts involved in this entire discussion either don't exist, or are  irrelevant. Some invoke deterministic universe, taking it as a synonym for "materialist", which is of course non-sensical (because chaos).

**Michael Gazzaniga** brings up parallelism (good), and that there's no point in thinking about the brain as a linear stream of events. It makes the time when the decision enters the awareness much less relevant. I wonder if he's right, and that's one of the most important insights some people don't get. It would be both optimistic and sad if it's true, as it is so simple!

Thought experiment about stretching both the scope of the decision (committing a crime instead of pressing a button), and time span (say, days instead of seconds). If we could predict at these scopes, would free will still exist? (a quote from Al Mele) The problem of course is that while as a thought experiment, it seems to be just a quantitative change in numbers without a qualitative change in the problem itself, in practice it is quite qualitative, because of the Lyapunov time for the brain (I hope I used this term correctly here!). The typical time at which the brain is chaotic, which is definitely not days.

 **Kathleen Vohs** published this famous paper about how reading about determinism (absense of free will) makes it more likely that people would cheat on a test. _Was it replicated?_
 
 Other people involved in similar projects and their interpretation: Michael Shadlen, Christof Koch, Patrick Haggard, Nicholas Mackintosh, Owen Jones.

# References
* Soon, C. S., Brass, M., Heinze, H. J., & Haynes, J. D. (2008). Unconscious determinants of free decisions in the human brain. Nature neuroscience, 11(5), 543. - the main paper described here, that started the conversation
* Vohs, K. D., & Schooler, J. W. (2008). The value of believing in free will: Encouraging a belief in determinism increases cheating. Psychological science, 19(1), 49-54.

# Attention is all you need


Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008).
(Google Research and Google Brain)
https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf

The paper introducing **Transformers**

> **Note:** this summary below is realy rough, was not polished, and was not reworked. For a better explanation of transformers, see the main  entry. More info, asides, comments, detective work, etc.

5k citations as of 2020, so seminal. I find it quite dry: it doesn't really try to explain the philosophy behind the model (what makes it work), neither does it build the rationale for the model. It just describes the architecture rather systematically, but leaves all conceptual work to the reader.

Interestingly, authors claim "equal contribution with random listing order". I kinda like it.

The paper that took RNNs + attention, and got rid of the RNN part (thus the name). On a language task (en→de, en→fr), show improvement + easier parallelization. Training took 4 days on 8 GPUs, which was apparently a ridiculous improvement.

They didn't "invent" self-attention, and give 4 references for what it is.

What they claim they were the first to do, is to use attention only for sequential tasks, without RNNs or sequence-aligned convolution. It seems that they mean "convolution that is like unrolled RNN, that runs _within_ each bag", as opposed to bag-like approach that handles each bags in the same way (like convolution), and somehow encodes word position (like convolution), but has to encode word position explicitly.



**Architecture:** Auto-regressive model: input + existing output → new output. Inside, input: symbols → continuous representation {z} → encoder → decoder → output. 

**Encoder** has 6 identical layers, each made of 2 sublayers: self-attention, followed by dense. Output_of_a_layer(x) = Normalize(x + sublayer(x)), aka "residual connections". All layers and embeddings match in dimension (512).

**Decoder** also has 6 layers, each of 3 sublayers: 2 same as in the encoder, the third one with "multi-head". Also residual connections.

**Attention**:  to a vector of an output, from these 3 vectors (in practice, packed into matrices, for parallel processing):
* Query
* Key
* Value

Attention = softmax(QKᵀ / √dim)∙V . Then multihead attention just concats h different parallel alternative "attentions" together (they used h=8), and each attention had lower dimensionality (512/8=64).

For decoders, used "masked attention", to manually make sure future doesn't affect the past.

For positions, used a weird overtone series (they don't call it that way): sin and cos at geometrically increasing frequencies!

> The original Transformers paper says: "The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed." I really don't know what it means. And for sin / cos, it says "We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos." Also no idea.

# Inception in visual cortex: in vivo-silico loops reveal most exciting images

Walker, E. Y., Sinz, F. H., Froudarakis, E., Fahey, P. G., Muhammad, T., Ecker, A. S., ... & Tolias, A. S. (2018). . *bioRxiv*, 506956. 
https://www.biorxiv.org/content/biorxiv/early/2018/12/28/506956.full.pdf 
 


Show images to an animal, while chronically recording from the cortex. Train ANN to produce cortical recordings from these images. Then use this ANN to synthesize "most exciting images" for various neurons. Test these images, show that they work better than "linear RFs".

" Each network consisted of a convolutional core computing nonlinear features from the image, a readout predicting the neuronal responses from the core, a shifter network accounting for eye movements, and a modulator network predicting an adaptive gain for each neuron based on behavioral variables " - ???

The model is described in the methods, but I skipped it for now.

# The 3 Tricks That Made AlphaGo Zero Work
by Seth Weidman, January 1st 2018
https://hackernoon.com/the-3-tricks-that-made-alphago-zero-work-f3d47b6686ef
 

A summary of how **AlphaGo** works

Actually surprisingly similar to how humans supposedly play: a combination of algorithmic lookahead (a tree of possible decisions) and ANN-based "untuition" (two, actually). They showed that this combo is allways better than straight network.

**Inputs**: current position + whose turn + last few turns (to avoid illegal moves by repetition). 

**Outputs**: 1) which moves to make (aka **policy**), 2) how likely is the current player to win (aka **value**)

For AlphaGo they trained policy on human games (30 million moves), but for AlphaGo Zero they did everything through self-play.

This second output (value) allowed algorythmic exploration (Monte Carlo Tree Search) using ANN assessment of each game state as a heuristics. So for every position, it new now only its ANN estimation of how good it is, but also whether it is likely to lead to even better positions.

**Architecture**: 
* 20 layers
* **two heads** to produce two outputs (policy and value). Trained simultaneously (body + head 1, then body + head 2), so othey shared a representation, but produced different outputs. 
* Layers are **residual**: it's like convolutional (mostly 3x3 by 128 deep, then some 3x3x256, finished with 3x3x512). Residual are strange: x→ W1→ relu()→ W2→ +x (the original x, added back at the end, for real). See He 2015.
* Average pool at the very end.

**Training**: 
1. Alternate between self-play (on current level), and training. 
2. Self-play: 1600 explorations into the future (MCTS) per turn. 
3. Each cycle: 500 000 games, 2048 randomly chosen positions + their assessments at the moment based on MCTS + whether the game was actually won. 
4. Use it for training. 
5. Once the trained network beats the previousn network in 55% of cases, switch to it for traiing.

**References**:
On multi-headed (multitask) learning:
https://ruder.io/multi-task/index.html#introduction

On residual networks:
Deep Residual Learning for Image Recognition, 2015, from Microsoft Research
https://arxiv.org/pdf/1512.03385.pdf

# Self-Supervised Representation Learning

Nov 10, 2019 by Lilian Weng 
https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html
 
  

Awesome informal review of the entire concept of self-supervized learning, with highlights from the field.

#### General idea:

Come up with a useless supervised task derived from data (use any part of data as a label, predict the label from the rest). By doing that, the model will have to learn rich representations. Then use intermediate level (representation) as an input to actual your actual model.

#### Examples:

* Take a small image, distort and transform it in various ways, teach the model to know that all these images still belong to the same class:  [Dosovitskiy et al., 2015](https://arxiv.org/abs/1406.6909)
* Rotate images, let the model guess down direction:  [Gidaris et al. 2018](https://arxiv.org/abs/1803.07728) 
* Get a bunch of patches, let it guess their relative position:  [Doersch et al. (2015)](https://arxiv.org/abs/1505.05192) Comes with pathologies: obvious straight lines, chromatic aberration (!!!) towards the sides of each photo. To kill first, distort and jitter; for the second - drop colors, or at least shift towards BW.
* Similar idea: reshuffle patches, let the model guess the original order:   [Noroozi & Favaro (2016)](https://arxiv.org/abs/1603.09246) - also use some permutation-invariant graph network that I don't get for now.
* Colorization:  [Zhang et al. 2016](https://arxiv.org/abs/1603.08511) Some interesting rebalancing of the loss function to make sure the model cares about rare but important colors, as opposed to just the sky and the leaves. Not sure how it works (read?)
* Predict one color channel from two others:  [Zhang et al., 2017](https://arxiv.org/abs/1611.09842) 
* Remove (previously added) noise:  [Vincent, et al, 2008](https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf) 
* Reconstruct a patch in the middle:  [Pathak, et al., 2016](https://arxiv.org/abs/1604.07379) 
* Bidirectional GAN: generate data from internal features, but also generate features from actual data. Let the discriminator guess which pair (features, data) is which (one has true data (image), another one doesn't). Thus, at the end, we'll have a bidirectional projection:  [Donahue, et al, 2017](https://arxiv.org/abs/1605.09782)  (I'm not sure how it helps exactly if encoder and decoder don't inform each other explicitly. 
* Tracking objects: [Wang & Gupta, 2015](https://arxiv.org/abs/1505.00687)  - I didn't understand this one. One interesting thing that I got (but that's for supervised learning) is the 
* Guess frame order:  [Misra, et al 2016](https://arxiv.org/abs/1603.08561) , one sequence of out many that has frames shuffled:   [Fernando et al. 2017](https://arxiv.org/abs/1611.06646) , or correct time direction:  [Wei et al., 2018](https://www.robots.ox.ac.uk/~vgg/publications/2018/Wei18/wei18.pdf) 
* Colorize a video from a control frame:  [Vondrick et al. (2018)](https://arxiv.org/abs/1806.09594)  - sounds like something in-between image colorization and motion tracking, huh?
* Let the robot move an object, and while doing so, learn that all camera views during the grasp encode the same object. Even though positions and views are all different. Did I get it right?  [Jang et al., 2018](https://arxiv.org/abs/1811.06964) 
* Teach that views from multiple cameras are still views of the same thing:   [Sermanet, et al. 2018](https://arxiv.org/abs/1704.06888) 
* Imagined goals: [Nair et al., 2018](https://arxiv.org/abs/1807.04742)  - didn't understand this one; something very robotics

# Randomly wired networks for image recognition
Xie, S., Kirillov, A., Girshick, R., & He, K. (2019). Exploring randomly wired neural networks for image recognition. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1284-1293).
https://arxiv.org/pdf/1904.01569v2.pdf
Facebook AI research



Instead of training a pre-design, they explore random connectivities. They succeed in finding connectivity patterns that have competitive accuracy on ImageNet recognition.

But these guys **train weights** after generating the networks. Unlike these guys: 

They built a **parameterized stochastic generator** g(θ,s), where s is a random generator seed. A good combo of randomness and reproducibility. Networks were generated as DAGs, guided by several priors:
* Network is split into "cells" - each a randomized subgraph
* Each cell received all inputs (???) from 2 previous cells
* Among other things (?) each cell contained 5 nodes connected _only_ to 2 nodes (???). Which nodes - was defined by LSTM during network generation (???)
* Once everything was rando-generated, all nodes within each cell without an output were forced to output to an extra "output" node

Interesting connections with NAS (Neural Architecture Search), which is LSTM-based, and optimized by RL.

> At this point I stopped reading, for now. 

# Other fun highlights

* Some nice links on architecture search in the beginning.
* A gret figure (Fig 4) with a bunch of rando-architectures. Looks like some kids book from the 70s, about yarn. Potentially a great pic for a "confusing slide" :)

# Limitations of Deep Learning for Vision, and How We Might Fix Them

Alan L. Yuille, Chenxi Liu. 2019. The Gradient
https://thegradient.pub/the-limitations-of-visual-deep-learning-and-how-we-might-fix-them/



Three waves of deep learning: 1950-60, 80-90, and now (2000-on). Most ideas come from the 90s, but GPUs made all the difference. The 2nd wave went down due to success of support vector machines and kernel methods. But in 2011 AlexNet demolished all competitors in image recognition, and neural nets are the SOTA since. 

Main limitations:
* Very (arguably, unreasonably) data-hungry
* Tend to exploit context (e.g. learn from traces of image origin)
* Biased towards "rare events" and unusual representations even for well-known objects (like, a slightly unsual angle of view can completely throw a deep net off) _- this sounds as the other side of context exploitation: deep nets tend to build "wrong" generalizations, paying attention to semantically "wrong" cues. Lack ability to generalize and transfer properly, which is probably due to the lack of unsupervised statistical "knowledge" about the world._
* Succeptible to adversarial attacks (photoshop a guitar on a monkey; no longer identified as a monkey) _- This sounds like an inability to detect a radical change in data distribution that humans somehow manage to deal with._
* Don't deal with combinatorial explosion well (_and they imply, that is why combining two very different objects is so effective as an adversarial attack - something I'm not sure I agree with_)

Predict that DL is great for tasks that don't have a risk of combinatorial explosion, like medical imaging for example. But bad for IRL tasks, like autonomous driving. Or **IQ tests**: apparently, DL cannot do them! Fun!

### How to overcome combinatorial explosion?

**Compositionality**: basically, a built-in reductionism. The idea that novel stuff can be understood hierarchically, as a sum of its parts.

**Causality**: posit as anotherkey feature that is necessary. _I'm actually not sure about it; or maybe I can agree if causality is understood as another type of relationship, but it doesn't feel like explicit predictive power in a deconstructed space is critical for everyday thinking of humans. It feels a bit too science-like, rationality, which is important for progress, but not for everyday actions. It's not like we rely on processing unexpected what-ifs on a daily basis, are we? _

### References:

* Original AlexNet papers: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. "Imagenet classification with deep convolutional neural networks." Advances in neural information processing systems. 2012.
* and: Deng, Jia, et al. "Imagenet: A large-scale hierarchical image database." Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. Ieee, 2009.
* Monkey with guitar: Wang, Jianyu, et al. "Visual concepts and compositional voting." Annals of Mathematical Sciences and Applications 3.1 (2018): 151-188.
* IQ: Barrett, David, et al. "Measuring abstract reasoning in neural networks." International Conference on Machine Learning. 2018.

# "Cheap moves" by Beau Sievers
https://twitter.com/beausievers/status/1218260165872406528


A nice tweet-thread about "unfair" questions in a discussion, analogous to cheap moves in board games, that almost feel like a form of bullying against new players, even though they  are would not pose that much of a problem to more experienced players.

* (to a psychologist): "The measurement of people reduces them to a list of numbers, doing violence to the complexity of their experience and the depth of their being."
* (to a neuroscientist): "It's unsurprising that this process is implemented in the brain, how could it be any other way? What does your work tell us about the purely psychological content of the process?"
* (to a composer): "You presented this work as a critique of colonialism, but because you are in a position of power, its actual function is to occupy space that could otherwise be used to center the work of the oppressed."
* (to a critical theorist): "This critical approach is self-undermining—the same method can be used to show that any application of it is itself contingent and ungrounded."
* (to an educator): "Your intervention cannot yet be applied system-wide, which means it cannot be implemented equitably."
* (to me, in about 45 minutes): “Interesting how you analogize interdisciplinary research to abstract strategy games, when in fact research is not a game, and people’s livelihoods are at stake.”

The problem is not that these questions are wrong; it's that they are unproductive, and effectively shut down people who are not too well versed in interdisciplinary dialogue. So people get offended, personal, and derail the conversation.

Used by the author as an argument for distant interdisciplinary collaborations :)

# I’m a Developer. I Won’t Teach My Kids to Code, and Neither Should You
By Joe Morgan


https://getpocket.com/explore/item/i-m-a-developer-i-won-t-teach-my-kids-to-code-and-neither-should-you

An example of a tragic take on CS education, that may sound reasonable at the surface, but actually doesn't make much sense. The author claims that one shouldn't teach kids to program, as syntax doesn't matter. What matters is problem-solving, and it is easier to teach in the real world (his example: fixing a chair, and making cookies).

What he totally misses, of course, is that in programming one doesn't engage with the real world, nor even artificial objects in the real world, but with logical constructs created by other people. Constructs that may be brilliant, or terrible, or (more often) - a mixture of both. Therefore, at least at the lower, primitive level, programming is mostly about hacking - about solving puzzles created by other people, that are known to have a solution. And you just need to figure it out.

Which is very different from "Real world" problems that typically don't have a solution, or at least you are never sure whether they do, and in many cases, what even counts as a solution.

And then of course there are the concepts of state, copying, information exchange between entities, following instructions, etc. that one has to internalize before they can even attempt to program.

To sum up, probably this can be considered a yet another case when a smart person, knowledgeable in one are, totally lacks awareness of their naivitet  in another, adjacent area (in this case - education). And so they end up making all sorts of wrong statements, but with a conviction. And in this case, the writing is also quite good, which is rather unfortunate, really.

# Some thoughts on teaching math in K12
https://www.theglobeandmail.com/opinion/article-the-right-formula-why-math-is-the-key-to-a-more-equitable-society/



Key points:
* **Structured inqury** is good. The "inquiry" part is about not falling to memorization, as it is useless
* But need also be "structured". Cognitive overload and lack of scaffolding also ruin things
* Another good thing: when math is not presented in a stereoptypical academic content (academic hierarchy), as kids self-identify like crazy, as either geeky or non-mathy
* Growth mindset

This makes me thing that the term **growth mindset** is actually used in two rather different ways. One, and that's the one that doesn't work, is this very specific priming idea in psychology, in which you're trying to subtly brainwash a person into believing in growth, and then they supposedly learn better. Except that they don't, as it didn't replicate (*did it?*). Another one, like used here, is rather about tricking people into not shutting down by self-categorization.

# Take on self-education in AI
Emil Warner, 20 Jan 2020
https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/



Interesting school in Paris: Ecole42 (or just 42): a peer-to-peer school without teachers.

His estimation of time it takes to become an independent ML practitioner: 

* FastAI (3 mo)
* Personalized projects / reproduce papers (3-12 mo)
* Flashcard the Deep Learning Book (4-6 mo)
* Flashcard ~100 papers in a niche (2m) _So that's roughly 2 papers a day, right? That's hard!_
* Publish first paper (6 mo)

So 2-3 years in total. Hmm.

Interesting tool: http://www.arxiv-sanity.com/

Good advice: **for personal projects, aim at compute time <10 minutes**. Main reason: you cannot contribute productively if you don't optimize hyperparameters, and with longer compute you won't be able to do a **hyperparameter sweep**. Which means that you'll stuck to suboptimal ad-hoc choices.

Endorses "The measure of Intelligence" by Francois Chollet.

A fun list of "alternative" (small-computer) areas of research, and potential sources for ideas.

Also a long discussion of how to **reform hiring process**, making it less official-education-dependent, and more personal-project dependent, which I kind of like, but not a greatest fan of.  Soudns mostly reasonable, but not ideal.


# Optimizing sample sizes in A/B testing
by Chris Said, Jan 2020
https://chris-said.io/2020/01/10/optimizing-sample-sizes-in-ab-testing-part-I/

  

He estimated (both modeled and derived, I think) the cost of running A/B testing for a certain amount of time. What power should you aim at? (Hint: not necessarily 80%; it's just another magic number).

Essentially, as time goes on, sample size increases, and the chances of making a mistake and picking a wrong option decrease, what it seems, exponentially. (Probably not at all exponential, but it kinda looks like one). The benefits grow real fast, as you are more and more likely to pick a correct answer as your sample size increases.

At the same time, running 2 options at the same time is costly, so instead of saturation at fixed benefit we the have a slow linear decline. Which means that there is actually an optimal time of running an A/B testing.

> I'm guessing, in practice one can either google it, or model it. At least for a given effect size it is fairly straightforward to model. With an unknown effect size, I'm less sure, but it would be a fun model to play with.

Practical consequences, and somewhat unintuitive advice for when underpowered experiments are best, and high power is not actually desired:
1. If the costs of testing is high (he calles it "high discount rate"). That's clear.
2. When the user base is small, the duration curve will be about the same, but at optimal duration, the power will still be fairly small. Just because, again, fixed costs of running a continuous test may easily overshadow the benefit of making a correct decision.

But that said, running an experiment for too long by mistake is almost always better than cutting it too short, as the decrease is slow and gradual, while the first kinda-exponential improvement is pretty steep.

# Small hyper-productive groups
https://twitter.com/david_perell/status/1221503770783354880

 

Examples: 
* Inklings
* Bloomsbury group (English writers, early 20 century)
* Clapham Sect (group of Church of England social reformers)
* Skunkworks project (Lockheed Martin, WW2)
* Junto Club (Benjamin Franklin, mutual improvement establishment)
* Tuusula Artist Community (Finland)

Core principles:
* Small group in one location, regular meeting, constant back-and-forth
* Don't accept new members easily, concentrate interactions within the group
* Autonomy
* The number of people working on each project viciously restricted. Avoid dilution of responsiblity
* Don't talk to the outside world before the project is done. Avoid dilution of what constitutes a product
* Good internal documentation, but minimal bureaucracy, minimal formal reporting

See for example: Skunkworth rules of innovation (googlable)

Related books, articles: 
* Where Good Ideas Come From, by Steven Johnson
* Collaborative Circles, Farrell

# Dennet's 4 creatures

 

Tower of Generate-and-Test, by Dennet (1995)

Four types of creatures
* Darwinian - innate behavior
* Skinnerian - learn it through association
* Popperian - can generate and test hypotheses
* Gregorian (after Richard Gregory) - have culture and communication

# Hamming on young mathematicians



Richard Hamming claimed that mathematicians and theor. physicists publish their most influential work “disgustingly young” (sic) not because they get dumber with age, but because of a shadow of success. They are afraid to start anew, ask small questions again, make baby steps.

Hamming called it “planting acorns”. Because mighty oaks only grow from small acorns. Early success is like a curse.

Related article about cognitive decline that technically starts already in one's 40s:
https://www.theatlantic.com/magazine/archive/2019/07/work-peak-professional-decline/590650/

# Tinbergen 4 "WHY"s



There are 4 types of "why" question in Biology (as per Tinbergen, 1963):
1. Function - what is it for? What's the evolutionary benefit of it?
2. Mechanism - how does it work?
3. Development - where does it come from, in this organism behavior?
4. Evolution - how did it develop?

Applies to all behaviors as well.

# Student evaluations


https://www.npr.org/sections/ed/2016/01/25/463846130/why-women-professors-get-lower-ratings
Why Female Professors Get Lower Ratings. 2016. Anya Kamenetz

# Extended mind
 

## Andrew Clark

Philosophy prof and author from U Edinburg ([wiki](https://en.wikipedia.org/wiki/Andy_Clark)). Apparently leading author on philosophy of mind extension; doing math on paper as a classicall example. "The boundary of skin and skull as arbitrary and cotnigively meaningless" (quoting wiki). Parity principle: there's no reason to rank different ways to achieve the same result (say, a mathematical calculation) if the result and the efficiency are the same.

> Mathematical formalistm is a great first example, but language is another one people rarely appreciate to its full extent. And then drawing figures (just Google "how to draw human figure") may be another example that, again, is seriously under-appreciated by non-professionals.

Also works on the general philosophy of cognitive processes, and on how we don't just produce responses to inputs, but are engaged in a never-ending quest in predicting inputs (predictive processing).

> Natural-Born Cyborgs (that I happen to have) is one of his books, but he wrote like 5 more.

# Music
 

McDermott, J., & Hauser, M. D. (2007). Nonhuman primates prefer slow tempos but dislike music overall. Cognition, 104(3), 654-668.
https://web.mit.edu/jhm/www/Pubs/McDermott_2007_monkey_tempo_silence_preferences.pdf

# Notes on databases
 

# Unprocessed:

* https://en.wikipedia.org/wiki/Database_transaction
* https://en.wikipedia.org/wiki/Consistency_(database_systems)
* https://en.wikipedia.org/wiki/Document-oriented_database
* https://en.wikipedia.org/wiki/Graph_database
* ACID transactions
* https://en.wikipedia.org/wiki/Polyglot_persistence
* https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch
* https://en.wikipedia.org/wiki/Eventual_consistency

Should not some of it go in the ?

# GIT


## Typical everyday use
* `git pull` - 	fetch latest changes, merge them, and rebase HEAD to the latest commit
* `git diff` - show differences to unstaged files
* `git add .` - stage everytyhing
* `git commit -m "Message"` - commit
* `git push` - push to origin ('Origin' is a silly name they use for a remote repo)

## Analyzing stuff
* `git status` - see uncommited files + comparison to origin
* `git diff` - see unstaged changes
* `git diff HEAD` - see both staged and unstaged changes
* `git show [branch]:[file]` - show file changes. Tons of output formatting. On itself, describes head. [1](https://git-scm.com/docs/git-show)
* `git blame [file]` - describes changes for a file. Note that filenames go without quotation marks.
* `git diff commit1 commit2` - compare two commits.
* `git log -p [file/dir]` - full history for this file, with diffs.
* `git add [file]` - add one file only.

## Backing up, carefully
* `git reset [file]` - when a file was staged, but not yet committed, unstage it.
* `git stash` - put all uncommited files to a buffer
* `git stash pop` - pop files from a buffer
* `git commit --amend` 
* `git rebase -i HEAD~3` - squash last 3 commits into one, interactively. **Do it only for commits that weren't pushed yet** (or you'll get a conflict with a remote repo), unless you're morally prepared to fix everything locally and force-push to origin, rewriting it. "Interactively" here means that a list of commits is generated, and you leave `pick` for those you want to leave; replace it with `drop` for those you want to delete, and `squash` for those that needs to be squashed (just make sure there's a 'pick' before it, for somewhere to squash to). This workflow is a reason why we shouldn't push to public repo too often (don't push micro-commits); wait until a reasonable piece of work is done, **clean the commits**, then push. [1](https://git-scm.com/docs/git-rebase#_interactive_mode), [2](https://medium.com/@porteneuve/getting-solid-at-git-rebase-vs-merge-4fa1a48c53aa)

## Backing up, panicky
* `git reflog` → find last commit that is good → `git reset HEAD@{index}`- resets head to this commit, deletes everything after.

## Branching
* `git branch` - to know the current branch
* `git checkout branch_name` - move to a target branch.
* `git branch branch_name` - creates a new branch.
* `git checkout -p branch_name` - an alias for creating a new branch at current HEAD, and checking it out. May be a good idea after a pull, to do all sorting in a safe branch, without endangering Master. [1](https://blog.carbonfive.com/2017/08/28/always-squash-and-rebase-your-git-commits/)

The concept of a **detached HEAD**: when HEAD points to an old commit. In this situation you cannot commit anything, as there's no branch to commit to (committing can only be done to the end of a branch). If you create a new branch there in the past however, you can commit, and then you can merge these changes if you need to. [1](https://www.atlassian.com/git/tutorials/using-branches/git-checkout)

## Dangerous practices
* `git rebase [target_branch] <source_branch>` - recommits new commits from the current branch (the one that is currently checked out), or the source_branch (if specified), to the end of the target branch; then moves HEAD to the target branch. The semantics here is "Rebase source onto target", and it is supposed to mirror "Merge source into target", as what you are doing is taking a thread of commits, and changing their base from whatever it is now, to target. 
The pluses and minuses of rebase (compared to Merge):
        * Good: it makes history linear, and thus clear, as there's no branching and merging. Say, if a month ago you created a side-project file in a side branch, and nobody touched it since, but kept working on the main thing, there's no need in merging this file in; you can as well just recommit it on top of master (rebase).
        * Good: allows code clean-up (e.g. described above for interactive rebase and commit squishing)
        * Bad: unlike for merging, commits are not inhereted, but are committed anew, and if branching was conceptually important, the history of it will be lost. Even worse, b1→rebase to b2→ merge to b1 will duplicate commits in b1 (they will be recommitted to b2 as new commits, but then merged back). "Always merge" may be a messy option, but it's the safest one for unexperienced teams ([ref](https://www.atlassian.com/git/articles/git-team-workflows-merge-or-rebase))
        * Bad: If you have a conflict, you'll see contradicting commits (one adding something, the other one reverting this something back). Which can be hard to clean up.
        * Bad: never use rebase on public branches, as rebasing rewrites history, and if somebody is synchronized with this branch, they'll get a weird conflict of histories. The only way to use it with remote branches is with hard-pushing at the end (rewriting the remote branch), which is of course an extreme measure. [1](https://www.atlassian.com/git/tutorials/merging-vs-rebasing), [2](https://medium.com/@porteneuve/getting-solid-at-git-rebase-vs-merge-4fa1a48c53aa), [3](http://gitready.com/advanced/2009/02/11/pull-with-rebase.html)
* `git rebase --onto [target_branch] [list of branches]` - allows advanced tree manipulation that I don't understand for now: [1](https://git-scm.com/docs/git-rebase)
* `git pull --rebase` - in practice, this command integrates unpulled (relatively recent) commits form the origin, placing them before (sic!) recent commits in the local branch. Equivalent to virtually rebasing to the origin, and then hard-pulling the result to the local branch. **The end-result is exactly as if you pulled origin before making your commits.** Note how in this case rebase also rewrite the history, but in a safer way (it can be pushed back to origin without any issues), because rebasing happens locally. [1](http://thelazylog.com/git-rebase-or-git-pull/), [2](https://www.atlassian.com/git/tutorials/merging-vs-rebasing), [3](http://gitready.com/advanced/2009/02/11/pull-with-rebase.html)
* `git push --force` - hard push to origin, overwriting it: generally, a very dangerous idea.
* `git fetch origin master; git reset --hard origin/master` - hard-pull from origin overwriting local files. Equally dangerous. [r1](https://stackoverflow.com/questions/1125968/how-do-i-force-git-pull-to-overwrite-local-files)

## Destructive commands
* `git clean` - from the current dir, removes all files that are not under version control (all untracked files)
* `git reset --hard [commit_id]` - purge all uncommited changes; reset to commit (last one by default)
* `git branch -d branch_name` - deletes branch

## Open questions
* `git reset`
* `--no-ff` for `git merge
* `git pull -f` - what does this f mean?
* `pick`
* `fixup` - condenses several commits into one?
* why `commit -am` (for committing all) is even an option? What's the point of committing all?
* pull requests?

## References
* Funny short cheatsheet "Dangit": http://dangitgit.com/

# Machine Learning Interviews
by Chip Huyen, self-published brochure
 

https://github.com/chiphuyen/machine-learning-systems-design/blob/master/build/build1/consolidated.pdf

Research environment is focused on training and ensembles (fractions of % in performance), while industry is interested in reliable pipelines (serving).

That's why Kaggle doesn't represent real life for example: the data is cleaned and nicely annotated, and also multiple testing situation, so the best model would not necessarily translate to other (even if similar) tasks.

When describing a mock ML pipeline, consider:
* Goals (including several possible reformulations). Relative costs of false-positives and false-negatives. Social biases.
* Data sources (avaiability, collection). User experience. Privacy concerns.
* Constaints; storage, whether all available at once, fits into memory. Data structures.
* Data pre-processing and representation
* Evaluation

Baselines:
* Random
* Human
* Simplistic heuristic

Some comments on working with very big datasets (when even one case doesn't fit into  memory: Gradient Checkpoint), parallel computing (Asynchronous SGD). Some refs.

Nice list of references to well documented ML cases on the web, followed by a [list of sample interview questions](https://github.com/chiphuyen/machine-learning-systems-design/blob/master/content/exercises.md).

# ML lore


**Deep learning maxims**: ([ref](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/)):
* Use ADAM with 3e-4 learning rate. Don't decay learning rate: ADAM takes care of that.
* ReLU are best units (except for LSTMs that use tanhs ([ref](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf)))
* Never use activation function in the last layer (last layer needs to scale)
* Use bias in every layer
* Use variance-scaled weight initialization: $\mathcal{N}(0,\sqrt{2/n})$, aka "He et al." (after 2015 paper)
* Normalize (-m, /s) input data. Particularly critical if L1/L2 regularization is used ([ref](https://medium.com/ai%C2%B3-theory-practice-business/top-6-errors-novice-machine-learning-engineers-make-e82273d394db))
* Compress (transform) variables if necessary (say, apply  tanh(x/C) )
* 128 filters in a convolution layer is a lot; if you need more, something is wrong
* Pooling (maxpooling) helps with transformation invariance (_why???_)
* For very different inputs (say, images + words) project to a relatively low-dim space first, then concat ([ref](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf))

**Checkpoints**:
* Normal training (minutes to hours, while playing and debugging): test every n_epochs, save k best models, based on validation metrics ([ref](https://blog.floydhub.com/checkpointing-tutorial-for-tensorflow-keras-and-pytorch/))
* Slow training (hours to days): possibly a nested strategy, with infrequent unconditional checkpoints, and more regular performance-based checkpoints

**Debugging tricks**:
* Fix random seed, to have reproducability
* Try a rock-bottom simple model, iteratively add stuff back
* Switch to a simplified training set (fewer labels, classes, [ref](http://josh-tobin.com/assets/pdf/troubleshooting-deep-neural-networks-01-19.pdf))
* Verify that your loss starts at a theoretically expected value ([ref](http://karpathy.github.io/2019/04/25/recipe/))
* Overfit one batch (shoudl always be doable)
* Play with learning rate (higher to get the gist of dynamics, lower to assess the minima)
* Play with batch size (1 to get the gist of what's happening, very high for most gentle and careful descent)
* Remove bach normalization, check if explosions or vanishings are happening (batch normalization is so good and useful that it obscures problems, such as NaNs)
* Find a bunch of incorrect predictions, look for commonalities
* Make sure you don't softmax outputs to a loss that expects logits or something like that
* Check for class imbalance in the testing set (compared to training)

**Unit tests for ML**:
* Unit tests for ML: write a test that takes one step, and compared before and after, or at least check that it changed ([ref](https://medium.com/@keeper6928/how-to-unit-test-machine-learning-code-57cf6fd81765))
* Assert that loss != 0 (ibid)
* Test that params that need to be frozen at a particular stage of training actually don't change (ibid)

**Other advice:**
* If labels are hard to get, use manual **active learning**: label small part of the dataset, use it to provide (bad) labels to the full dataset; make the model report uncertainty, use it to identify observations that need to be labeled first to most improve model performance ([ref](https://www.jeremyjordan.me/ml-projects-guide/))
* Separate data pre-processing from the learning pipeline: at rearch phase you want to pre-process data once, then play with it repeatedly ([ref](https://medium.com/infinity-aka-aseem/things-we-wish-we-had-known-before-we-started-our-first-machine-learning-project-336d1d6f2184))
* You can always gain a few more % by using ensembles ([ref](http://karpathy.github.io/2019/04/25/recipe/))

# MNIST


A famous database. Included as a standard dataset in both scikit-learn, and tensorflow. Comes already split into training and testing sets (60000 and 10000 examples, respectively). It seems that to keep everything comparable, it is not common to change testing split for this set.

Links:
* A zoo of all thinkable approaches to classification, applied to this dataset, each shown with a reference and a test error rate: http://yann.lecun.com/exdb/mnist/index.html
* A similar collection of best results, achieved with different methods: https://paperswithcode.com/sota/image-classification-on-mnist

The current SOTA is about 0.2% error rate (1-accuracy).

> One could wonder to what extent current SOTA aproaches are over-optimized for this testing set particularly. Is there a way to target-overfit here? I'm sure there could be a theoretical estimation for that.

# Pandas


* `[[]]` simply means "a list inside a `[]` call"
* Select columns by label: `d['x']`. Alternative spelling: `d.x`. Returns a series. 
* Select rows by label: `d.loc[1]`. Works for both df (returns a row-series), and for column-series (returns a single value).
* **Chained Assignment**: a problem while writing to a frame, selecting by both column and row.
    * Good: reference both by label (index): `d.loc[1,'x']`
    * Also Good: reference both by position:`d.iloc[1,0]`. Row goes first. 
    * Read, but not write: `d.x[1]`, which is equivalent to `d['x'][1]`.
    * Read but not write: Both `d.x.iloc[1]` and `d.iloc[1].x`. Documentation states that whether any given slice would work or not is "officially unpredictable", so chaining shoud never be used.
* Out-of-range integers are forgiven (ignored) on reading, but cause an error if you try to write
* For **conditional data retrieval**: 
    * either **logical indexing** `d.loc[d.x>0]` (can be combined with list comprehensions, and can be written to) 
    * or **queries**: `d.query('x>0')` (easier for a human to read; reads slightly faster, but cannot be written to).
* Conditional indexing supports functions, as long as they take and return Pandas series, or something compatible, like a Numpy array).

# SQL


**Some refs:**
* [Good reference](https://www.w3schools.com/sql/default.asp)

## Generic query
```sql
SELECT col1, MAX(col2) AS colname2
FROM table1 AS t1
WHERE (col1=1 AND col2>2) OR NOT (col5 IN ("cat","dog")) OR (col8 BETWEEN 3 AND 7)
GROUP BY col3
HAVING COUNT(*)>1 or SUM(col7)>0
ORDER BY col4 DESC, col5 ASC
LIMIT 10
FROM table2 AS t2 JOIN *
ON t1.id=t2.id
WHERE t2.col2 IS NULL;
```

##  More bells and whistles
* `DISTINCT` - return unique results only. Can go right after select: `SELECT DISTINCT`, or can be put inside count, like in `COUNT (DISTINCT col2)`.
* `LIKE` - search for a pattern in text; goes inside the condition: `WHERE col LIKE 'a%'`. Supports wildcards: `%` for any number of characters (including none), `_` for a single character; `[ab]` for either a or b (as in regular expressions), `[^1859636977ab]` for any character except a and b (not on all systems). In general, wildcards seem to differ a bit across systems.
* Depending on the version, SQL may use either `<>` or `!=` for 'not equal'. It seems that `!=` is actually preferred, but `<>` is older, and so is used as a legacy operator. At least in some versions, `!<` (not less than) and `!>` also exist. They seem to compare strings, in a case-sensitive maner, and if string would meet a number, it seems there will be an attempt to convert them for comparison. It is also possible to cast a string to a different type, or to transform it, including changing its encoding (see below).
* Precedence: arithmetics (including bitwise `~&|`) > comparisons > `NOT` > `AND` > `OR` and its friends (`LIKE`, `IN`, `BETWEEN`, `ALL`, `ANY`, `SOME`) > assignment.
* `IN` can use either a fixed list, or a subquery.
* `AVG`, `MIN`, `MAX` - other functions for grouped queries, similar to SUM. They can also go in the SELECT block (to be returned, instead of conditioned), and combined with aliasing.
* `COALESCE (col1, col2, "default value")` - returns the first non-null element from this list
* `CASE` - a whole case switch system for binning values on-the fly, and returning the bin name (uses WHEN, THEN, and ELSE as other keywords inside the CASE structure. Read the manual if needed.)
* `HAVING` is used for clauses on aggregate functions, as it is performed after WHERE (and after grouping).
* `ASC` is used with `ORDER BY` by default, and so can be omitted
* Some systems use `TOP 10` (SQL Server, MS Access) or `ROWNUM <= 10` (Oracle) instead of `LIMIT 10` (mySQL). Those that use TOP also support a codeword `TOP 10 PERCENT`.
* `OFFSET 1` - a rather rare thing that goes in the same part as LIMIT, and makes the query return not the rows that were found, but rows that are offset from this rows by this number.
* `UNION`, `INTERSECT`, `EXCEPT` - Logical operations on selects. Usage: `SELECT * FROM table1 UNION SELECT * FROM table 2;`. The tables should match in terms of their columns, otherwise it'll break (use JOINs if the tables don't match perfectly). The first table that was called defines column names for the entire output. By default UNION only returns dinstinct rows, but use `UNION ALL` if duplicates are needed.

### Joins
* `INNER JOIN` - only those records that exist in both tables. A simple `JOIN` without anything else, is an INNER JOIN by default.
* `LEFT JOIN` - all rows from the first table, and if possible - matching rows from the 2nd table. If not possible, it pads the output with nulls. Some manuals claim that one should add the word OUTER before JOIN, but it doesn't seem to be necessary.
* `RIGHT JOIN`- same as left, but reverse
* `FULL JOIN` - combines both tables (and thus acts same as UNION describe above).
* To imitate subtraction, do: `SELECT * FROM t1 LEFT JOIN t2 ON t1.key=t2.key WHERE t2.key is NULL;`. This way it will first try to JOIN, but set t2.key to null for all missing elements, and then they'll immediately be filtered.
* Something like a self-join is also possible, using syntax with aliasing: `SELECT a.name AS name1, b.name AS name2 FROM table1 AS a, table1 AS b WHERE a.manager=b.id;`. In this case we'll have a list of all relations between people in an organization, all from one self-referencing table.

### Subquery
`SELECT * FROM table1 WHERE id IN (SELECT ...);`.
* `ALL`, `ANY` - used within where or having, as conditions on subqueries. So you can do `WHERE col1 = ANY (SELECT ...)`.
* `SOME` - exact synonim to `ANY` used in some versions.
* `EXISTS`- to check whether a subquery returned anything: `WHERE EXISTS (SELECT ...)`.

### Virtual table
`CREATE VIEW view_name AS SELECT * FROM table1 WHERE coll="whatever";`. Can also be updated by `CREATE OR REPLACE VIEW view_name`. After you are done with this virtual table, it should be dropped using `DROP VIEW view_name`.

## Insert data
`INSERT INTO table1 (col1, col2) VALUES (1, "dog");`. If you know the order of columns, you can also skip the first brackeet, and just do `INSERT INTO table1 VALUES (...);`. If not all columns are specified, all remaining will be set to null.

Interesting way to copy some selected stuff from one table to another: `INSERT INTO table1 SELECT * FROM table2 WHERE condition;`. Interestingly, the condition may reference both tables, allowing for interesting data movements. If the table doesn't exist yet, use a different syntax: `SELECT * INTO table1 FROM table2 ...;`.

## Update rows
Like a simple select query (without grouping and other fancy things obviously), just instead of SELECT you do: `UPDATE table1 SET col1=1, col2="dog" WHERE id=0;`. Be careful to always have a `WHERE` statement there, as without it the statement will still be valid; just it will update all records. Updates can directly reference table fields, so things like `col1 = col1+1` are normal. Updates can also reference other tables by running a subquery, or doing a JOINT, or (in some versions) even by having a FROM sequence directly following the UPDATE part.

## Create stuff
* Create database: `CREATE DATABASE dbname;`
* Create a table: `CREATE TABLE table1 (col1 type, col2 type ...);`. There are many types, but key ones are: INT (medium, 4 bytes), BIGINT, DOUBLE, BOOL, CHAR(size) - fixed length, TEXT - variable length, LONGTEXT, ENUM(val1,val2,...) - a limited list of options, DATE, TIME. These really depend on the system though, so need to be researched.
* Columns can have **constraints** that are specified during creation, like that: `col_name data_type constraint`, where constraints come from the following list: `NOT NULL` - cannot be nulll; `UNIQUE` - all records are unique; `PRIMARY KEY` - both not null and unique (often combined with `AUTO INCREMENT`, which is not technically a constraint, but a built-in resolution of it); `FOREIGN KEY REFERENCES table2(keycol) ` - a key for a diff table; `CHECK (col_name>0)` - requires a condition to be met; `DEFAULT value` - sets a default value; `INDEX` - makes retrieval faster. If a constraint like CHECK or UNIQUE isn't met, it triggers an error, and in a real system it shoudl be handled properly (see below for control structures). Also the syntax of constraints differs a bit between systems.
* Another way to build an index: `CREATE INDEX ind ON table1 (col1);`. More than one column may be indexed, which may make reading faster, but it will also make writing slower. Ideally, indices should match selects, but maybe almost anti-match insertions, deletions, and updates.

## Change table structure
`ALTER TABLE tb ADD col_name type;`, or `ALTER TABLE tb DROP COLUMN col_name`, or `ALTER TABLE tb MODIFY COLUMN col_name type` (some systems use ALTER COLUMN instead of MODIFY). Constraints may be removed from columns using `ALTER TABLE tb DROP CONSTRAINT constraint;` (slightly diff syntax in MySQL).

## Delete stuff
* Delete a row: `DELETE FROM table WHERE condition;`. As usual, without WHERE would delete everything.
* Delete (drop) a whole table: `DROP TABLE table1;`. Simple and brutal.
Apparently it's possible not to grant users permissions to delete rows and drop tables (something like `REVOKE DELETE ON table TO username;` for this particular user), and it is possible to create server-wide triggers and auto-rollback on dropping; and in some editions. There may also be a way to protect tables using a schema, but I'm not sure about that.
* `BACKUP DATABASE dbname` also exists.

## Functions on data
There are lots of built-in functions; too many to mention, including math, trigonometry, string manipulation (like `LEFT`, `LEN`, `LOWER`, and alike), and what not. Some interesting ones that are not obvious are `LEAST` and `GRATEST` that work across differenct columns within the same row, as opposed to MAX and MIN that work along all rows (entries) for a returned column. There are also lots of **system-specific operaions on dates and times**. Read the manual.

## Control structures
Apparently SQL (especially larger systems, like **SQLServer**, or **Transact-SQL** from Microsoft) also has its own system of control structures, with IF, ELSE, BEGIN TRY, BEGIN CATCH, WHILE, and what not.