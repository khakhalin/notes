# Deep learning

Some terminology:
* **Hyperparameters**: those somewhat arbitrary values that define the type of solution the model is looking for, and the process of descent. Examples: learning rate, batch size.
* **Learning rate**: Goldilocks principle - the best learning rate should "magically" put you in the minimum in a very few steps. Large learning rate leads to noisy oscillations after what looked like a convergence. It may even break everything (unstable).
* **Mini-batch**: process >1 (usually 10-1000) points at a time. Somewhere in between fully stochastic descent (1 point at a time) and math-optimal (all points every time).

## Backpropagation
Essentially a chain diff rule, for cost function J by weights w.
Consider a network of several layers (full = dense = all to all), eventually all convering on one output element. MSE loss: J=(a-y)², where a = out = h(z) = h(∑ w10_i a1_i) = dot product of of prev layer activations with 1→0 weights. (I'll be numbering layers backwards, starting from 0 for the output layer)

To change one of the weights w10_i, we need to know ∂J/∂w10_i . Dropping i (but actually it's still for one weight for now): ∂J/∂w10 = ∂J/∂a ∂a/∂z ∂z/∂w10 = 2(a-y) h'(z) a1,
because ∂J/∂a = 2(a-y),
∂a/∂z = ∂h(z)/∂z = h'(z), and
∂z/∂w10 = a1. That is, activation a1_i arriving from the layer 1, that gets multiplied by w10_i.

So like Hebbian rule: if (a-y) is large, and a1_i is large, then w10_i matters, and gets changed.
(And we get a similar formula for bias, as technically it's always h(w∙a+b) and not just h(w∙a): so we just get 1 instead of a_1 in the formula, because ∂z/∂b = 1).

Now we can go deeper, to the yet-previous layer 2→1, with weights w21_j.
No need to sum yet, as w21_j only affects one element in the 1st layer: a1_i:
Again, we have:
∂J/∂w21_j = ∂J/∂a1_i ∂a1_i/∂w21_j.
∂J/∂a1_i can be calculated as above = 2(a-y) h'(z) w10_i,
while ∂a1_i/∂w2_j = ∂a1_i/∂z1_i ∂z1_i/∂w2_j = h'(z1_i) a2_j (activation a2_j from yet prev layer).
So the full expression: ∂J/∂w21_j = 2(a-y) h'(z) w10_i h'(z1_i) a2_j.

Now consider layer 3→2. Again chain rule, but now while w32_k coming from a3_k affects only one a2_j, this activity in turn affects all of the a1_i. And then it gathers back on our single output. So index i is no longer fixed, but we have to run a sum for it:
∂J/∂w32_k = ∑_i ∂J/∂a1_i ∂a1_i/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i ∂a1_i/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i h'(z1_i) w21_j ∂a2_j/∂w32_k = 
2(a-y) h'(z) ∑_i w10_i h'(z1_i) w21_j h'(z2_j) a3_k.

And so on; we can now do it for layers. What's important is that we essentially take the end error derivative 2(a-y), scale it by h'(z); then multiply by the same vector of  w10 that were used on the forward pass, to get "error-effects" at the previous layer 1. Then same way we took the vector of "error-derivatives" (gradient) from layer 1, scale each element by h'(z1), multiply it by transposed matrix w21, and get "error-effects" at layer 2. Backprop!

And the problems are immediately felt: if a certain w_ji=0, it kills the effect of all weights converging on element j. Moreover, if the value of z_j is such that it drives h'(z_j) to zero, it also kills the gradient (aka **vanishing gradients**). Say, for sigmoids it happens for very high or very small z; for ReLUs it happens for any z<0, and they can't recover (aka "Dead ReLUs"). And the other way around, in a deep network, gradients can grow arbitrarily large (**exploding gradients**).

References: 
* [Backprop calc by 3blue1brown](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)

## Loss functions
Obvious choice for continuous output: **Eucleadian distance** (aka **Mean Squared Error**, or **MSE**)

For classification: **Cross-Entropy Loss**. The ground-truth is one-hot encoded vector; the prediction is a vector of probabilities. Definition: H(p,q) = -∑ p_i ∙ lg(q_i) where p_i are probabilities of different classes in the training set, and q_i are probabilities predicted by the model.

Motivation: essentially, average log-likelihood. Assume that the predictions of the model follow a true correct distribution, with q_i encoding P(x_i). Then what it the probability of observing n_i cases of x_i? Assuming that test cases are independent, P = ∏P(each observation) = ∏q_i ^ n_i . If we lg it (to replace products with sums), we get the total log-likelihood: L = ∑ n_i ∙ lg(q_i) . If you average it by dividing by the total number of test cases, and denote p_i = n_i / N, we get loss = ∑ p_i ∙ lg(q_i) =-H(p,q) . And then you are trying to minimize this value by making the model make predictions q_i such as the observed values (from P) are "minimally strange". Feels Bayesian, huh?

SVMs use a special thing called **Hinge loss** (see chapter "Classification")

**Huber loss**: a compromise between MSE loss that is tolerant to small noise (behaves nicely around 0, as x² ≪ x for small x), but isn't super-sensitive to outliers (behaves as mean absolute error there). Essentially, just a parabola with arms that smoothly transition to linear at some point, and continue like that. Formula: L = 1/2∙x² for x<d, but (abs(x)-d/2)∙d for x>d. ([wiki ref](https://en.wikipedia.org/wiki/Huber_loss))

References:
* [by Daniel Godoy](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) - a visual explanation for a binary case. Good intuition for why -log(1-prediction) makes sense: if you were very certain, and got it wrong, that's a more important learning point compared to a case when you were pretty lukewarm about it to begin with.
* [Wiki for cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy)
* [A list of losses supported by Keras](https://keras.io/losses/)

## Regularization
(see also: Ridge regression in [[04_Regression]])

**L2 regularization**: use a modified loss = Loss(Data|Model) + λ∑ω² where ω are model weights, and λ is a hyperparameter known as **Regularization rate**. High values of λ push {ω} towards normal distribution, while low λ make the distributin of {ω} closer to uniform. Discourages sparseness of {ω}, as having many small weights becomes better than having a few solid ones. So, in a way, L2 regularization is pro-democracy, and anti-parsimony. As λ is increased, weights are pressed down asymptotically.

Alternative: **L1 regularization**, aka **Lasso** (least absolute shrinking). Loss = Loss(Data|Model) +  λ∑abs(ω) . Aggressive shrinkage of small ω; encourages parsimony; discourages "leakage of features". As λ is increased, weights, one by one, suddenly drop dead from something to zero.

**Elastic net**: some sort of combination of both, with two different λ. Flexible, nice.

## Dropout
Huge breakthrough, discovered in 2012: resolves overfitting in deep networks. Randomly inactivate a large share (~50%?) of all units in every layer at every training step; then only consider the output of remaining units; and only train them. When all weights are learned together, many of them slack out by just running to the ground (zero), and not contributing to anything (would have to be pruned). But with drop-out, we have something of a built-in ensemble method when many sub-networks have to train in parallel.

> Is this intuition even true? Is dropout in any way similar to ensemble? Are ensambles and regularization secretly the same thing?

A gradient of a dropped-out network is the same as for a full network, but with an additional "regularization term" for activation: ∂J_dropped/∂w = ∂J/dw + p(1-p)w∙I² , where p is the probability of elements staying active at each step (they use fixed probability, and not a fixed number of active elements). _Ref 1 pretends to have math, but immediately plunges from obvious to confusing._

Because of that, dropout is most effective for p=0.5 (it maximizes p(1-p)). In practice, it's OK to set p at 0.5 for intermediate layers, but it should be higher (0.8?) for input layers, in order not to imitate undersampling. Keras takes 1-p as an argument (p of dropping), so adjust accordingly.

**Gaussian dropout**: instead of completely eliminating a node at each step, just multiply each activation by 𝒩(1,σ). A cool thing is that it doesn't change the gradient on average, so nothing needs to be scaled. (_What does it mean?_)

> How much exactly does drop-out help? How to quantify that? How do people quantify effectiveness of things like regularization? If dropout is similar to regularization, is it strictly better than regularization, or is it the same? If we explicitly add this wI^2-proportional (but fixed) regularization, will it give the same result? I'm guessing the answer is "no", but why?

There's a claim that dropout should not be used on convolutional networks, because it does not actually work (does not inactivate elements), as convolution introduces coordination between changes in different weights. Refs [1](https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2), [2](https://www.kdnuggets.com/2018/09/dropout-convolutional-networks.html)

Refs:
* [Simplified math](https://towardsdatascience.com/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275), by Chitta Ranjan - soso

## Activation functions
https://en.wikipedia.org/wiki/Gated_recurrent_unit

# Other layer types

## Pooling layers
**Maxpool**: Only retains the maximal value, and drops all the rest. Mostly used in convolutional networks, in which case it has dimensions, depth, and a stride. Has no learnable parameters. At backpropagation step, it only propagates to those neurons that contributed to the selected value (max value); all other neurons aren't updated. Good for downsampling feature maps, where the presence of a feature (quantified by different components of the depth-vector) is more important than the precise position of this feature.

Extreme version of this: **Global pooling** - entire tensor in one vector (equivalent to doing maxpooling with W and H of full image). A faster alternative to having a fully connected layer from a convolutional layer to a global feature vector.

**Average pooling**: obviously, just averages all the inputs. Good for downsampling.

Refs: [one](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/)

## Softmax
Takes in an aribtrary vector, and transforms it so that all values are positive, between 0 and 1, and the sum is = 1 (and so outputs can be interpreted as probabilities). Formula: first exp(each input), then divide by the sum of all. Preferred last layer in classification networks (in which case it's used with cross-entropy loss function).

If placed in the middle it is roughly equivalent to an exponential activation layer with batch normalization. But apparently it is possible to use it in the middle as well ([link example](https://github.com/gorobei9/jtest/blob/master/machine-learning/MNIST%20for%20Crazy%20People.ipynb)).

How to train Softmax layers? Better to balance labels, to use all training points for the label trained, but only a random subsample for negative labels. _Is it to avoid overtraining on negative examples, as for any given label the majority of examples in a natural dataset will be negative?_

## Batch normalization

Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.
[https://arxiv.org/abs/1502.03167](<https://arxiv.org/abs/1502.03167>)
Main paper on batch norm (with like 30k references)

# CNNs
**Convolutional Neural Networks**

* Wavenet: https://deepmind.com/blog/article/wavenet-generative-model-raw-audio

## Transformers
#attention

An alternative to RNNs (summary ref: [1](https://towardsdatascience.com/transformers-141e32e69591)). Text is inherently a stream of signals (words), organized in time, so it makes sense that it should be analyzed by **RNNs**. The problem is that text has long-ish connections (say, objects are elliptically passed from one sentence to another), and RNNs have problems with that (forget, or rather, don't even learn). Then **LSTM** was invented, which improves things, but not enough, as the context still tends to decay exponentially. Also, hard to parallelize, as inputs need to be processed word-by-word. Then they came up with **attention**, by assigning every word a certain "hidden state", and passing it to the entire line (_??? I really not sure how it works_), but still it didn't help enough. The lack of parallelization was the main bottleneck.

But convolutional networks that process info in chunks are highly parallelizable! And the information depth is lower (instead of going through all N elements, activation from the furthest element climbs up the hirarchy through just log N layers). Transformers combine CNNs with attention, and consist of several (original paper has N = 6) **encoders** and the same number of **decoders**. Each encoder has the same architecture, but different parameters, and the same is true for d coders. 

Each **encoder** consists of two layers: **Self-Attention** block, consisting of several self-attention operations performed in parallel (aka **Multi-Headed attention**), followed by a shallow **FF network**. First encoder receives words, and embeds each word in an **embedding** vector (say, length 512). Self-attention transforms this vector with 3 trainable matrices into 3 smaller vectors (length 64) **Query, Key, and Value**. Then, for each pair of words ij, we calculate s_ij = ⟨q_i , k_j⟩, scale it down roughly (s_ij/8), and softmax (now ∑s_ij = 1). These numbers S now quantify interactions between the words. Then we calculate the weighted sum of S for each word j related to the target word i, using vector v for this word as weights: z_i = ∑ s_ij ∙ v_j. This vector {z_i} becomes an output of the self-attention layer. 

In practice of course, vector multiplications for each word happen not in a loop, but in matrices Q, K, and V, with columns corresponding to words. A full formula for attention, parallelized, for all words : $A = Ⰿ(Q^T K / \sqrt{d})\cdot V$, where Ⰿ stands for soft-max. 

Then h=8 attentions are calculated in parallel, each with a different set of QKVW. The outputs are concatecated together, to form a longer vector of length 512, which is passed to the next encoder. This is called **Multihead Attention**.

Each A is then passed to a FF network. The **FF network** consists of a dense ReLu layer, followed by a dense linear layer, with coefficients W_1 and W_2.  that does $Ⱊ (A\cdot W_1)\cdot W_2$, where Ⱊ stands for ReLu, and is shaped as 512→2048→512.

> Is it actually "Each column of A"? Still confused about how many words are processed together!!

At the embedding stage, they also encode word positions. _Not sure how it happened exactly_

Each **decoder** had 3 layers: a similar Self-Attention, followed by an **Encoder-Decoder Attention** layer, in which Q comes from the previous layer, whille K and V  come from an output of a matching encoder, followed by a dense layer W (FF network). Apparently, it mimics what RNNs were known to do well before (RNN encoder-decoder mechanism).

References:
* Re-implementation of the original paper, in Python: http://nlp.seas.harvard.edu/2018/04/03/attention.html
* https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8
* https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1
* [[Vaswani2017attention]]  - original Transformers paper, titled "Attention is all you need". Doesn't actually describe the motivation, nor how they came up with this, nor how it works, nor the philosophy of it. [Direct pdf link](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)

**Criticism.** There are claims that transformers are overrated, and same results may be achieved with simple architectures, and much lower computer:
* Merity, S. (2019). Single Headed Attention RNN: Stop Thinking With Your Head. arXiv preprint arXiv:1911.11423. [link](https://arxiv.org/abs/1911.11423)