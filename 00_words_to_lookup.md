# General To-Dos:

* Move old recommendatinos from Sven down here
* Move all old Jaeger and network papers here

# Words and topics to look up

Random pieces of math
* Refresh the general flavor of Lagrangians and Hamiltonians

Classic ML and friends
* Bayesian PCA
* Kalman filter (and its friends)
* Mean field particle methods ([starting with wiki](https://en.wikipedia.org/wiki/Mean_field_particle_methods))
* Resampling techniques: SMOTE, isotonic regression, Platt scaling
* Hungarian algorithm

Deep Learning:
* Find a review on transformers, before reading primary papers
* How to tell the receptive field of a neuron in a deep network? Do they gradient descend on it?
* Heuristic search - seems to be a topic in oldschool AI, but isn't it too general? Or do they mean something very specific by it?
* ADAM optimizer - what's the difference from othes?
* Xavier initialization of weights - what's cool about it? Also, is it true that variance-scaled init is better? ([claim](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/))
* Network distillation - why does it even work? Not how, but why?
* inception blocks
* residual blocks / networks
* Style transfer - how does it work?
* How to actually organize curriculum learning? Is there an automated strategy here?
* noise-contrastive estimation
* self-supervised boosting

Technical stuff
* YAML - some standard for keeping stuff that's like XML, but human-readable. Who uses it? Is it popular? ([wiki](https://en.wikipedia.org/wiki/YAML))
* JSON - how does it work, why everybody love it, and is it the present or the future? ([wiki](https://en.wikipedia.org/wiki/JSON))
* bash
* Hadoop
* Spark

Network science
* Perronâ€“Frobenius theorem (something related to centralities? or network graph eigenvectors?)

Neuro
* ...
