# General To-Dos:

#todo

* Current point: https://developers.google.com/machine-learning/crash-course/training-neural-networks/best-practices
* Write a short Pandas cheat sheat about everything selection!!!
* Why drop-out works?
* Do people use leakyReLus often? Do they prefer them for pruning, or is it ignored?
* Do people use pseudo-ranking (ranking followed by approximation) for variable pre-processing? Does it have a name?
* Double-check what is eager about eager mode\
* Move old recommendatinos from Sven down here
* Move all old Jaeger and network papers here

# Words and topics to look up

Random pieces of coding
* from sklearn import metrics

Classic ML and friends
* RBF - Radial basis function kernel: https://en.wikipedia.org/wiki/Radial_basis_function_kernel
* AdaBoost
* QDA - Quadratic Discriminant Analysis
* Expectation-Maximization
* Kalman filter (and its friends)
* Bayesian PCA
* more on Gaussian processes
* Graphical models
* Hidden markov models
* Variational inference
* Resampling techniques: SMOTE, isotonic regression, Platt scaling
* Hungarian algorithm

Deep Learning:
* How to tell the receptive field of a neuron in a deep network? Do they gradient descend on it?
* Heuristic search - seems to be a topic in oldschool AI, but isn't it too general? Or do they mean something very specific by it?
* ADAM optimizer - what's the difference from othes?
* Xavier initialization of weights - what's cool about it? Also, is it true that variance-scaled init is better? ([claim](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/))
* Network distillation - why does it even work? Not how, but why?
* inception blocks
* residual blocks / networks
* Style transfer - how does it work?
* How to actually organize curriculum learning? Is there an automated strategy here?
* noise-contrastive estimation
* self-supervised boosting

Technical stuff
* regex in Python
* YAML - some standard for keeping stuff that's like XML, but human-readable. Who uses it? Is it popular? ([wiki](https://en.wikipedia.org/wiki/YAML))
* JSON - how does it work, why everybody love it, and is it the present or the future? ([wiki](https://en.wikipedia.org/wiki/JSON))
* bash
* Hadoop
* Spark

Random pieces of math
* Refresh the general flavor of Lagrangians and Hamiltonians
* Perronâ€“Frobenius theorem - something related to centralities and or network graph eigenvectors

Neuro
* ...
