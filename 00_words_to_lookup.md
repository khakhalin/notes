# General To-Dos:

#todo
Current point in the Google Course: https://developers.google.com/machine-learning/crash-course/production-ml-systems

# Words and topics to look up

* Move old recommendatinos from Sven down here
* Make sure all Eric's recommendations are here

Classic ML and friends
* std rotate	
* Hungarian algorithm
* Bloom filter
* Kalman filter (and its friends)
* RBF - Radial basis function kernel: https://en.wikipedia.org/wiki/Radial_basis_function_kernel
* AdaBoost
* Expectation-Maximization
* Bayesian PCA
* more on Gaussian processes
* Graphical models
* Hidden markov models
* Markov random fields (precursor of modern graph models?)
* Variational inference
* Resampling techniques: SMOTE, isotonic regression, Platt scaling

Deep Learning:
* Double-check what is eager about eager mode
* How to tell the receptive field of a neuron in a deep network? Do they gradient descend on it?
* Maximum Mean Discrepancy trick
* Replica trick and log-partition function (??)
* Gumbel-max trick
* Russian roulette trick
* Bayesian conjugacy trick
* Reparameterization trick
* Duality trick, easier Fenchel
* Doubly Reparameterized Gradient Estimators
* Do people use leakyReLus often? Do they prefer them for pruning, or is it ignored?
* ADAM optimizer - what's the difference from othes?
* Xavier initialization of weights - what's cool about it? Also, is it true that variance-scaled init is better? ([claim](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/))
* inception blocks
* residual blocks / networks
* noise-contrastive estimation
* self-supervised boosting

Technical stuff
* Relational databases: Snowflake, Redshift
* regex in Python
* JSON - how does it work, why everybody love it, and is it the present or the future? ([wiki](https://en.wikipedia.org/wiki/JSON))
* YAML - some standard for keeping stuff that's like XML, but human-readable. Who uses it? Is it popular? ([wiki](https://en.wikipedia.org/wiki/YAML))
* bash
* Hadoop
* Spark
* Python closures, why they are a thing, what's dangerous about them, and how to use them

Other math
* How to calculate single-value-decomposion and eigenvector decomposition in practice? How do they code it for numpy, for example?
* FFT
* Lagrange multiplyier - proof
* Hamiltonians and how dynamical systems invoke them
* Perronâ€“Frobenius theorem - something related to centralities and or network graph eigenvectors

Neuro
* ...
