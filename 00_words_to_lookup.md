# Words and topics to look up

* How to tell the receptive field of a neuron in a deep network? Do they gradient descend on it?
* YAML - some standard for keeping stuff that's like XML, but human-readable. Who uses it? Is it popular? ([wiki](https://en.wikipedia.org/wiki/YAML))
* JSON - how does it work, why everybody love it, and is it the present or the future? ([wiki](https://en.wikipedia.org/wiki/JSON))
* ADAM optimizer - what's the difference from othes?
* Xavier initialization of weights - what's cool about it? Also, is it true that variance-scaled init is better? ([claim](https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/))
* transformers
* inception blocks
* residual blocks / networks
* Lottery ticket hypothesis
* Network distillation - why does it even work?
* Style transfer - how does it work?
* How to actually organize curriculum learning? Is there an automated strategy here?

Classic ML and friends
* Helmholtz machine - some sort of associative network by Hilton?
* Resampling techniques: SMOTE, isotonic regression, Platt scaling
* Kalman filter (and its friends)
* KL divergence and its friends from information theory
* Hungarian algorithm

Network science
* Bonacich centrality
* communicability centrality
* Perronâ€“Frobenius theorem (something related to centralities? or network graph eigenvectors?)
* Free energy minimization in the brain

