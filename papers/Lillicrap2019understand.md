# What does it mean to understand a neural network?

Lillicrap, T. P., & Kording, K. P. (2019). What does it mean to understand a neural network?. arXiv preprint arXiv:1907.06374.
https://arxiv.org/abs/1907.06374

Parents: developmental neuro, [[interpretability]]
See also: kolmogorov complexity, crutchfield complexity, [[curriculum]] learning
Related papers: [[Cichy2019dlasmodels]]

#opinion #complexity #interpretability


Imagine an ANN doing something cool. We can code it, describe it (as vectors of params), or look at its elementary operations. But we usually cannot grasp "how" it works.

> My thoughts: But how do we understand how another human acts, or maybe even how we function? Say, you learned to hit a target with a ball, or tell a raven from a crow. What does it mean to "understand" how you do it? Arguably, we can only claim that we understand how we can do it, it if we can explain it, so essentially create a "curriculum learning" sequence that, when presented to a different but similar human, would teach them how to perform same function. And the neater (shorter), simpler (robust to noise), and more clear (effective) this explanation is, the more we feel that we "get it".

> If translated to an ANN case, to "understand an ANN", rather counter-intuitively, may mean "to provide an efficient distillation paradigm".

They mention "compressability of rules", so again "distillation".

"Neuroscience should focus on understanding development and learning" - wholeheartedly agree!

Human understanding is necessarily based on compactness: as we can only engage with a limited number of statements at any given time, we only feel that we "understand" something when the description at any moment is compact (like a few lines of code for an ANN).

> What about a hierarchical approach, like writing a book of 5 pars, where each part has 5 statements, each explained in 5 paragraphs? It feels to me that, technically, it's more about hierarchical compactness, than about compactness alone. Which again may be described in terms similar to that of a "curriculum", no?

Random trivia:
* Go has more than 10^(10^48) possible games :)
* Humans know (and can recognize) about 30 000 categories of objects.

A short summary of current attempts at understanding (with 2-3 refs for each) that mostly don't work:
* Sensitivity of outputs to changes in the system
* Adversarial stimuli (illusions) [[illusions]]
* Perturbation by removal of units (aka **ablation**)

> Then, I'd say, a good strategy would be to try "super-distillation": that is, finding a curriculum of a _very small number of stimuli_ that, when paired with a high learning rate (and maybe an unusual evolution of this learning rate), would bring a network to a target state. Possible study plan: train a network. Reverse-engineer a sequence of stimuli that, when given to this network, with a high learning rate, would bring it to a trained state as fast as possible. These stimuli won't be realistic obviously, but what would they be? We can try to go forward (from the same random state as the original network), or backwards (from trained to equal everything, subtracting info instead of adding it). If the process if starting state-dependent, we'd also have to check whether it teachers other networks similarly well (say, in case of distillation), and if not, we'll have to make sure it's not idiosyncratic to a network (how? not sure).

Example from neuroscience: people feel that they "understand the brain" if they have a mid-level "lossy model", similar to physics: not just the underlying forces, but also not just a list of outputs (behaviors), but something in-between, even if it's imperfect.

"Instead of asking how the brain works we should, arguably, ask how it learns to work."

### Some curious refs from this paper:

History is full of seemingly impossible things that ended up being possible (4):
* Michael Dickinson. Solving the mystery of insect flight. Scientific American, 284(6):48â€“57, 2001.

Humans know more than a megabyte worth of info about their own language (21):
* Francis Mollica and Steven T Piantadosi. Humans store about 1.5 megabytes of information during language acquisition. Royal Society Open Science, 6(3):181393, 2019.