# Deep Learning: Our Miraculous Year 1990-1991

Jürgen Schmidhuber
http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html

#review

Sort of a memoir blog post?

Claim that in this one academic year they actually published a first:
* Unsupervised pre-training before supervised training
* Distillation (that they called "collapsing" or "compressing")
* Sequential attention
* Hierarchical reinforcement learning
* "Fast weights": a network that outputs weights for another network, providing a "good guess" for it

The bizarre thing apparently is that all of these approach were later reinvented, without citations to his work. Huh. A beef with Geoff Hinton? Or not?

Related: Alexey Ivakhnenko apparently invented backprop in 1965, but is almost never cited ([another ref by Jürgen](http://people.idsia.ch/~juergen/deep-learning-conspiracy.html)).

Apparently he also proposed GANs in 1992 in some way, which led to a strange "long question" in 2016 [Neurips presentation by Ian Goodfellow](https://www.youtube.com/watch?v=HGYYEUSm-0Q&t=3779s). Except he called it "predictability minimization", and apparently Ian addresses it in his (original?) GAN paper.

Schmidthuber also  wrote a review of that whole GAN history in 2019:
https://arxiv.org/pdf/1906.04493.pdf