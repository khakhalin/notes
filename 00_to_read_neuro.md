# To-read: Neuro

#neuro #bib #todo

Se also: [[12_Neuro]] - for an official hub title page bibliography	

Perez-Nieves, N., Leung, V. C., Dragotti, P. L., & Goodman, D. F. (2020). Neural heterogeneity promotes robust learning. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.12.18.423468v1

Hasson, U., Nastase, S. A., & Goldstein, A. (2020). Direct Fit to Nature: An Evolutionary Perspective on Biological and Artificial Neural Networks. Neuron, 105(3), 416-434.
https://www.cell.com/neuron/pdf/S0896-6273(19)31044-X.pdf
An opinion (perspective) on how we could, and should, use deep models to understand psychology and neuroscience, as they are not that different than the results of evolution, after all.

Haber, A., & Schneidman, E. (2020). Learning the architectural features that predict functional similarity of neural networks. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.04.27.057752v1

# General, top, and fun

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.
https://www.pnas.org/content/116/21/10537.short

Backpropagation and the brain
Timothy P. Lillicrap, Adam Santoro, Luke Marris, Colin J. Akerman & Geoffrey Hinton (2020)
https://www.nature.com/articles/s41583-020-0277-3
(Opinion piece)

# Topology and development

Hassan, B. A., & Hiesinger, P. R. (2015). Beyond molecular codes: simple rules to wire complex brains. Cell, 163(2), 285-291.
https://www.cell.com/cell/fulltext/S0092-8674(15)01193-9
Developmental biology bordering fractals and graphs (maybe? not sure, but judging from the pics) - a nice review-like paper (perspective); worth a priority read :)

Zador, A. M. (2019). A critique of pure learning and what artificial neural networks can learn from animal brains. Nature communications, 10(1), 1-7.
https://core.ac.uk/download/pdf/227723588.pdf

Curto, C., & Morrison, K. (2019). Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience. Current opinion in neurobiology, 58, 11-20.
https://www.sciencedirect.com/science/article/pii/S0959438819300443 
Review. Exactly what it should be (networks, motifs, dynamics)

Feldman, D. P., & Crutchfield, J. P. (1998). Measures of statistical complexity: Why?. Physics Letters-Section A, 238(4), 244-252.
http://www.disca.upv.es/pabmitor/FILES/Complexity/971111_GEN_4.pdf
On Crutchfield complexity measure (that peaks for meaningful networks)

Morrison, A., Aertsen, A., and Diesmann, M. (2007). Spike-timing dependent plasticity in balanced random networks. Neural Computation, 19:1437–1467.

Lazar, A., Pipa, G., & Triesch, J. (2006, April). The combination of STDP and intrinsic plasticity yields complex dynamics in recurrent spiking networks. In ESANN (pp. 647-652).

Ozturk, I., & Halliday, D. M. (2016, December). Mapping spatio-temporally encoded patterns by reward-modulated STDP in Spiking neurons. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series on (pp. 1-8). IEEE.

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of complex computational structures from chaotic neural networks through reward-modulated Hebbian learning. Cerebral cortex, 24 (3), 677-690.
#todo

Gilson, M., Burkitt, A., & van Hemmen, J. L. (2010). STDP in recurrent neuronal networks. Spike-timing dependent plasticity, 271.

Naudé, J., Cessac, B., Berry, H., & Delord, B. (2013). Effects of cellular homeostatic intrinsic plasticity on dynamical and computational properties of biological recurrent neural networks. Journal of Neuroscience, 33(38), 15032-15043.

Gutig R, Aharonov R, Rotter S, Sompolinsky H. Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity. The Journal of neuroscience: the official journal of the Society for Neuroscience. 2003;23(9):3697–714. Epub 2003/05/09.

Morrison A, Diesmann M, Gerstner W. Phenomenological models of synaptic plasticity based on spike timing. Biol Cybern. 2008;98(6):459–78. Epub 2008/05/21.

Gilson M, Burkitt AN, Grayden DB, Thomas DA, van Hemmen JL. Emergence of network structure due to spike-timing-dependent plasticity in recurrent neuronal networks III: Partially connected neurons driven by spontaneous activity. Biol Cybern. 2009;101(5–6):411–26.

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning. Cerebral Cortex, 24, 677-690.

Fauth, M., & Tetzlaff, C. (2016). Opposing effects of neuronal activity on structural plasticity. Frontiers in neuroanatomy, 10, 75.
https://www.frontiersin.org/articles/10.3389/fnana.2016.00075/full
Review, looking how synapse potentiation (aka "activity") and elimination (aka "structural plasticity") compete and shape micro-connectivity.

Rabinovich, M., Volkovskii, A., Lecanda, P., Huerta, R., Abarbanel, H. D. I., & Laurent, G. (2001). Dynamical encoding by networks of competing neuron groups: winnerless competition. Physical review letters, 87(6), 068102.	
https://pdfs.semanticscholar.org/a0e4/249b0ab6e34699b12d49ad0d0a4c5e121886.pdf

Lagzi, F., & Rotter, S. (2015). Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state. PloS one, 10(9), e0138947.
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0138947

Gutkin, B. S., Laing, C. R., Colby, C. L., Chow, C. C., & Ermentrout, G. B. (2001). Turning on and off with excitation: the role of spike-timing asynchrony and synchrony in sustained neural activity. Journal of computational neuroscience, 11(2), 121-134.
Apparently, sorta seminal paper in this narrow field. But check out their later papers as well:
https://scholar.google.com/citations?user=Q2aFVoYAAAAJ&hl=en&oi=sra

# Synfire chains, Sequences

Zheng, P. and Triesch, J. (2014). Robust development of synfire chains

Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of spiking model neurons. Nature Neuroscience, 19(3):350–355.
Sounds like may be a neat review or opinion piece.

Gilra, A. and Gerstner, W. (2017). Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. Elife, 6(e28295):1–38.

Park, Y., Choi, W., & Paik, S. B. (2017). Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories. Scientific reports, 7(1), 5671.

Lee, H., Choi, W., Park, Y., & Paik, S. B. (2020). Distinct role of flexible and stable encodings in sequential working memory. Neural Networks, 121, 419-429.

Nicola, W. and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE training. Nature Communications, 8(1):1–15.

Nicola, W. and Clopath, C. (2019). A diversity of interneurons and Hebbian plasticity facilitate rapid compressible learning in the hippocampus. Nature Neuroscience, 22(7):1168–1181.

Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. Journal of Neuroscience, 33(23):9565–9575.

Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., and Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101(March):1–12.

Time perception in brain networks: may be related?
Integrating time from experience in the lateral entorhinal cortex
https://www.nature.com/articles/s41586-018-0459-6
Activity in perceptual classification networks as a basis for human subjective time perception
https://www.nature.com/articles/s41467-018-08194-7
Changing temporal context in human temporal lobe promotes memory of distinct episodes
https://www.nature.com/articles/s41467-018-08189-4
Also a blog post summary: https://www.quantamagazine.org/how-the-brain-creates-a-timeline-of-the-past-20190212/

Bobier, B., Stewart, T. C., & Eliasmith, C. (2014). A unifying mechanistic model of selective attention in spiking neurons. PLoS computational biology, 10(6).
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003577
Model of the cortex.

# Single-neuron calculations

David, Beniaguev, Segev Idan, and London Michael. "Single Cortical Neurons as Deep Artificial Neural Networks." bioRxiv (2019): 613141. https://www.biorxiv.org/content/10.1101/613141v1.full.pdf

Jones, I. S., & Kording, K. P. (2020). Can Single Neurons Solve MNIST? The Computational Power of Biological Dendritic Trees. arXiv preprint arXiv:2009.01269.
https://arxiv.org/abs/2009.01269

Abraham, W. C., Jones, O. D., & Glanzman, D. L. (2019). Is plasticity of synapses the mechanism of long-term memory storage?. NPJ science of learning, 4(1), 1-10.
https://www.ncbi.nlm.nih.gov/pubmed/31285847
A review on alternatives (non-synaptic) ways too store memories in the brain.

Dendritic action potentials and computation in human layer 2/3 cortical neurons
Albert Gidon1, Timothy Adam Zolnik1, Pawel Fidzinski2,3, Felix Bolduan4, Athanasia Papoutsi5, Panayiota Poirazi5, Martin Holtkamp2, Imre Vida3,4, Matthew Evan Larkum. Science  03 Jan 2020:
https://science.sciencemag.org/content/367/6473/83
Apparently show that individual cortical piramidal neurons can do XOR.
#dendritic

Unifying Long-Term Plasticity Rules for Excitatory Synapses by Modeling Dendrites of Cortical Pyramidal Neurons
C Ebner, C Clopath, P Jedlicka, H Cuntz - Cell Reports, 2019
https://www.sciencedirect.com/science/article/pii/S2211124719315591
Compartments, NMDA, all sorts of STDP stuff.

# Credit assignment

Bengio, Y., Lee, D. H., Bornschein, J., Mesnard, T., & Lin, Z. (2015). Towards biologically plausible deep learning. arXiv preprint arXiv:1502.04156.

Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2019). A solution to the learning dilemma for recurrent networks of spiking neurons. bioRxiv, 738385.
https://www.biorxiv.org/content/10.1101/738385v3
Something like backpropagation in spiking networks.

Lawrence, C., Sztyler, T., & Niepert, M. (2020). Explaining Neural Matrix Factorization with Gradient Rollback. arXiv preprint arXiv:2010.05516.
https://arxiv.org/abs/2010.05516a

Payeur, A., Guerguiev, J., Zenke, F., Richards, B., & Naud, R. (2020). Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.03.30.015511v1

Aljadeff, J., D'amour, J., Field, R. E., Froemke, R. C., & Clopath, C. (2019). Cortical credit assignment by Hebbian, neuromodulatory and inhibitory plasticity. arXiv preprint arXiv:1911.00307.
[https://arxiv.org/pdf/1911.00307.pdf](<https://arxiv.org/pdf/1911.00307.pdf>)

Gütig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience, 9(3), 420.

Zenke, F., Poole, B., & Ganguli, S. (2017, August). Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3987-3995). JMLR. org.
https://arxiv.org/pdf/1703.04200.pdf
Abstract synapses (not biological) that somehow prioritize what to learn based on some information? Claim to be biologically inspired. Well cited.

Krieg, D., & Triesch, J. (2014). A unifying theory of synaptic long-term plasticity based on a sparse distribution of synaptic strength. Frontiers in synaptic neuroscience, 6, 3.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941589/
Try to marry Hebbian with gradient descent and something else? Lots of math, not that well cited.

Rolfe, Jason Tyler (2012) Intrinsic Gradient Networks. Dissertation (Ph.D.), California Institute of Technology.
https://thesis.library.caltech.edu/6953/
PhD thesis that was never published on biologically plausible gradient descend in the brain? Check it out.

Kunin, D., Nayebi, A., Sagastuy-Brena, J., Ganguli, S., Bloom, J., & Yamins, D. L. (2020). Two Routes to Scalable Credit Assignment without Weight Symmetry. arXiv preprint arXiv:2003.01513.	
https://arxiv.org/abs/2003.01513
Classical backprop uses same weights there and back; aka "weight symmetry". They seem to claim that by making the rules only slightly non-local, they can solve stuff.
Code: https://github.com/neuroailab/neural-alignment

# RL in the brain

#rl

Dabney, W., Kurth-Nelson, Z., Uchida, N., Starkweather, C. K., Hassabis, D., Munos, R., & Botvinick, M. (2020). A distributional code for value in dopamine-based reinforcement learning. Nature, 1-5.
https://www.nature.com/articles/s41586-019-1924-6
(obv behind paywall)
DeepMind. Some cover of it:
https://www.technologyreview.com/s/615054/deepmind-ai-reiforcement-learning-reveals-dopamine-neurons-in-brain/

Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., ... & Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21(6), 860.
https://www.nature.com/articles/s41593-018-0147-8

Fremaux N, Sprekeler H, Gerstner W. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS computational biology. 2013;9(4):e1003024.

# DL models of brains

#deepneuro

Deep Neural Networks as Scientific Models (2020). Radoslaw M. Cichy, Daniel Kaiser. Trends in Cognitive Sciences.
https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(19)30034-8
Seem to suggest that for some types of tasks data it may be easier to model a process with a DL and interpret a DL, instead of directly interpreting the data?

https://www.biorxiv.org/content/10.1101/838383v1
Training deep neural density estimators to identify mechanistic models of neural dynamics
Gonçalves .. Macke 2019
About how to use deep learning to guess neuronal parameters to fit the actual activity of the network (?) They seem to be looking at actual V(t) though.

Yaoda Xu, Maryam Vaziri-Pashkam (2020) Limited correspondence in visual representation between the human brain and convolutional neural networks
https://www.biorxiv.org/content/10.1101/2020.03.12.989376v1 
Essentially, some critique of deepneuro!

Richards, B. A., Xia, F., Santoro, A., Husse, J., Woodin, M. A., Josselyn, S. A., & Frankland, P. W. (2014). Patterns across multiple memories are identified over time. Nature neuroscience, 17(7), 981.
https://www.nature.com/articles/nn.3736

Lillicrap, T. P., & Scott, S. H. (2013). Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics. Neuron, 77(1), 168-179.
https://www.sciencedirect.com/science/article/pii/S0896627312009920

Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS computational biology, 10(11), e1003915.
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915
Very well cited, this one.

Kording, K. P., Kayser, C., Einhauser, W., & Konig, P. (2004). How are complex cell properties adapted to the statistics of natural stimuli?. Journal of neurophysiology, 91(1), 206-212.
https://www.ncbi.nlm.nih.gov/m/pubmed/12904330/

Sorscher, B., Mel, G., Ganguli, S., & Ocko, S. (2019). A unified theory for the origin of grid cells through the lens of pattern formation. In Advances in Neural Information Processing Systems (pp. 10003-10013).
https://papers.nips.cc/paper/9191-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation
Supposedly, explains the development of grid cells, synthesizing two existing theories (recurrent with lateral inhibition and spontaneous development during navigation?)

Michaels, J. A., Schaffelhofer, S., Agudelo-Toro, A., & Scherberger, H. (2019). A neural network model of flexible grasp movement generation. bioRxiv, 742189.
https://www.biorxiv.org/content/10.1101/742189v1
Modeled vision-controlled grasping (based on monkey data) as convolutional network (based on Alexnet, it seems) → RNN (fully connected) → bottlneck of 8 neurons → another RNN → another bottleneck → another RNN → output. They call these bottlenecks "sparsity", which is interesting. I wonder whether it is fair to call it brain-inspired. Is it something that works, as it forces the learning of representations, and then only claim that it is brain-inspired, to give it some conceptual validity? Or is there actual evidence in the brain that connections between motor systems are throttled like that? Something to explore. Also they mention some "feedback connections" that may be connections between RNN modules, but I'd need to read the methods section carefully to see what's happening there. One of the most important aspects of this paper for me may be the neuro-inspired analysis of model performance that they do, and the very attitude to validation that they have. I wonder if that's something that can be applied to spiking networks.

Doerig, A., Schmittwilken, L., Sayim, B., Manassi, M., & Herzog, M. H. (2019). Capsule Networks as Recurrent Models of Grouping and Segmentation. BioRxiv, 747394.
https://www.biorxiv.org/content/10.1101/747394v2
Something on capsule networks better reproducing vision?

# Memory storage

Goode, T. D., Tanaka, K. Z., Sahay, A., & McHugh, T. J. (2020). An Integrated Index: Engrams, Place Cells, and Hippocampal Memory. Neuron.
https://www.cell.com/neuron/fulltext/S0896-6273(20)30528-6

Masse, N. Y., Yang, G. R., Song, H. F., Wang, X. J., & Freedman, D. J. (2019). Circuit mechanisms for the maintenance and manipulation of information in working memory. Nature neuroscience, 1.
https://www.biorxiv.org/content/biorxiv/early/2018/05/21/305714.full.pdf

González, O. C., Sokolov, Y., Krishnan, G. P., Delanois, J. E., & Bazhenov, M. (2020). Can sleep protect memories from catastrophic forgetting?. Elife, 9, e51005.
https://elifesciences.org/articles/51005
Model in which they show that sleep-reply can help to disentangle memories in a recurrent network, improving memory recall.

Dentate gyrus circuits for encoding, retrieval and discrimination of episodic memories. 2020. Thomas Hainmueller & Marlene Bartos. Nature Neuroscience. Review. May be interesting!
https://www.nature.com/articles/s41583-019-0260-z?WT.mc_id=TWT_NatRevNeurosci
It appears that they didn't pre-print it though, so paywalled, at least for now.

Context-modular memory networks support high-capacity, flexible, and robust associative memories. (2020). William F Podlaski,  Everton J Agnes,  Tim P Vogels
https://www.biorxiv.org/content/10.1101/2020.01.08.898528v1
About Hopfield limit. It seems they use an architecture in which large groups neurons or synapses can be modulated (via gating, inhibition) in a context-dependent manner. Show an increase in memory capacity; changes in active components (states), robustness to noise, memory search (??), memory stability. Gating of memories. Substrate for continuous learning? Related to the problem of [[catastrophic_forgetting]], including [this paper](https://www.pnas.org/content/115/44/E10467.short).
There's also a [tweetprint](https://twitter.com/TPVogels/status/1215572496570896384).

# Bottom-up validation

Geiger, F., Schrimpf, M., Marques, T., & DiCarlo, J. (2020). Wiring Up Vision: Minimizing Supervised Synaptic Updates Needed to Produce a Primate Ventral Stream. bioRxiv.
https://www.biorxiv.org/content/10.1101/2020.06.08.140111v1
Protoid: Geiger2020ventralstream

Misra, D. (2019). Mish: A Self Regularized Non-Monotonic Neural Activation Function. arXiv preprint arXiv:1908.08681.
https://arxiv.org/abs/1908.08681
Claim that if you make activation function non-monotonic (almost like a sigmoid, but with slight overshoots), it actually helps in some ways.
Github: https://github.com/digantamisra98/Mish
May be a curious parallel to this recent finding in Neuro, about XOR computation by dendrites of an individual cell Gidon, A., Zolnik, T. A., Fidzinski, P., Bolduan, F., Papoutsi, A., Poirazi, P., ... & Larkum, M. E. (2020). Dendritic action potentials and computation in human layer 2/3 cortical neurons. Science, 367(6473), 83-87. (no free pdf)

Lappalainen, J., Herpich, J., & Tetzlaff, C. (2019). A theoretical framework to derive simple, firing-rate-dependent mathematical models of synaptic plasticity. Frontiers in computational neuroscience, 13, 26.
https://www.frontiersin.org/articles/10.3389/fncom.2019.00026/full

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.

Lim, S. et al. Inferring learning rules from distributions of fring rates in cortical neurons. Nat. Neurosci. 18, 1804–1810 (2015).

Doerig, A., Bornet, A., Choung, O. H., & Herzog, M. H. (2019). Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines. bioRxiv, 744268.
https://www.sciencedirect.com/science/article/pii/S0042698919302299
Seem to be claiming that humans process images fundamentally differently than convolutional networks, because responding to perturbations follows a different logic. Sounds interesting!

Deep neuroethology of a virtual rodent
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Ölveczky
https://arxiv.org/abs/1911.09451
Apparently create a vidrual 3D rodent (like, with muscles, joints and what not), make it move in virtual environment, learn to move, then study its network using neuro methods.

Li, Z., Brendel, W., Walker, E., Cobos, E., Muhammad, T., Reimer, J., ... & Tolias, A. (2019). Learning from brains how to regularize machines. In Advances in Neural Information Processing Systems (pp. 9525-9535).
https://arxiv.org/abs/1911.05072

Whiteway, M. R., & Butts, D. A. (2019). The quest for interpretable models of neural population activity. Current opinion in neurobiology, 58, 86-93.

Feather, J., Durango, A., Gonzalez, R., & McDermott, J. (2019). Metamers of neural networks reveal divergence from human perceptual systems. In Advances in Neural Information Processing Systems (pp. 10078-10089).
https://papers.nips.cc/paper/9198-metamers-of-neural-networks-reveal-divergence-from-human-perceptual-systems.pdf
Metameres: in this case, different stimuli that cause identical activation in a part of a network. They seem to be claiming that there's a difference between humans and ANNs here.

Calhoun, A. J., Pillow, J. W., & Murthy, M. (2019). Unsupervised identification of the internal states that shape natural behavior. Nature Neuroscience, 1-10.
https://www.nature.com/articles/s41593-019-0533-x
Use unsupervise learning to identify internal (latent) states in a fly; then correlate these states with activity of individual neurons.

Spoerer, C. J., Kietzmann, T. C., & Kriegeskorte, N. (2019). Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition. bioRxiv, 677237.
https://www.biorxiv.org/content/10.1101/677237v3.full
Recurrent convolutional network  works better than a feed-forward convolutional network. It's slower, but better. Claim that it's similar to primate vision somehow.

# Dynamics

Maheswaranathan, N., Williams, A., Golub, M., Ganguli, S., & Sussillo, D. (2019). Universality and individuality in neural dynamics across large populations of recurrent networks. In_Advances in neural information processing systems_(pp. 15603-15615).

Merel, J., Botvinick, M., & Wayne, G. (2019). Hierarchical motor control in mammals and machines. Nature Communications, 10(1), 1-12.
https://www.nature.com/articles/s41467-019-13239-6
Opinion? Seems short and general, so give it priority.

Recurrence is required to capture the representational dynamics of the human visual system
Kleinman
https://www.pnas.org/content/116/43/21854
Visual representation, clustering objects, cortex, primates

Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales
Kaplan .. Zimmer
https://www.cell.com/neuron/fulltext/S0896-6273(19)30932-8
C elegans, activation dynamics. How fixed action patterns emerge from (are encoded by) the nervous system.

Wilson–Cowan Equations for Neocortical Dynamics (2016)
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4733815/

# Decision-making

https://www.biorxiv.org/content/10.1101/798553v1 
Recurrent neural network models of multi-area computation underlying decision-making Michael Kleinman, Chandramouli Chandrasekaran, Jonathan C Kao
It seems that they try recurrent networks (RNN) running in parallel, and compare their decision times to that from monkey cortex. Then show that one RNN doesn't match the distribution of times, but if you have several running in parallel, and then synthesizing info, you get similar results? Claim that different modes of distributed computation are experimentally testable like that.

Zoltowski, D. M., Pillow, J. W., & Linderman, S. W. (2020). Unifying and generalizing models of neural dynamics during decision-making. arXiv preprint arXiv:2001.04571.
https://arxiv.org/abs/2001.04571

# Variability, Stochastic Behaviors

#behav

Brembs, B. (2011). Towards a scientific concept of free will as a biological trait: spontaneous actions and decision-making in invertebrates. Proceedings of the Royal Society B: Biological Sciences, 278(1707), 930-939.
https://royalsocietypublishing.org/doi/full/10.1098/rspb.2010.2325

Noble, R., & Noble, D. (2018). Harnessing stochasticity: How do organisms make choices?. Chaos: An Interdisciplinary Journal of Nonlinear Science, 28(10), 106309.
https://aip.scitation.org/doi/full/10.1063/1.5039668#.XuDOjHNMscw.twitter

Murakami, M., Shteingart, H., Loewenstein, Y., & Mainen, Z. F. (2016). Distinct sources of deterministic and stochastic components of action timing decisions in rodent frontal cortex. bioRxiv, 088963.

Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., & Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature neuroscience, 22(12), 2066-2077.
https://www.biorxiv.org/content/biorxiv/early/2018/10/11/439885.full.pdf

# CogSci and Development

Mandler, J. M. (1988). How to build a baby: On the development of an accessible representational system. Cognitive development, 3(2), 113-136. [link](https://s3.amazonaws.com/academia.edu.documents/48154476/How_to_build_a_baby_On_the_development_o20160818-28052-ns7nel.pdf?response-content-disposition=inline%3B%20filename%3DHow_to_build_a_baby_On_the_development_o.pdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWOWYYGZ2Y53UL3A%2F20200223%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200223T181627Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Signature=11b1618e368d462df1af3b595c4c4c0775001e9a19e536a709c9b6f6d93ca8b1)
Quite popular (500 citations), strongly endorsed by Melanie Mitchell.
And part 2, with even more citations (1.6k):
Mandler, J. M. (1992). How to build a baby: II. Conceptual primitives. Psychological review, 99(4), 587.
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.460.5280&rep=rep1&type=pdf

# Other

https://www.biorxiv.org/content/10.1101/2020.03.24.006775v1
The mouse cortico-tectal projectome

Rahmani, P., Peruani, F., & Romanczuk, P. (2019). Flocking in complex environments--attention trade-offs in collective information processing. arXiv preprint arXiv:1907.11691.
https://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1007697
If you have a school of fish, with limited computational capacity, and only some individuals are "informed" (of danger, for example), what would be the optimal flocking strategy? A modeling paper.

Suzuki, M., & Larkum, M. E. (2020). General Anesthesia Decouples Cortical Pyramidal Neurons. Cell, 180(4), 666-676.
https://www.cell.com/action/showPdf?pii=S0092-8674%2820%2930105-7
About how general anesthesia actually retains FF signalling, but disrupts feedback signalling, and it is enough to disrupt consciousness. In all cortical neurons. Seems fun, and  important to understand the logic of cortical processing.

Harnessing behavioral diversity to understand neural computations for cognition Simon Musall, Anne E Urai, David Sussillo, Anne K Churchland
[https://www.sciencedirect.com/science/article/pii/S0959438819300285]
Potentially the most important section: "Relating rich behavior to neural activity by studying ANNs"

Learning predictive structure without a teacher: decision strategies and brain routes
Zoe Kourtzi, Andrew E Welchman
https://www.sciencedirect.com/science/article/pii/S0959438818302393 

The language of the brain: real-world neural population codes J Andrew Pruszynski, Joel Zylberberg
https://www.sciencedirect.com/science/article/pii/S0959438818302137 
Mostly (only?) about motor cortex, and how representation works there. Seems useful and accessible.

Data-driven models in human neuroscience and neuroengineering Bingni W. Brunton, Michael Beyeler. 
A review of current ML solutions for this general problem; still may be useful.

An argument for hyperbolic geometry in neural circuits
Tatyana O Sharpee
Interesting, but not immediately clear. But intriguing. Not much math, so should be very readable.

Mind the last spike — firing rate models for mesoscopic populations of spiking neurons Tilo Schwalger, Anton V Chizhov
https://www.sciencedirect.com/science/article/pii/S095943881930039X 
Review

https://www.biorxiv.org/content/10.1101/837567v1 
Interrogating theoretical models of neural computation with deep inference
Bittner .. Cunningham 2019
Apparently a very similar paper (to Goncalves 2019), but independent.

Balleine, B. W. (2019). The Meaning of Behavior: Discriminating Reflex and Volition in the Brain. *Neuron*, *104*(1), 47-62. 
Review. Potentially an interesting paper on free will, behaviorally (and computationally?) defined.

Classes of dendritic information processing Alexandre Payeur, Jean-Claude Béïque, Richard Naud

Constraining computational models using electron microscopy wiring diagrams Ashok Litwin-Kumar, Srinivas C Turaga

Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2, 4.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/
1.5k citations; seminal conceptual work apparently. Important for ML?

# ML of brain data

Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504.
https://www.biorxiv.org/content/10.1101/133504v2.abstract
A review.

