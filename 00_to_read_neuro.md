To-read: Neuro
===

**Bibliography collections:**
* [[bib_reconstruction]] - On reconstructing biological connectivity from spike-trains and alike

# General and top

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.
https://www.pnas.org/content/116/21/10537.short

# Topology and development

Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience Carina Curto, Katherine Morrison
https://www.sciencedirect.com/science/article/pii/S0959438819300443 
Review. Exactly what it should be (networks, motifs, dynamics)

Morrison, A., Aertsen, A., and Diesmann, M. (2007). Spike-timing dependent plasticity in balanced random networks. Neural Computation, 19:1437–1467.

Lazar, A., Pipa, G., & Triesch, J. (2006, April). The combination of STDP and intrinsic plasticity yields complex dynamics in recurrent spiking networks. In ESANN (pp. 647-652).

Ozturk, I., & Halliday, D. M. (2016, December). Mapping spatio-temporally encoded patterns by reward-modulated STDP in Spiking neurons. In Computational Intelligence (SSCI), 2016 IEEE Symposium Series on (pp. 1-8). IEEE.

Emergence of complex computational structures from chaotic neural networks through reward-modulated hebbian learning. GM Hoerzer, R Legenstein, W Maass, Cerebral Cortex, 24, 677-690, 2014

Gilson, M., Burkitt, A., & van Hemmen, J. L. (2010). STDP in recurrent neuronal networks. Spike-timing dependent plasticity, 271.

Naudé, J., Cessac, B., Berry, H., & Delord, B. (2013). Effects of cellular homeostatic intrinsic plasticity on dynamical and computational properties of biological recurrent neural networks. Journal of Neuroscience, 33(38), 15032-15043.

Fremaux N, Sprekeler H, Gerstner W. Reinforcement learning using a continuous time actor-critic framework with spiking neurons. PLoS computational biology. 2013;9(4):e1003024.

Gutig R, Aharonov R, Rotter S, Sompolinsky H. Learning input correlations through nonlinear temporally asymmetric Hebbian plasticity. The Journal of neuroscience: the official journal of the Society for Neuroscience. 2003;23(9):3697–714. Epub 2003/05/09.

Morrison A, Diesmann M, Gerstner W. Phenomenological models of synaptic plasticity based on spike timing. Biol Cybern. 2008;98(6):459–78. Epub 2008/05/21.

Gilson M, Burkitt AN, Grayden DB, Thomas DA, van Hemmen JL. Emergence of network structure due to spike-timing-dependent plasticity in recurrent neuronal networks III: Partially connected neurons driven by spontaneous activity. Biol Cybern. 2009;101(5–6):411–26.

Hoerzer, G. M., Legenstein, R., & Maass, W. (2014). Emergence of Complex Computational Structures From Chaotic Neural Networks Through Reward-Modulated Hebbian Learning. Cerebral Cortex, 24, 677-690.

Fauth, M., & Tetzlaff, C. (2016). Opposing effects of neuronal activity on structural plasticity. Frontiers in neuroanatomy, 10, 75.
https://www.frontiersin.org/articles/10.3389/fnana.2016.00075/full
Review, looking how synapse potentiation (aka "activity") and elimination (aka "structural plasticity") compete and shape micro-connectivity.

# Criticality, inhibition, dynamics

Popular account, harvest all links:
https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/

25 years of criticality in neuroscience — established results, open controversies, novel concepts J Wilting, V Priesemann
[https://www.sciencedirect.com/science/article/pii/S0959438819300248]
Very short, so a must read.

Mechanisms underlying gain modulation in the cortex (2020)
Katie A. Ferguson & Jessica A. Cardin 
https://www.nature.com/articles/s41583-019-0253-y?WT.mc_id=TWT_NatRevNeurosci
New review in Nature; ppl seem to like

Sussillo, D. and Abbott, L. F. (2009). Generating Coherent Patterns of Activity from Chaotic Neural Networks. Neuron, 63(4):544–557.

Universality and individuality in neural dynamics across large populations of recurrent networks Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, David Sussillo
https://arxiv.org/abs/1907.08549 
 
Mastrogiuseppe, F. and Ostojic, S. (2018). Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks. Neuron, 554 99(3):609–623.e29.

Cortical computations via metastable activity Giancarlo La Camera, Alfredo Fontanini, Luca Mazzucato
https://www.sciencedirect.com/science/article/pii/S0959438818302198 
Interesting; includes a section of spiking models of those. It's like criticality, but better, and also super-relevant.

Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., & Gerstner, W. (2011). Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks. *Science*, *334*(6062), 1569-1573. 
About what tunes inhibitory neurons, that in turn make excitatory activity properly sparse.

Schaub, M. T., Billeh, Y. N., Anastassiou, C. A., Koch, C., and Barahona, M. (2015). Emergence of Slow-Switching Assemblies in Structured Neuronal Networks. PLoS Computational Biology, 11(7):1–28.

Kim, J. K., & Fiorillo, C. D. (2017). Theory of optimal balance predicts and explains the amplitude and decay time of synaptic inhibition. Nature communications, 8, 14566.
https://www.nature.com/articles/ncomms14566
How ideally balanced inhibition can emerge from anti-hebbian plasticity. Skim.
 
# Reservoir computing

Tanaka, G., Yamane, T., Héroux, J. B., Nakane, R., Kanazawa, N., Takeda, S., ... & Hirose, A. (2019). Recent advances in physical reservoir computing: a review. Neural Networks.
https://www.sciencedirect.com/science/article/pii/S0893608019300784
High priority #todo

Pogodin, R., Corneil, D., Seeholzer, A., Heng, J., & Gerstner, W. (2019). Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks. arXiv preprint arXiv:1910.10559. 
https://arxiv.org/pdf/1910.10559.pdf 
Reservoir computer + a "working memory network"

Rotermund, D., & Pawelzik, K. R. (2019). Biologically plausible learning in a deep recurrent spiking network. bioRxiv, 613471.
https://www.biorxiv.org/content/10.1101/613471v1.full

# Synfire chains, Sequences

Zheng, P. and Triesch, J. (2014). Robust development of synfire chains

Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of spiking model neurons. Nature Neuroscience, 19(3):350–355.
Sounds like may be a neat review or opinion piece.

Gilra, A. and Gerstner, W. (2017). Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. Elife, 6(e28295):1–38.

Park, Y., Choi, W., & Paik, S. B. (2017). Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories. Scientific reports, 7(1), 5671.

Lee, H., Choi, W., Park, Y., & Paik, S. B. (2020). Distinct role of flexible and stable encodings in sequential working memory. Neural Networks, 121, 419-429.

Nicola, W. and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE training. Nature Communications, 8(1):1–15.

Nicola, W. and Clopath, C. (2019). A diversity of interneurons and Hebbian plasticity facilitate rapid compressible learning in the hippocampus. Nature Neuroscience, 22(7):1168–1181.

Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. Journal of Neuroscience, 33(23):9565–9575.

Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., and Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101(March):1–12.

Time perception in brain networks: may be related?
Integrating time from experience in the lateral entorhinal cortex
https://www.nature.com/articles/s41586-018-0459-6
Activity in perceptual classification networks as a basis for human subjective time perception
https://www.nature.com/articles/s41467-018-08194-7
Changing temporal context in human temporal lobe promotes memory of distinct episodes
https://www.nature.com/articles/s41467-018-08189-4
Also a blog post summary: https://www.quantamagazine.org/how-the-brain-creates-a-timeline-of-the-past-20190212/

# Credit assignment

Bellec, G., Scherr, F., Subramoney, A., Hajek, E., Salaj, D., Legenstein, R., & Maass, W. (2019). A solution to the learning dilemma for recurrent networks of spiking neurons. bioRxiv, 738385.
https://www.biorxiv.org/content/10.1101/738385v3
Something like backpropagation in spiking networks.

Aljadeff, J., D'amour, J., Field, R. E., Froemke, R. C., & Clopath, C. (2019). Cortical credit assignment by Hebbian, neuromodulatory and inhibitory plasticity. arXiv preprint arXiv:1911.00307.
[https://arxiv.org/pdf/1911.00307.pdf](<https://arxiv.org/pdf/1911.00307.pdf>)

Gütig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience, 9(3), 420.

Zenke, F., Poole, B., & Ganguli, S. (2017, August). Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3987-3995). JMLR. org.
https://arxiv.org/pdf/1703.04200.pdf
Abstract synapses (not biological) that somehow prioritize what to learn based on some information? Claim to be biologically inspired. Well cited.

Krieg, D., & Triesch, J. (2014). A unifying theory of synaptic long-term plasticity based on a sparse distribution of synaptic strength. Frontiers in synaptic neuroscience, 6, 3.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941589/
Try to marry Hebbian with gradient descent and something else? Lots of math, not that well cited.

Rolfe, Jason Tyler (2012) Intrinsic Gradient Networks. Dissertation (Ph.D.), California Institute of Technology.
https://thesis.library.caltech.edu/6953/
PhD thesis that was never published on biologically plausible gradient descend in the brain? Check it out.

# Single-neuron calculations

Abraham, W. C., Jones, O. D., & Glanzman, D. L. (2019). Is plasticity of synapses the mechanism of long-term memory storage?. NPJ science of learning, 4(1), 1-10.
https://www.ncbi.nlm.nih.gov/pubmed/31285847
A review on alternatives (non-synaptic) ways too store memories in the brain.

Dendritic action potentials and computation in human layer 2/3 cortical neurons
Albert Gidon1, Timothy Adam Zolnik1, Pawel Fidzinski2,3, Felix Bolduan4, Athanasia Papoutsi5, Panayiota Poirazi5, Martin Holtkamp2, Imre Vida3,4, Matthew Evan Larkum. Science  03 Jan 2020:
https://science.sciencemag.org/content/367/6473/83
Apparently show that individual cortical piramidal neurons can do XOR.

David, Beniaguev, Segev Idan, and London Michael. "Single Cortical Neurons as Deep Artificial Neural Networks." bioRxiv (2019): 613141. https://www.biorxiv.org/content/10.1101/613141v1.full.pdf

Unifying Long-Term Plasticity Rules for Excitatory Synapses by Modeling Dendrites of Cortical Pyramidal Neurons
C Ebner, C Clopath, P Jedlicka, H Cuntz - Cell Reports, 2019
https://www.sciencedirect.com/science/article/pii/S2211124719315591
Compartments, NMDA, all sorts of STDP stuff.

# Memory storage

Context-modular memory networks support high-capacity, flexible, and robust associative memories. (2020). William F Podlaski,  Everton J Agnes,  Tim P Vogels
https://www.biorxiv.org/content/10.1101/2020.01.08.898528v1
About Hopfield limit. It seems they use an architecture in which large groups neurons or synapses can be modulated (via gating, inhibition) in a context-dependent manner. Show an increase in memory capacity; changes in active components (states), robustness to noise, memory search (??), memory stability. Gating of memories. Substrate for continuous learning? Related to the problem of catastrophic forgetting, including [this paper](https://www.pnas.org/content/115/44/E10467.short).
There's also a [tweetprint](https://twitter.com/TPVogels/status/1215572496570896384).

Masse, N. Y., Yang, G. R., Song, H. F., Wang, X. J., & Freedman, D. J. (2019). Circuit mechanisms for the maintenance and manipulation of information in working memory. Nature neuroscience, 1.
https://www.biorxiv.org/content/biorxiv/early/2018/05/21/305714.full.pdf

# DL models of brains

https://www.biorxiv.org/content/10.1101/838383v1
Training deep neural density estimators to identify mechanistic models of neural dynamics
Gonçalves .. Macke 2019
About how to use deep learning to guess neuronal parameters to fit the actual activity of the network (?) They seem to be looking at actual V(t) though.

Richards, B. A., Xia, F., Santoro, A., Husse, J., Woodin, M. A., Josselyn, S. A., & Frankland, P. W. (2014). Patterns across multiple memories are identified over time. Nature neuroscience, 17(7), 981.
https://www.nature.com/articles/nn.3736

Wang, J. X., Kurth-Nelson, Z., Kumaran, D., Tirumala, D., Soyer, H., Leibo, J. Z., ... & Botvinick, M. (2018). Prefrontal cortex as a meta-reinforcement learning system. Nature neuroscience, 21(6), 860.
https://www.nature.com/articles/s41593-018-0147-8

Lillicrap, T. P., & Scott, S. H. (2013). Preference distributions of primary motor cortex neurons reflect control solutions optimized for limb biomechanics. Neuron, 77(1), 168-179.
https://www.sciencedirect.com/science/article/pii/S0896627312009920

Khaligh-Razavi, S. M., & Kriegeskorte, N. (2014). Deep supervised, but not unsupervised, models may explain IT cortical representation. PLoS computational biology, 10(11), e1003915.
https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003915
Very well cited, this one.

Kording, K. P., Kayser, C., Einhauser, W., & Konig, P. (2004). How are complex cell properties adapted to the statistics of natural stimuli?. Journal of neurophysiology, 91(1), 206-212.
https://www.ncbi.nlm.nih.gov/m/pubmed/12904330/

Sorscher, B., Mel, G., Ganguli, S., & Ocko, S. (2019). A unified theory for the origin of grid cells through the lens of pattern formation. In Advances in Neural Information Processing Systems (pp. 10003-10013).
https://papers.nips.cc/paper/9191-a-unified-theory-for-the-origin-of-grid-cells-through-the-lens-of-pattern-formation
Supposedly, explains the development of grid cells, synthesizing two existing theories (recurrent with lateral inhibition and spontaneous development during navigation?)

# Bottom-up validation

Misra, D. (2019). Mish: A Self Regularized Non-Monotonic Neural Activation Function. arXiv preprint arXiv:1908.08681.
https://arxiv.org/abs/1908.08681
Claim that if you make activation function non-monotonic (almost like a sigmoid, but with slight overshoots), it actually helps in some ways.
Github: https://github.com/digantamisra98/Mish
May be a curious parallel to this recent finding in Neuro, about XOR computation by dendrites of an individual cell Gidon, A., Zolnik, T. A., Fidzinski, P., Bolduan, F., Papoutsi, A., Poirazi, P., ... & Larkum, M. E. (2020). Dendritic action potentials and computation in human layer 2/3 cortical neurons. Science, 367(6473), 83-87. (no free pdf)

Lim, S. et al. Inferring learning rules from distributions of fring rates in cortical neurons. Nat. Neurosci. 18, 1804–1810 (2015).

Doerig, A., Bornet, A., Choung, O. H., & Herzog, M. H. (2019). Crowding Reveals Fundamental Differences in Local vs. Global Processing in Humans and Machines. bioRxiv, 744268.
https://www.sciencedirect.com/science/article/pii/S0042698919302299
Seem to be claiming that humans process images fundamentally differently than convolutional networks, because responding to perturbations follows a different logic. Sounds interesting!

Deep neuroethology of a virtual rodent
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Ölveczky
https://arxiv.org/abs/1911.09451
Apparently create a vidrual 3D rodent (like, with muscles, joints and what not), make it move in virtual environment, learn to move, then study its network using neuro methods.

Li, Z., Brendel, W., Walker, E., Cobos, E., Muhammad, T., Reimer, J., ... & Tolias, A. (2019). Learning from brains how to regularize machines. In Advances in Neural Information Processing Systems (pp. 9525-9535).
https://arxiv.org/abs/1911.05072

Whiteway, M. R., & Butts, D. A. (2019). The quest for interpretable models of neural population activity. Current opinion in neurobiology, 58, 86-93.

Feather, J., Durango, A., Gonzalez, R., & McDermott, J. (2019). Metamers of neural networks reveal divergence from human perceptual systems. In Advances in Neural Information Processing Systems (pp. 10078-10089).
https://papers.nips.cc/paper/9198-metamers-of-neural-networks-reveal-divergence-from-human-perceptual-systems.pdf
Metameres: in this case, different stimuli that cause identical activation in a part of a network. They seem to be claiming that there's a difference between humans and ANNs here.

Calhoun, A. J., Pillow, J. W., & Murthy, M. (2019). Unsupervised identification of the internal states that shape natural behavior. Nature Neuroscience, 1-10.
https://www.nature.com/articles/s41593-019-0533-x
Use unsupervise learning to identify internal (latent) states in a fly; then correlate these states with activity of individual neurons.

Spoerer, C. J., Kietzmann, T. C., & Kriegeskorte, N. (2019). Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition. bioRxiv, 677237.
https://www.biorxiv.org/content/10.1101/677237v3.full
Recurrent convolutional network  works better than a feed-forward convolutional network. It's slower, but better. Claim that it's similar to primate vision somehow.

# Free energy
#freeenergy

Gershman, S. J. (2019). What does the free energy principle tell Us about the brain?. arXiv preprint arXiv:1901.07945.
https://arxiv.org/abs/1901.07945

The Markov blankets of life: autonomy, active inference and the free energy principle
Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston and Julian Kiverstein
Published:17 January 2018
https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0792

Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature reviews neuroscience, 11(2), 127.
3k references! So people do find it useful after all?

https://www.frontiersin.org/articles/10.3389/fnsys.2019.00042/full 
The Dialectics of Free Energy Minimization Evert A. Boonstra and Heleen A. Slagter
Seems to be a review explaining the free energy minimization principle for an organism. It's not math though; looks almost like philosophy?

# Large-scale dynamics

Recurrence is required to capture the representational dynamics of the human visual system
Kleinman
https://www.pnas.org/content/116/43/21854
Visual representation, clustering objects, cortex, primates

https://www.biorxiv.org/content/10.1101/798553v1 
Recurrent neural network models of multi-area computation underlying decision-making Michael Kleinman, Chandramouli Chandrasekaran, Jonathan C Kao
It seems that they try recurrent networks (RNN) running in parallel, and compare their decision times to that from monkey cortex. Then show that one RNN doesn't match the distribution of times, but if you have several running in parallel, and then synthesizing info, you get similar results? Claim that different modes of distributed computation are experimentally testable like that.

Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales
Kaplan .. Zimmer
https://www.cell.com/neuron/fulltext/S0896-6273(19)30932-8
C elegans, activation dynamics. How fixed action patterns emerge from (are encoded by) the nervous system.

# Variability

Murakami, M., Shteingart, H., Loewenstein, Y., & Mainen, Z. F. (2016). Distinct sources of deterministic and stochastic components of action timing decisions in rodent frontal cortex. bioRxiv, 088963.

Findling, C., Skvortsova, V., Dromnelle, R., Palminteri, S., & Wyart, V. (2019). Computational noise in reward-guided learning drives behavioral variability in volatile environments. Nature neuroscience, 22(12), 2066-2077.
https://www.biorxiv.org/content/biorxiv/early/2018/10/11/439885.full.pdf

# Other

Harnessing behavioral diversity to understand neural computations for cognition Simon Musall, Anne E Urai, David Sussillo, Anne K Churchland
[https://www.sciencedirect.com/science/article/pii/S0959438819300285]
Potentially the most important section: "Relating rich behavior to neural activity by studying ANNs"

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.

Learning predictive structure without a teacher: decision strategies and brain routes
Zoe Kourtzi, Andrew E Welchman
https://www.sciencedirect.com/science/article/pii/S0959438818302393 

The language of the brain: real-world neural population codes J Andrew Pruszynski, Joel Zylberberg
https://www.sciencedirect.com/science/article/pii/S0959438818302137 
Mostly (only?) about motor cortex, and how representation works there. Seems useful and accessible.

Data-driven models in human neuroscience and neuroengineering Bingni W. Brunton, Michael Beyeler. 
A review of current ML solutions for this general problem; still may be useful.

An argument for hyperbolic geometry in neural circuits
Tatyana O Sharpee
Interesting, but not immediately clear. But intriguing. Not much math, so should be very readable.

Mind the last spike — firing rate models for mesoscopic populations of spiking neurons Tilo Schwalger, Anton V Chizhov
https://www.sciencedirect.com/science/article/pii/S095943881930039X 
Review

https://www.biorxiv.org/content/10.1101/837567v1 
Interrogating theoretical models of neural computation with deep inference
Bittner .. Cunningham 2019
Apparently a very similar paper (to Goncalves 2019), but independent.

Balleine, B. W. (2019). The Meaning of Behavior: Discriminating Reflex and Volition in the Brain. *Neuron*, *104*(1), 47-62. 
Review. Potentially an interesting paper on free will, behaviorally (and computationally?) defined.

Classes of dendritic information processing Alexandre Payeur, Jean-Claude Béïque, Richard Naud

Constraining computational models using electron microscopy wiring diagrams Ashok Litwin-Kumar, Srinivas C Turaga

Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2, 4.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/
1.5k citations; seminal conceptual work apparently. Important for ML?

# ML of brain data

Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504.
https://www.biorxiv.org/content/10.1101/133504v2.abstract
A review.

