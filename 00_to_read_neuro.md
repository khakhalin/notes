# To-read: Neuro

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.
https://www.pnas.org/content/116/21/10537.short

## Network structure

Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience Carina Curto, Katherine Morrison
https://www.sciencedirect.com/science/article/pii/S0959438819300443 
Review. Exactly what it should be (networks, motifs, dynamics)

## Criticality and small-scale dynamics

Popular account, harvest all links:
https://www.quantamagazine.org/brains-may-teeter-near-their-tipping-point-20180614/

25 years of criticality in neuroscience — established results, open controversies, novel concepts J Wilting, V Priesemann
[https://www.sciencedirect.com/science/article/pii/S0959438819300248]
Very short, so a must read.

Sussillo, D. and Abbott, L. F. (2009). Generating Coherent Patterns of Activity from Chaotic Neural Networks. Neuron, 63(4):544–557.

Universality and individuality in neural dynamics across large populations of recurrent networks Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, David Sussillo
https://arxiv.org/abs/1907.08549 
 
Mastrogiuseppe, F. and Ostojic, S. (2018). Linking Connectivity, Dynamics, and Computations in Low-Rank Recurrent Neural Networks. Neuron, 554 99(3):609–623.e29.

Cortical computations via metastable activity Giancarlo La Camera, Alfredo Fontanini, Luca Mazzucato
https://www.sciencedirect.com/science/article/pii/S0959438818302198 
Interesting; includes a section of spiking models of those. It's like criticality, but better, and also super-relevant.

Vogels, T. P., Sprekeler, H., Zenke, F., Clopath, C., & Gerstner, W. (2011). Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks. *Science*, *334*(6062), 1569-1573. 
About what tunes inhibitory neurons, that in turn make excitatory activity properly sparse.

Schaub, M. T., Billeh, Y. N., Anastassiou, C. A., Koch, C., and Barahona, M. (2015). Emergence of Slow-Switching Assemblies in Structured Neuronal Networks. PLoS Computational Biology, 11(7):1–28.
 
## Reservoir computing

Pogodin, R., Corneil, D., Seeholzer, A., Heng, J., & Gerstner, W. (2019). Working memory facilitates reward-modulated Hebbian learning in recurrent neural networks. arXiv preprint arXiv:1910.10559. 
https://arxiv.org/pdf/1910.10559.pdf 
Reservoir computer + a "working memory network"

Rotermund, D., & Pawelzik, K. R. (2019). Biologically plausible learning in a deep recurrent spiking network. bioRxiv, 613471.
https://www.biorxiv.org/content/10.1101/613471v1.full

## Synfire chains

Zheng, P. and Triesch, J. (2014). Robust development of synfire chains

## Developing topology

Morrison, A., Aertsen, A., and Diesmann, M. (2007). Spike-timing dependent plasticity in balanced random networks. Neural Computation, 19:1437–1467.

## Credit assignment

Lehmann, M. P., Xu, H. A., Liakoni, V., Herzog, M. H., Gerstner, W., & Preuschoff, K. (2019). One-shot learning and behavioral eligibility traces in sequential decision making. eLife, 8, e47463.
https://elifesciences.org/articles/47463

Aljadeff, J., D'amour, J., Field, R. E., Froemke, R. C., & Clopath, C. (2019). Cortical credit assignment by Hebbian, neuromodulatory and inhibitory plasticity. arXiv preprint arXiv:1911.00307.
[https://arxiv.org/pdf/1911.00307.pdf](<https://arxiv.org/pdf/1911.00307.pdf>)

Gütig, R., & Sompolinsky, H. (2006). The tempotron: a neuron that learns spike timing–based decisions. Nature neuroscience, 9(3), 420.

Zenke, F., Poole, B., & Ganguli, S. (2017, August). Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 (pp. 3987-3995). JMLR. org.
https://arxiv.org/pdf/1703.04200.pdf
Abstract synapses (not biological) that somehow prioritize what to learn based on some information? Claim to be biologically inspired. Well cited.

Krieg, D., & Triesch, J. (2014). A unifying theory of synaptic long-term plasticity based on a sparse distribution of synaptic strength. Frontiers in synaptic neuroscience, 6, 3.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3941589/
Try to marry Hebbian with gradient descent and something else? Lots of math, not that well cited.

Rolfe, Jason Tyler (2012) Intrinsic Gradient Networks. Dissertation (Ph.D.), California Institute of Technology.
https://thesis.library.caltech.edu/6953/
PhD thesis that was never published on biologically plausible gradient descend in the brain? Check it out.

## Bottom-up validation

Lim, S. et al. Inferring learning rules from distributions of fring rates in cortical neurons. Nat. Neurosci. 18, 1804–1810 (2015).

## Free energy

#freeenergy

Gershman, S. J. (2019). What does the free energy principle tell Us about the brain?. arXiv preprint arXiv:1901.07945.
https://arxiv.org/abs/1901.07945

The Markov blankets of life: autonomy, active inference and the free energy principle
Michael Kirchhoff, Thomas Parr, Ensor Palacios, Karl Friston and Julian Kiverstein
Published:17 January 2018
https://royalsocietypublishing.org/doi/full/10.1098/rsif.2017.0792

Friston, K. (2010). The free-energy principle: a unified brain theory?. Nature reviews neuroscience, 11(2), 127.
3k references! So people do find it useful after all?

https://www.frontiersin.org/articles/10.3389/fnsys.2019.00042/full 
The Dialectics of Free Energy Minimization Evert A. Boonstra and Heleen A. Slagter
Seems to be a review explaining the free energy minimization principle for an organism. It's not math though; looks almost like philosophy?

## Sequence learning in spiking networks

Abbott, L. F., DePasquale, B., and Memmesheimer, R. M. (2016). Building functional networks of spiking model neurons. Nature Neuroscience, 19(3):350–355.
Sounds like may be a neat review or opinion piece.

Gilra, A. and Gerstner, W. (2017). Predicting non-linear dynamics by stable local learning in a recurrent spiking neural network. Elife, 6(e28295):1–38.

Park, Y., Choi, W., & Paik, S. B. (2017). Symmetry of learning rate in synaptic plasticity modulates formation of flexible and stable memories. Scientific reports, 7(1), 5671.

Lee, H., Choi, W., Park, Y., & Paik, S. B. (2020). Distinct role of flexible and stable encodings in sequential working memory. Neural Networks, 121, 419-429.

Nicola, W. and Clopath, C. (2017). Supervised learning in spiking neural networks with FORCE training. Nature Communications, 8(1):1–15.

Nicola, W. and Clopath, C. (2019). A diversity of interneurons and Hebbian plasticity facilitate rapid compressible learning in the hippocampus. Nature Neuroscience, 22(7):1168–1181.

Brea, J., Senn, W., and Pfister, J.-P. (2013). Matching Recall and Storage in Sequence Learning with Spiking Neural Networks. Journal of Neuroscience, 33(23):9565–9575.

Manohar, S. G., Zokaei, N., Fallon, S. J., Vogels, T. P., and Husain, M. (2019). Neural mechanisms of attending to items in working memory. Neuroscience and Biobehavioral Reviews, 101(March):1–12.

## Large-scale dynamics

Recurrence is required to capture the representational dynamics of the human visual system
Kleinman
https://www.pnas.org/content/116/43/21854
Visual representation, clustering objects, cortex, primates

https://www.biorxiv.org/content/10.1101/798553v1 
Recurrent neural network models of multi-area computation underlying decision-making Michael Kleinman, Chandramouli Chandrasekaran, Jonathan C Kao
It seems that they try recurrent networks (RNN) running in parallel, and compare their decision times to that from monkey cortex. Then show that one RNN doesn't match the distribution of times, but if you have several running in parallel, and then synthesizing info, you get similar results? Claim that different modes of distributed computation are experimentally testable like that.

Nested Neuronal Dynamics Orchestrate a Behavioral Hierarchy across Timescales
Kaplan .. Zimmer
https://www.cell.com/neuron/fulltext/S0896-6273(19)30932-8
C elegans, activation dynamics. How fixed action patterns emerge from (are encoded by) the nervous system.

## Other

Harnessing behavioral diversity to understand neural computations for cognition Simon Musall, Anne E Urai, David Sussillo, Anne K Churchland
[https://www.sciencedirect.com/science/article/pii/S0959438819300285]
Potentially the most important section: "Relating rich behavior to neural activity by studying ANNs"

Raman, D. V., Rotondo, A. P., & O’Leary, T. (2019). Fundamental bounds on learning performance in neural circuits. Proceedings of the National Academy of Sciences, 116(21), 10537-10546.

Learning predictive structure without a teacher: decision strategies and brain routes
Zoe Kourtzi, Andrew E Welchman
https://www.sciencedirect.com/science/article/pii/S0959438818302393 

The language of the brain: real-world neural population codes J Andrew Pruszynski, Joel Zylberberg
https://www.sciencedirect.com/science/article/pii/S0959438818302137 
Mostly (only?) about motor cortex, and how representation works there. Seems useful and accessible.

Data-driven models in human neuroscience and neuroengineering Bingni W. Brunton, Michael Beyeler. 
A review of current ML solutions for this general problem; still may be useful.

An argument for hyperbolic geometry in neural circuits
Tatyana O Sharpee
Interesting, but not immediately clear. But intriguing. Not much math, so should be very readable.

Mind the last spike — firing rate models for mesoscopic populations of spiking neurons Tilo Schwalger, Anton V Chizhov
https://www.sciencedirect.com/science/article/pii/S095943881930039X 
Review

https://www.biorxiv.org/content/10.1101/837567v1 
Interrogating theoretical models of neural computation with deep inference
Bittner .. Cunningham 2019
Apparently a very similar paper (to Goncalves 2019), but independent.

Balleine, B. W. (2019). The Meaning of Behavior: Discriminating Reflex and Volition in the Brain. *Neuron*, *104*(1), 47-62. 
Review. Potentially an interesting paper on free will, behaviorally (and computationally?) defined.

Classes of dendritic information processing Alexandre Payeur, Jean-Claude Béïque, Richard Naud

Constraining computational models using electron microscopy wiring diagrams Ashok Litwin-Kumar, Srinivas C Turaga

Kriegeskorte, N., Mur, M., & Bandettini, P. A. (2008). Representational similarity analysis-connecting the branches of systems neuroscience. Frontiers in systems neuroscience, 2, 4.
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/
1.5k citations; seminal conceptual work apparently. Important for ML?

## ML of brain

Kietzmann, T. C., McClure, P., & Kriegeskorte, N. (2018). Deep neural networks in computational neuroscience. bioRxiv, 133504.
https://www.biorxiv.org/content/10.1101/133504v2.abstract
A review.

## Reverse engineering through ML

Deep neuroethology of a virtual rodent
Josh Merel, Diego Aldarondo, Jesse Marshall, Yuval Tassa, Greg Wayne, Bence Ölveczky
https://arxiv.org/abs/1911.09451
Apparently create a vidrual 3D rodent (like, with muscles, joints and what not), make it move in virtual environment, learn to move, then study its network using neuro methods.

Li, Z., Brendel, W., Walker, E., Cobos, E., Muhammad, T., Reimer, J., ... & Tolias, A. (2019). Learning from brains how to regularize machines. In Advances in Neural Information Processing Systems (pp. 9525-9535).
https://arxiv.org/abs/1911.05072

Whiteway, M. R., & Butts, D. A. (2019). The quest for interpretable models of neural population activity. Current opinion in neurobiology, 58, 86-93.

Feather, J., Durango, A., Gonzalez, R., & McDermott, J. (2019). Metamers of neural networks reveal divergence from human perceptual systems. In Advances in Neural Information Processing Systems (pp. 10078-10089).
https://papers.nips.cc/paper/9198-metamers-of-neural-networks-reveal-divergence-from-human-perceptual-systems.pdf
Metameres: in this case, different stimuli that cause identical activation in a part of a network. They seem to be claiming that there's a difference between humans and ANNs here.

Calhoun, A. J., Pillow, J. W., & Murthy, M. (2019). Unsupervised identification of the internal states that shape natural behavior. Nature Neuroscience, 1-10.
https://www.nature.com/articles/s41593-019-0533-x
Use unsupervise learning to identify internal (latent) states in a fly; then correlate these states with activity of individual neurons.

Spoerer, C. J., Kietzmann, T. C., & Kriegeskorte, N. (2019). Recurrent networks can recycle neural resources to flexibly trade speed for accuracy in visual recognition. bioRxiv, 677237.
https://www.biorxiv.org/content/10.1101/677237v3.full
Recurrent convolutional network  works better than a feed-forward convolutional network. It's slower, but better. Claim that it's similar to primate vision somehow.